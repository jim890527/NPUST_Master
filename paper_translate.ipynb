{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3515bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_path = \"C:/Users/user/Desktop/paper/next/Identification_of_Driver_Phone_Usage_Violations_via_State-of-the-Art_Object_Detection_with_Tracking.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcd76bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = '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'\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"請你成為文章翻譯的小幫手，請協助翻譯以下技術文件，以繁體中文輸出\"},\n",
    "        {\"role\": \"user\", \"content\": sentences[0]},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03434dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "pdf_name = paper_path\n",
    "reader = PdfReader(pdf_name)\n",
    "number_of_pages = len(reader.pages)\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for i in range(number_of_pages):\n",
    "    page = reader.pages[i]\n",
    "    text = page.extract_text()\n",
    "    sentences = sent_tokenize(text)\n",
    "    input_sentences = ''\n",
    "  \n",
    "    for sentence in sentences:\n",
    "        input_sentences += sentence\n",
    "        if len(input_sentences) > 1000:\n",
    "            chunks.append(input_sentences)\n",
    "            input_sentences = ''\n",
    "    chunks.append(input_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "781bda98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: Identiﬁcation of Driver Phone Usage Violations via\n",
      "State-of-the-Art Object Detection with Tracking\n",
      "Steven Carrell\n",
      "School of Computing\n",
      "Newcastle University\n",
      "Newcastle upon Tyne, United Kingdom\n",
      "steven.carrell.ncl@gmail.comAmir Atapour-Abarghouei\n",
      "Department of Computer Science\n",
      "Durham University\n",
      "Durham, United Kingdom\n",
      "amir.atapour-abarghouei@durham.ac.uk\n",
      "Abstract —The use of mobiles phones when driving have been\n",
      "a major factor when it comes to road trafﬁc incidents and\n",
      "the process of capturing such violations can be a laborious\n",
      "task.Advancements in both modern object detection frameworks\n",
      "and high-performance hardware has paved the way for a more\n",
      "automated approach when it comes to video surveillance.In\n",
      "this work, we propose a custom-trained state-of-the-art object\n",
      "detector to work with roadside cameras to capture driver phone\n",
      "usage without the need for human intervention.The proposed\n",
      "approach also addresses the issues caused by windscreen glare\n",
      "and introduces the steps required to remedy this.\n",
      "翻譯結果: 透過最先進的物件偵測與跟蹤技術來辨別駕駛手機使用違規行為的方法\n",
      "\n",
      "史蒂芬·卡雷爾\n",
      "紐卡斯爾大學計算機學院\n",
      "英國紐卡斯爾\n",
      "電子郵件：steven.carrell.ncl@gmail.com\n",
      "\n",
      "阿米爾·阿塔普-阿巴格魯伊\n",
      "杜倫大學計算機科學系\n",
      "英國杜倫\n",
      "電子郵件：amir.atapour-abarghouei@durham.ac.uk\n",
      "\n",
      "摘要：行駛時使用手機一直是道路交通事故的主要因素，而捕捉此類違規行為的過程可能是一個繁瑣的任務。現代物件偵測框架和高性能硬體的進步為影像監控的自動化方法鋪平了道路。在這項工作中，我們提出了一款定制的最先進物件偵測器，可與路邊攝像機配合使用，捕捉駕駛手機使用的違規行為，並不需要人為干預。所提出的方法還解決了風擋反光所引起的問題，介紹了必要的修復步驟。\n",
      "原文: Twelve pre-\n",
      "trained models are ﬁne-tuned with our custom dataset using\n",
      "four popular object detection methods: YOLO, SSD, Faster R-\n",
      "CNN, and CenterNet.Out of all the object detectors tested, the\n",
      "YOLO yields the highest accuracy levels of up to \u001896% (AP10)\n",
      "and frame rates of up to \u001830 FPS.DeepSort object tracking\n",
      "algorithm is also integrated into the best-performing model to\n",
      "collect records of only the unique violations, and enable the\n",
      "proposed approach to count the number of vehicles.The proposed\n",
      "automated system will collect the output images of the identiﬁed\n",
      "violations, timestamps of each violation, and total vehicle count.Data can be accessed via a purpose-built user interface.Index Terms —Mobile phone detection, YOLO Object Detec-\n",
      "tion, Intelligent Transportation Systems, Deep Learning\n",
      "I. I NTRODUCTION\n",
      "According to the World Health Organization (WHO), ap-\n",
      "proximately 1.3 million people die each year as a result of\n",
      "road trafﬁc accidents [1].A contributing factor towards this is\n",
      "the use of a handheld mobile devices while operating a motor\n",
      "vehicle.\n",
      "翻譯結果: 使用自訂數據集，我們利用四種流行的物體檢測方法：YOLO、SSD、Faster R-CNN和CenterNet，對十二個預訓練模型進行微調。在所有測試過的物體檢測器中，YOLO的準確度最高，可達96％（AP10），幀速率可達30 FPS。最佳表現模型還整合了DeepSort物體跟踪算法，僅收集唯一違規記錄，並使所提出的方法能夠計算車輛數量。所提出的自動化系統將收集識別違規的輸出圖像、每次違規的時間戳和總車輛數量。數據可通過專門設計的用戶界面進行訪問。關鍵詞：手機檢測、YOLO物體檢測、智能交通系統、深度學習。根據世界衛生組織（WHO）的數據，每年有大約130萬人因道路交通事故喪生。其中一個貢獻因素是在操作機動車輛時使用手持式行動設備。\n",
      "原文: Drivers using a mobile phone are approximately four\n",
      "times more likely to be involved in a crash than drivers not\n",
      "using their phone [1].In 2017, the UK Government doubled the penalty for using\n",
      "a mobile phone while driving to 6 points and a £200 ﬁne\n",
      "(up from 3 points and £100) [2].A study carried out in 2015\n",
      "suggests that there is a negative correlation between a higher\n",
      "ﬁne and the likelihood of a person using their phone [3].Typically, catching drivers using their phones involves road-\n",
      "side police performing the laborious task of capturing the\n",
      "violation as it happens.Unless there are signiﬁcant resources\n",
      "dedicated to this task, there is a strong probability that many\n",
      "of these violations will go undetected.This opens the door for\n",
      "a more automated process of capturing these violations.Recent years have seen the development of object detection\n",
      "in video surveillance [4].Object detection frameworks such as\n",
      "(a) Step one.(b) Step two.Fig.1: Example of the proposed two-step approach: the ﬁrst\n",
      "step (left) detects windscreen; the second step (right) ﬁrst crops\n",
      "the driver’s side and only then detects the phone.\n",
      "翻譯結果: 使用手機的駕駛員相較於不使用手機的駕駛員，發生事故的機率大約高出四倍 [1]。英國政府在2017年將開車時使用手機的罰款加倍至200英鎊並處以6分的扣分（原先為100英鎊並處以3分）[2]。2015年進行的一項研究指出，罰款越高，人們使用手機的機率就越低 [3]。通常，抓到使用手機的駕駛需要路邊警察進行費時的捕捉違規行為任務。除非有大量資源專門用於此任務，否則很有可能會漏檢許多違規行為。這就開啟了一種更自動化的捕捉這些違規行為的方法。近年來，視頻監控中的物件檢測技術已經得到發展[4]。物件檢測框架（如圖1所示）可以進行屏幕偵測和手機偵測。提出了一種兩步驟的方法，第一步（左圖）檢測汽車的擋風玻璃，第二步（右圖）首先裁剪出駕駛員的那一側，再進行手機的檢測。\n",
      "原文: Faster Region Based Convolutional Neural Networks (Faster\n",
      "R-CNN) [5] and Single Shot Detector (SSD) [6] have made\n",
      "it possible to take video images and detect objects with both\n",
      "high accuracy and speed [7].In this work, we propose a fully-automated system that will\n",
      "take live video from roadside surveillance cameras and detect\n",
      "if a driver is using a mobile phone whilst the vehicle is in\n",
      "operation.We will explore different quality cameras (high-end\n",
      "and low-end) whilst addressing challenges such as windscreen\n",
      "glare, tinted windows and low-light scenarios.In order for the\n",
      "system to be fully automated, it will need to have the ability\n",
      "to log each unique violation as well as to save the images\n",
      "corresponding to each violation.We propose two methods for achieving this task; ﬁrst, a\n",
      "single-step method focusing on efﬁciency and speed, using a\n",
      "single trained model to detect violations.This method will\n",
      "suffer a trade-off with accuracy due to potential issues caused\n",
      "by having to ﬁnd an extremely small object (phone) within\n",
      "a large image.\n",
      "翻譯結果: 更快的基于区域的卷积神经网络（Faster R-CNN）[5]和单次检测器（SSD）[6]使得在视频图像中高精度和高速检测物体成为可能[7]。在本文中，我们提出了一个完全自动化的系统，将从路侧监控摄像头获取实时视频并检测车辆行驶时是否有司机使用手机。我们将探索不同质量的摄像头（高端和低端），同时解决挑战，如挡风玻璃反光、有色车窗和低光照情况。为了使系统完全自动化，它需要具有记录每个唯一违规行为以及保存与每个违规行为相对应的图像的能力。我们提出了两种实现这个任务的方法；第一种是单步方法，专注于效率和速度，使用单个训练模型来检测违规行为。由于可能需要在大图像中查找极小的物体（手机），因此此方法将牺牲准确性。\n",
      "原文: The second method (two-step) focuses on\n",
      "achieving high accuracy by running two individually trained\n",
      "models simultaneously.The single-step system is trained to detect two classes,\n",
      "namely mobile phone and licence plate with the plate only\n",
      "used as a method of counting the total number of vehicles.The two-step system (Figure 1) ﬁrst detects the windscreen\n",
      "and then uses the cropped image of the driver’s side of the\n",
      "windscreen as the input to the second step to detect the mobile\n",
      "phone.Similar to the licence plate in the ﬁrst (single-step)\n",
      "method, the windscreen is used to count the number of vehiclesarXiv:2109.02119v3  [cs.CV]  8 Oct 2021\n",
      "翻譯結果: 第二種方法（兩步驟法）著重於通過同時運行兩個單獨訓練的模型來實現高精度。單步系統的訓練是為了檢測兩類物件，即手機和車牌，而車牌僅作為計算車輛總數的方法。兩步驟系統（圖1）首先檢測擋風玻璃，然後使用司機側擋風玻璃的裁剪圖像作為第二步驟的輸入，以檢測手機。與第一（單步驟）方法的車牌類似，擋風玻璃用於計算車輛數量。\n",
      "原文: so the number of detections can be worked out as a proportion\n",
      "to the total number of vehicles.Models are trained using a\n",
      "number of different state-of-the art object detector frameworks.These include YOLO (You Only Look Once) v3 [8] and v4\n",
      "[9], SSD [10], [11], Faster R-CNN [5] and Centernet [11].Model performance is measured via Average Precision (AP)\n",
      "[12], and using both the PASCAL VOC evaluation metric\n",
      "where the Intersection Over Union (IoU) score is >0.5 [13],\n",
      "whilst also testing IoU >0.1 due to the nature of objects being\n",
      "detected in this particular application.The proposed solution\n",
      "will be designed to work with a live video; therefore, we also\n",
      "evaluate the efﬁciency of the proposed method by calculating\n",
      "the frame rate of the output - i.e.frames per second (FPS).The images that are used to train our model on the phone\n",
      "class will predominantly be obtained and created especially\n",
      "for this project.In order for the model to detect mobile phone\n",
      "use violations with a reasonable level of accuracy, the training\n",
      "images will be replications of the real-world scenario of a\n",
      "person using their phone whilst driving (examples of training\n",
      "images can be seen in Figure 8).\n",
      "翻譯結果: 因此，偵測數量可以計算為所有車輛的比例。模型使用多種最先進的物體檢測框架進行訓練。這些包括YOLO（You Only Look Once）v3 [8]和v4 [9]、SSD [10]、[11]、Faster R-CNN [5]和Centernet [11]。模型性能通過平均精度（AP）[12]進行測量，並使用PASCAL VOC評估指標，其中交集比聯集（IoU）得分> 0.5 [13]，同時也測試IoU> 0.1，因為在此特定應用中檢測對象的性質。所提出的解決方案將被設計為可在實時視頻中運行；因此，我們還通過計算輸出的幀率（即每秒幀數（FPS））來評估所提出的方法的效率。用於在手機類上訓練我們的模型的圖像大多是通過特別為此項目獲取和創建的。為了使模型以合理的精度檢測手機使用違規行為，訓練圖像將是人們駕駛時使用手機的現實場景的複製（可見於圖8的訓練圖像範例）。\n",
      "原文: Once the ﬁnal model is chosen, we integrate a tracking\n",
      "algorithm [14] into our model to avoid the issue of logging\n",
      "multiple detections for the same violation.This will allow\n",
      "the system to keep track of the total number of violations\n",
      "for a given duration.This data can then be analysed using a\n",
      "purposely designed user interface.This work aims to successfully develop a system that can\n",
      "work with a roadside camera 24 hours a day to automatically\n",
      "detect whether a person is using their mobile phone when in\n",
      "operation of a motor vehicle.In short, the primary contribu-\n",
      "tions of this paper are as follows:\n",
      "\u000fTrain and evaluate multiple object detection methods [5],\n",
      "[8]–[11] to detect mobile phone use violations with a\n",
      "reasonable degree of accuracy and speed by establishing\n",
      "an appropriate trade-off between predictive performance\n",
      "and efﬁciency.\u000fTest the trained models on both full images and cropped\n",
      "windscreens to determine if a single-step or two-step\n",
      "approach is more appropriate.The single-step method\n",
      "operates on the full image only, while the two-step system\n",
      "ﬁrst ﬁnds windscreen and then uses this cropped image\n",
      "to detect the phone (Section III).\n",
      "翻譯結果: 一旦選定最終模型，我們將整合一個追蹤演算法[14]到我們的模型中，以避免多次記錄相同違規情況的問題。這將允許系統在特定時間內跟踪違規總數。這些數據可以通過特意設計的用戶界面進行分析。本項工作旨在成功開發一個可以與路邊攝像頭24小時自動檢測人們在駕駛機動車輛時是否使用手機的系統。簡而言之，本文的主要貢獻如下：\n",
      "\n",
      "- 通過建立適當的預測性能和效率之間的權衡，培訓和評估多個物體檢測方法[5]，[8]-[11]，以合理的精度和速度檢測手機使用違規情況。\n",
      "- 在全圖像和裁剪的擋風玻璃上測試訓練好的模型，以確定單步或兩步方法更適合。單步方法僅在完整圖像上運作，而兩步系統首先找到擋風玻璃，然後使用此裁剪圖像來檢測手機（第三節）。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: \u000fEstablish what can be achieved with on a low-cost\n",
      "budget using a low-end consumer camera solution as well\n",
      "as providing insights on what can be achieved with a\n",
      "reasonable budget using high-end cameras.\u000fEnsure issues such as windscreen glare and poorly-lit\n",
      "environments are addressed so the system can work at\n",
      "any time of the day.\u000fIntegrate a tracking algorithm [14] to identify unique\n",
      "detections in order to log useful data whilst providing\n",
      "snapshots of the violations.Collected data can be ac-\n",
      "cessed through a purposely designed user interface.To better enable reproducibility, the source code for theproject is publicly available1.The remainder of the paper is organised as follows.Related\n",
      "work on mobile phone usage detection within the existing\n",
      "literature is presented in Section II.The proposed approach\n",
      "is described in Section III.Experimental results are examined\n",
      "and discussed in Section IV.We address limitations and future\n",
      "work in Section V, before ﬁnally concluding the paper in\n",
      "Section VI.\n",
      "翻譯結果: 該文件旨在確定在低成本和低端消費級攝像頭解決方案下可以實現什麼，並提供在合理預算下使用高端攝像頭可以實現什麼的洞察。同時解決像風擋反光和光照不良等問題，以便系統在任何時間都可以正常工作。整合跟踪算法[14]以識別唯一檢測，以記錄有用的數據並提供違規的快照。收集的數據可以通過專門設計的用戶界面訪問。為了更好地實現可重現性，該項目的源代碼是公開可用的1。本文的其餘部分分別為以下內容：第二部分介紹現有文獻中關於手機使用檢測的相關工作。第三部分描述了提出的方法。第四部分檢查並討論實驗結果。我們在第五部分討論了限制和未來工作，最後在第六部分總結本文。\n",
      "原文: II.R ELATED WORK\n",
      "We consider related work in the context of object detection\n",
      "in general (Section II-A) and speciﬁc systems utilised in\n",
      "distracted driver identiﬁcation applications (Section II-B).A.Object Detection\n",
      "Traditionally, any kind of object identiﬁcation with the use\n",
      "of video surveillance would be done manually and could\n",
      "involve the laborious task of humans either monitoring the\n",
      "live stream of the camera or reviewing historic footage.This\n",
      "difﬁcult and tiresome task meant that a more intelligent ap-\n",
      "proach would eventually be required [15].Object detection is a\n",
      "computer vision task concerned with detecting and classifying\n",
      "objects within an image.The use of this technology has paved\n",
      "the way for a more automated solution into video surveillance\n",
      "applications.Examples of such tasks include Licence Plate\n",
      "Recognition (LPR) [16], people tracking, vehicle counting, or\n",
      "unattended baggage in airports.We can describe two-major historic milestones in the de-\n",
      "velopment of object detection: “traditional object detection\n",
      "period” (pre 2014) [17]–[19] and “learning-based detection\n",
      "period” [20].\n",
      "翻譯結果: II. 相關工作\n",
      "我們在一般物體偵測（第 II-A 部分）和應用於分心駕駛辨識的特定系統（第 II-B 部分）的上下文中考慮相關工作。\n",
      "\n",
      "A. 物體偵測\n",
      "傳統上，使用視頻監控來進行任何類型的物體識別都是手動完成的，可能涉及人類監視攝像頭的實時流或查看歷史記錄的費力任務。這項困難而繁瑣的任務意味著最終需要更智能的方法 [15]。物體偵測是一項計算機視覺任務，旨在檢測和分類圖像中的物體。該技術的應用為視頻監控應用程序開辟了更自動化的解決方案。此類任務的示例包括車牌識別（LPR） [16]、人員追蹤、車輛計數或未監管的行李在機場。\n",
      "\n",
      "我們可以描述物體偵測的兩個主要歷史里程碑：“傳統物體偵測時期”（2014 年之前）[17]–[19]和“基於學習的偵測時期”[20]。\n",
      "原文: It was not until 2015 where object detection\n",
      "could be utilised in real-time video with the development of\n",
      "Faster Regional-Based Neural Network (Faster R-CNN) [5],\n",
      "an improvement from its predecessor R-CNN [21] in both\n",
      "accuracy and speed.Modern-day object detectors can be categorised into two\n",
      "types: one-stage (YOLO [8], [9], SSD [10], and CenterNet\n",
      "[11]) and two-stage (R-CNN series including Fast R-CNN\n",
      "[22], Faster R-CNN [5], R-FCN [23], and Libra R-CNN [24]).Two-stage detectors generally split the overall object detection\n",
      "task as follows: the ﬁrst stage is to generate proposals, and then\n",
      "the second stage focuses on the veriﬁcation and recognition\n",
      "of these proposals [25].The two-stage detectors are typically\n",
      "slower due to their heavy-head design but detect with a higher\n",
      "level of accuracy [25], whilst one-stage approaches tend to be\n",
      "faster but can be more limited in terms of their predictive\n",
      "performance [26].B.Distracted Driver Identiﬁcation\n",
      "Recent years have seen a rise in the number of distracted\n",
      "driver identiﬁcation systems.\n",
      "翻譯結果: 直到2015年，隨著Faster Regional-Based Neural Network (Faster R-CNN) [5]的開發，物體偵測才能在實時視頻中應用，Faster R-CNN相較於其前身R-CNN [21] 在準確性和速度上都有所改進。現代物體檢測器可以分為兩種類型:單階段(YOLO [8], [9], SSD [10], and CenterNet [11])和雙階段(R-CNN系列包括Fast R-CNN [22]、Faster R-CNN [5]、R-FCN [23]和Libra R-CNN [24])。 雙階段檢測器通常將整個物體檢測任務分為兩個階段:第一階段是生成提議，第二階段則專注於對這些提議進行驗證和識別 [25]。由於其較重的設計，雙階段檢測器通常比較慢，但檢測精度較高 [25]。相反，單階段方法往往更快，但在預測性能方面可能更加有限[26]。B.分心駕駛識別近年來，分心駕駛識別系統的數量逐漸增加。\n",
      "原文: One such approach [27] utilises\n",
      "current infrastructure using LPR [16] roadside cameras.The\n",
      "system contains a three-stage approach to detecting whether\n",
      "the driver is using their phone.The ﬁrst stage is to detect\n",
      "the windscreen, which is then cropped and processed in the\n",
      "1https://github.com/carrell-ncl/Windscreen2\n",
      "翻譯結果: 一個這樣的方法[27]利用現有的基礎設施，使用道路監視攝像頭中的車牌識別(LPR)[16]。該系統包含三個階段的方法，以檢測駕駛員是否在使用手機。第一階段是檢測擋風玻璃，然後對其進行裁剪並在1中處理。https://github.com/carrell-ncl/Windscreen2\n",
      "原文: second model to identify whether a person can be clearly\n",
      "seen.This process is in place to ensure that the images with\n",
      "undesirable reﬂection effects are not processed.The ﬁnal\n",
      "stage will then detect for mobile phone usage [27].The\n",
      "authors recognise a limitation of this system [27]; the images\n",
      "could not be acquired during summer days between 12:00 and\n",
      "15:00 due to excessive amount of windscreen glare.Initial\n",
      "testing for this work shows windscreen glare for the majority\n",
      "of the day (as discussed in Section III-A1), suggesting that\n",
      "this would not be appropriate for some territories.Fig.2: The three-stage mobile usage violation detection ap-\n",
      "proach of Alkan et al.[27].Another study builds upon a software already developed\n",
      "by the Dutch Police, which ﬁrst looks for a licence plate,\n",
      "based on which it detects and outputs the driver’s side of the\n",
      "windscreen [28].These images are used as inputs to the trained\n",
      "model where hands, phone and face are detected.The image of\n",
      "the hand (taken from bounding box) is classiﬁed using VGG-\n",
      "16 [29].\n",
      "翻譯結果: 第二個模型是用來判斷一個人是否可以清晰看到。這個過程是為了確保處理的圖像沒有不良反射效果。最後一個階段將進行手機使用的檢測[27]。作者認識到這個系統的一個限制[27]，在夏天的12:00到15:00期間無法獲取圖像，因為風擋玻璃反射過多。初步測試顯示大部分時間都會有風擋玻璃反射（在第III-A1節中討論），這表明這可能不適用於某些地區。圖2：Alkan等人[27]的三階段行車中手機使用違規檢測方法。另一個研究基於荷蘭警方已開發的軟件，在這個軟件中首先尋找車牌，然後檢測並輸出駕駛員方向的風擋玻璃[28]。這些圖像被用作訓練模型的輸入，手、手機和臉部被檢測出來。從邊框中取出的手部圖像使用VGG-16[29]進行分類。\n",
      "原文: The model looks to eliminate the issue of falsely\n",
      "classifying objects such as phone mounts.This is done by\n",
      "checking where the phone is positioned in relation to the head\n",
      "and hand: if the distance is greater than a set threshold, then it\n",
      "is not classiﬁed.The system is dependent on the Dutch Police\n",
      "windscreen detector, which only works for Dutch Plates.Due\n",
      "to the reliance on a third party software to make this work,\n",
      "there is a lack of control on a signiﬁcant portion of the overall\n",
      "approach.There could be issues with support further down the\n",
      "line, or changes to licensing.It seems that more work would\n",
      "be required in order for this system to be deployable.Another work monitors the driver using their phone in\n",
      "addition to hand position [30].This approach uses cameras\n",
      "inside the vehicle positioned towards both the driver and\n",
      "the steering wheel.They propose a Multiple Scale Faster R-\n",
      "CNN [5] to detect both mobile phone and hands.Geometric\n",
      "information is then extracted to determine if the driver is using\n",
      "their phone [30].\n",
      "翻譯結果: 該模型旨在消除誤分類物品（如手機支架）的問題。這是通過檢查手機相對於頭部和手的位置來完成的：如果距離大於設定的閾值，則不進行分類。系統依賴於荷蘭警方的擋風玻璃檢測器，該檢測器僅適用於荷蘭牌照。由於依賴第三方軟件使其運作，因此在整體方法的一個重要部分上缺乏控制。在支持方面可能會存在問題，或者可能會發生許可證的變更。似乎需要進行更多的工作，以使該系統可部署。另一個工作對使用手機和手的位置監測駕駛員進行了監控。此方法使用安裝在車內的攝像頭，分別對駕駛員和方向盤進行拍攝。他們提出了一種多規模更快的 R-CNN 方法來檢測手機和手​​的位置。然後提取幾何信息以判斷駕駛員是否正在使用手機。\n",
      "原文: Mass deployment of this system could prove\n",
      "costly and impractical due to its reliance on cameras within\n",
      "the vehicle as well as the drivers being aware that they are\n",
      "being monitored.Our proposed system looks to solve the limitations of prior\n",
      "work starting with the issues of windscreen glare where the\n",
      "use of a polarising ﬁlter on the camera lens has been explored.The trained object detector is designed to work off a standard\n",
      "video surveillance roadside camera and as a result, limit the\n",
      "cost of deployment.The next section describes our approach\n",
      "for building this system in more detail.Object Detector Backbone Image Resolution\n",
      "YOLOv3 [3] Darknet-53 [3] 320, 416, 512\n",
      "YOLOv4 [9] CSPDarknet-53 [9] 320, 416, 512\n",
      "Faster R-CNN [5] Resnet101 [31] 640\n",
      "Faster R-CNN [5] Resnet152 [31] 640\n",
      "Centernet [11] Resnet101 FPN [31] 512\n",
      "SSD [10] Mobilenetv2 FPNLite [32] 640\n",
      "SSD [10] ResNet50 V1 FPN [31] 640\n",
      "SSD [10] ResNet101 V1 FPN [31] 640\n",
      "TABLE I: Chosen object detectors and pre-trained base models\n",
      "(Backbone) ﬁne-tuned and evaluated as part of this work.\n",
      "翻譯結果: 這個系統的大量部署可能會因其依賴車輛內的相機以及司機知道他們正在被監控而變得昂貴和不實用。我們提出的系統旨在解決先前工作的限制，從擋風玻璃反光問題開始，已經探索了在相機鏡頭上使用偏光濾鏡。訓練過的物體檢測器設計為使用標準視頻監視道路攝像頭，從而限制部署成本。下一節將更詳細地描述我們構建此系統的方法。\n",
      "\n",
      "物體檢測器 骨幹網絡 圖像分辨率\n",
      "YOLOv3 [3] Darknet-53 [3] 320、416、512\n",
      "YOLOv4 [9] CSPDarknet-53 [9] 320、416、512\n",
      "Faster R-CNN [5] Resnet101 [31] 640\n",
      "Faster R-CNN [5] Resnet152 [31] 640\n",
      "Centernet [11] Resnet101 FPN [31] 512\n",
      "SSD [10] Mobilenetv2 FPNLite [32] 640\n",
      "SSD [10] ResNet50 V1 FPN [31] 640\n",
      "SSD [10] ResNet101 V1 FPN [31] 640\n",
      "表I：本文選擇的物體檢測器和預訓練基模型（骨幹網絡），在本工作中進行微調和評估。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: III.A PPROACH\n",
      "Here, we describe the steps taken to build the proposed so-\n",
      "lution of a fully-automated system to detect driver violations.We propose two methods for achieving this: a single-step and\n",
      "a two-step approach (Figure 3), where by step we refer to\n",
      "a dedicated trained model in the overall system architecture.The single-step model is trained to detect both a licence plate\n",
      "and a person using their phone from a single image input\n",
      "in one forward pass through the model.A key advantage of\n",
      "this approach is that running a single model to complete the\n",
      "entire task at once results in a more light-weight faster system.A potential limitation of this method, however, would be the\n",
      "trade-off with accuracy as the trained model will be attempting\n",
      "to detect a very small object within a large image.To remedy this issue, we also propose a two-step solution,\n",
      "which ﬁrst detects the windscreen of the vehicle, and then uses\n",
      "the cropped image of only the driver side as the input for the\n",
      "next step to search for the phone.\n",
      "翻譯結果: III. 做法\n",
      "這裡我們將描述建立提案的全自動系統，以偵測司機違規動作的步驟。我們提出了兩種實現方法：單步和雙步 (圖3)，其中，步驟是指整體系統架構中的專用訓練模型。單步模型受訓練以從單張圖片輸入中同時檢測車牌和使用手機的人。此方法的主要優點在於，僅需運行單一模型即可完成整個任務，因此系統更輕量且速度更快。然而，此方法的一個潛在限制是當受訓練模型試圖在大圖像中檢測非常小的物體時，會牺牲一定的準確性。為解決此問題，我們還提出了雙步方案，先檢測車輛的擋風玻璃，然後使用僅包含駕駛員側面的裁剪圖像作為下一步的輸入以搜索手機的位置。\n",
      "原文: An overview of the process\n",
      "of the two-step approach is seen in Figure 3.To enable a rigorous analysis and provide insight into the\n",
      "requirements of such an automated distracted driver identiﬁca-\n",
      "tions system, both the proposed one-step and two-step models\n",
      "will be evaluated for accuracy and speed.To decide upon the best object detection model, we evaluate\n",
      "four popular frameworks with various backbone models and\n",
      "image input sizes [3], [5], [9], [10].In total, we ﬁne-tune\n",
      "12 pre-trained models with our custom dataset, where we\n",
      "have phone and licence plate for the single-step method,\n",
      "and windscreen and phone for the two-step one.The chosen\n",
      "architectures used in this project are listed in Table I.To train and evaluate the models, images are acquired using\n",
      "high-end (Aviglon2, Axis3) and low-end (ELP4) cameras under\n",
      "varying weather conditions.Details of all equipment used can\n",
      "be found in Table II.Figure 4 demonstrates the difference in\n",
      "quality between a high and low-end camera, where all other\n",
      "conditions are identical.\n",
      "翻譯結果: 兩個步驟方法的流程概述如圖3所示。為了進行嚴謹的分析並提供洞察力，該自動分心駕駛員識別系統的要求，比較評估了提出的單步和兩步模型的準確性和速度。為了決定最好的物體檢測模型，我們評估了四個流行框架和不同的主幹模型和圖像輸入大小 [3]，[5]，[9]，[10]。總共，我們微調了12個預訓練模型，使用我們的自定義數據集，其中對於單步方法我們有手機和車牌，對於兩步方法我們有擋風玻璃和手機。本項目中使用的選定架構列在表I中。為了訓練和評估模型，使用高端（Aviglon2、Axis3）和低端（ELP4）攝像頭在不同的天氣條件下獲取圖像。所有使用的設備的詳細信息可以在表II中找到。圖4演示了高端和低端攝像頭之間質量的差異，其他條件都相同。\n",
      "原文: We discuss the camera and hardware\n",
      "setup in the following section.A.Camera and hardware setup\n",
      "To successfully develop an automated mobile phone use\n",
      "detection system, a practical feasibility study is required to\n",
      "2https://www.avigilon.com/\n",
      "3https://www.axis.com/en-gb\n",
      "4http://www.elpcctv.com/\n",
      "翻譯結果: 我們在下面的章節討論攝影機和硬體的設定。A.攝影機和硬體設定\n",
      "為了成功開發自動化手機使用偵測系統，需要進行實用性可行性研究。\n",
      "2https://www.avigilon.com/\n",
      "3https://www.axis.com/en-gb\n",
      "4http://www.elpcctv.com/\n",
      "原文: Detect \n",
      "windscreen Input Image Crop driver side \n",
      "of windscreenDetect phoneOutput image with \n",
      "overlayYOLOv3 -\n",
      "320 \n",
      "Model 1\n",
      "YOLOv3 -\n",
      "320 \n",
      "Model 2\n",
      "Fig.3: Two-step approach using YOLOv3 with input size of 320 \u0002320.Each input frame resized to 320 \u0002320 then passed\n",
      "through the ﬁrst model to detect windscreen.Image cropped on driver side of the windscreen is then resized to 320 \u0002320.Cropped image is passed through second model to detect phone.Output image is the original frame with overlay of predicted\n",
      "windscreen and phone bounding boxes.(a) High-end camera example.(b) Low-end camera example.Fig.4: Two images taken under the same conditions, using\n",
      "both high-end and low-end cameras.ensure that images of a high enough quality could be captured\n",
      "through a car windscreen in all weather conditions.It is\n",
      "highlighted in the work of Alkan et al.[27] that images\n",
      "captured in certain hours of the day present the challenge\n",
      "of windscreen glare, whilst night-time and poorly-lit areas\n",
      "can result in dark unusable images.\n",
      "翻譯結果: 偵測車窗 輸入圖像 裁剪駕駛員一側的車窗 檢測手機 輸出帶有覆蓋層的圖像 YOLOv3- 320 模型1 YOLOv3- 320 模型2 圖3: 采用320*320的輸入大小使用YOLOv3的兩步法。將每個輸入幀調整為320*320，然後通過第一個模型來檢測車窗。然後在車窗的駕駛員一側裁剪圖像，並調整大小為320*320。剪切圖像通過第二個模型進行檢測手機。輸出圖像是帶有預測的車窗和手機邊界框的原始幀的覆蓋層。(a)高端攝像頭示例。(b)低端攝像頭示例。圖4:使用高端和低端攝像頭在相同條件下拍攝的兩張圖片，確保在所有天氣條件下都可以捕捉到足夠高質量的汽車車窗圖像。Alkan等人的研究強調了一天中特定時間拍攝的圖像存在車窗眩光的挑戰，而夜間和光線不足的區域可能導致黑暗無法使用的圖像。\n",
      "原文: We also acknowledge\n",
      "that tinted windscreens may result in cameras not having\n",
      "reasonable visibility into the vehicle.While this can simply\n",
      "be resolved by having the camera in night-mode and using\n",
      "excessive amounts of IR, this issue is not common in the UK\n",
      "due to legal restrictions, so it is beyond the scope of this work.The primary objective of this paper is to not just create\n",
      "an object detector that could capture phone usage violations,\n",
      "but one that could do this during all hours of the day.In\n",
      "this section, we address the following challenges and propose\n",
      "solutions:\n",
      "1) Windscreen Glare: One of the most difﬁcult challenges\n",
      "when trying to see inside the vehicle is windscreen glare.This will usually occur when the sun is in a particular part\n",
      "Fig.5: Image during sunny/cloudy day without a polarising\n",
      "ﬁlter to show the adverse effects of windscreen reﬂection.(a) Without polarizing ﬁlter.(b) With polarizing ﬁlter.Fig.6: Images passed through YOLOv3-416 object detector\n",
      "to detect a person.Top image without a polarizing ﬁlter is\n",
      "completely unable to detect the person.\n",
      "翻譯結果: 我們也承認，有色濾鏡的擋風玻璃可能會導致攝像頭無法合理地看到車內情況。雖然這個問題可以通過將相機設置在夜間模式並使用大量的紅外線來簡單解決，但是這個問題在英國由於法律限制並不常見，因此超出了本研究的範圍。本文的主要目標不僅是創建一個能夠捕捉手機使用違規行為的對象檢測器，而且是能夠在全天候進行此操作的。在本節中，我們解決以下挑戰並提出解決方案：\n",
      "1）擋風玻璃反光：當試圖看到車內情況時，最困難的挑戰之一是擋風玻璃的反光。這通常會在太陽處於特定位置時發生。\n",
      "圖5：在沒有偏光濾鏡的情況下的晴天/多雲天氣的圖像，以顯示擋風玻璃反射的不良影響。(a)沒有偏光濾鏡。(b)有偏光濾鏡。\n",
      "圖6：通過YOLOv3-416對象檢測器傳遞的圖像，以檢測一個人。頂部的圖像沒有偏光濾鏡，完全無法檢測到人。\n",
      "原文: Bottom image with the\n",
      "polarizing ﬁlter is detecting the person with 91% conﬁdence.of the sky and can be made worse when clouds are present\n",
      "as they can be reﬂected quite signiﬁcantly on the windscreen.An example of how glare and cloud reﬂection can completely\n",
      "obstruct the view into the vehicle from the windscreen can be\n",
      "seen in Figure 5.Our preliminary tests found that the issue of\n",
      "glare would occur during the majority of the day.We can solve this issue by using a polarising ﬁlter which is\n",
      "ﬁxed to the camera lens.The effectiveness of this solution is\n",
      "demonstrated in Figure 6, which shows the same image taken\n",
      "翻譯結果: 下方使用偏光濾鏡的圖像，以91％的信心偵測到了人物。當雲彩存在時，晴天的天空會造成更嚴重的問題，因為它們可以在汽車擋風玻璃上產生相當明顯的反射。如圖5所示，當反光和雲反射完全遮擋住從擋風玻璃進入車內的視線時，這就是這個問題的一個例子。我們的初步測試發現，光的問題會在大部分的日子裡發生。我們可以通過在攝像頭鏡頭上安裝偏光濾鏡來解決這個問題。如圖6所示，這種解決方案的有效性得到了證明，它顯示了拍攝相同圖像的結果。\n",
      "原文: Equipment Make Model number Resolution/Wavelength Lens Origin\n",
      "Camera Avigilon 2.0C-H5A-B1 2MP 4.7 - 84.6mm Canada\n",
      "Camera Axis P1353 1.3MP 5-50mm Sweden\n",
      "Camera ELP ELP-USB-FHD01M-SFV 2MP 5-50mm China\n",
      "Infra-Red Raytec V AR2-i8-1 850nm 10 Degrees UK\n",
      "Infra-Red Raytec V AR2-i8-1-730 730nm 10 Degrees UK\n",
      "TABLE II: List of the cameras, IR and lens setup used in this work.(a) 850nm IR.(b) 730nm IR.Fig.7: Night images with active IR - 850nm vs 730nm.both without and with a polarising ﬁlter.We tested the impact\n",
      "of the polarising ﬁlter on the overall system by running the\n",
      "images through a pre-trained YOLOv3 model to see if it can\n",
      "detect the person inside the vehicle.We see from the images\n",
      "that it cannot detect the person when no ﬁlter is used, but\n",
      "detects the person with 91% conﬁdence when the polarising\n",
      "ﬁlter is added, which points to the importance of including a\n",
      "polarising ﬁlter with the system hardware.2) Low-light conditions: In order for the system to func-\n",
      "tion successfully in low-light conditions, we would need to\n",
      "consider an appropriate light source.\n",
      "翻譯結果: 設備 品牌型號 解析度/波長 鏡頭 產地\n",
      "攝影機 Avigilon 2.0C-H5A-B1 2MP 4.7-84.6mm 加拿大\n",
      "攝影機 Axis P1353 1.3MP 5-50mm 瑞典\n",
      "攝影機 ELP ELP-USB-FHD01M-SFV 2MP 5-50mm 中國\n",
      "紅外線 Raytec V AR2-i8-1 850nm 10度 英國\n",
      "紅外線 Raytec V AR2-i8-1-730 730nm 10度 英國\n",
      "表格二：本實驗使用的攝影機，紅外線和鏡頭設置列表。(a) 850nm紅外線。(b) 730nm紅外線。圖7：活躍紅外線下的夜間影像-850nm vs 730nm。使用和不使用偏光濾鏡。我們通過運行預先訓練好的YOLOv3模型來測試偏光濾鏡對整個系統的影響，以查看它是否能夠檢測車輛內的人員。我們可以從圖像中看出，在不使用濾鏡的情況下，它無法檢測到人員，但在添加偏光濾鏡後，可以以91%的置信度檢測到人員，這指出了包括偏光濾鏡在內的系統硬件的重要性。2）低光照條件：為了使系統在低光照條件下成功運作，我們需要考慮適當的光源。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: Directional white visible\n",
      "light is not used with the camera as it is considered to be a risk\n",
      "of causing glare to the driver.Instead, active Infra Red (IR)5is\n",
      "used, where two different wavelengths are tested: 850 and 730\n",
      "nanometers (nm).Figure 7 demonstrates both wavelengths of\n",
      "IR working well in low-light conditions, however it is clear\n",
      "that the camera is able to capture more details using 730nm.Details of the IR used for this work can be found in Table II.It is noted that IR can also be used during the day with the\n",
      "camera in monochrome setting, though more IR is required\n",
      "due to the higher ambient light levels.Further studies would be\n",
      "useful in determining the optimal IR power, typically measured\n",
      "in\u0016W/cm2, but this is outside the scope of this work.The deployed system will be designed to expect both\n",
      "RGB (day) and monochrome images (night).A proportion\n",
      "of monochrome images taken with IR have been used in the\n",
      "training and testing of the model.5https://www.rayteccctv.comB.Dataset\n",
      "This section describes the custom dataset used to train and\n",
      "test the models used in the proposed system.\n",
      "翻譯結果: 在此相機中不會使用方向性的白光可見光，因為它被認為有可能造成駕駛者的眩光危險。相反，使用主動式紅外線(IR)，經過測試有兩種不同波長：850納米和730納米。圖7展示了這兩種波長的IR在低光條件下的良好效果，然而清楚地看出，相機使用730納米時能夠捕捉到更多的細節。有關此工作中使用的IR的詳細信息可在表II中找到。需注意的是，即使在白天，相機也可以使用黑白模式設置下的IR，但需要更多的IR，以應對更高的環境光水平。進一步的研究將有助於確定最佳的IR功率，通常以W/cm2為測量單位，但這超出了此工作的範圍。部署系統將設計為可接受RGB（白天）和黑白影像（夜晚）。使用IR拍攝的黑白影像比例已用於模型的訓練和測試中。由自定義數據集訓練和測試所使用的模型在本節中說明。\n",
      "原文: 1) Training images: For the single-step approach, the\n",
      "dataset consists of 2,150 images of phone and 2,235 images\n",
      "of licence plates.The licence plate images are obtained from\n",
      "the Google Open Images Dataset [33].For the phone class, we\n",
      "used a small portion of mobile phone stock images in order\n",
      "to pick up simple detections.However, the main proportion\n",
      "of images would need to be obtained/created speciﬁcally for\n",
      "this project with a mixture of quality, weather conditions,\n",
      "and distance to best represent the real-world scenarios (Figure\n",
      "8).Despite the class being labeled “phone”, the majority of\n",
      "what is actually being detected is multiple variations of hand\n",
      "positions holding a phone.In order to ensure that our model\n",
      "is trained to detect these types of images, 1,700 of the 2,150\n",
      "phone images are obtained speciﬁcally for this project.The two-step approach required obtaining 263 images of ve-\n",
      "hicle windscreens, with some of these images having multiple\n",
      "vehicles, so the total number of windscreens annotated is 487.\n",
      "翻譯結果: 1) 訓練圖像：對於單步驟方法，數據集包括 2,150 張手機圖像和 2,235 張汽車牌照圖像。汽車牌照圖像是從 Google 開放圖像數據集 [33] 獲取的。對於手機類別，我們使用了一小部分手機庫存圖像，以便進行簡單的檢測。然而，大部分圖像需要特別為此項目獲取/創建，以混合品質、天氣條件和距離來最好地表示真實世界的情況 (圖 8)。儘管該類別被標記為“手機”，但實際上被檢測的大部分內容是手持手機的多個變化。為了確保我們的模型訓練能夠檢測這些類型的圖像，2,150 張手機圖像中的 1,700 張是專門為此項目獲取的。雙步驟方法需要獲取263張車輛擋風玻璃圖像，其中部分圖像有多輛車，因此標註的總風擋玻璃數量為487。\n",
      "原文: These images are used to train the ﬁrst model in the two-step\n",
      "pipeline, which will be used to crop the vehicle windscreens.The second model is trained using only the phone images.2) Test images: The images used to evaluate the trained\n",
      "models consist of 216 images of a person using their phone\n",
      "whilst driving.It is important to note that due to there being no\n",
      "access to public trafﬁc camera footage, the test images were\n",
      "taken with the aid of volunteers to best represent that of a\n",
      "real-world application.These were obtained using a mixture\n",
      "of high-end and low-end cameras (Table II).The videos used\n",
      "to obtain these test images were not used in the training of the\n",
      "object detectors.Test images for the two-step approach were\n",
      "obtained by cropping out only the windscreen of these same\n",
      "images.C. Training the object detectors\n",
      "One of the main challenges for successfully detecting a\n",
      "person using their phone is the ability for the system to\n",
      "detect small objects with signiﬁcant variation.To address this,\n",
      "multiple object detection methods are trained and evaluated.\n",
      "翻譯結果: 這些圖像用於訓練兩步驟流程中的第一個模型，該模型將用於裁剪車輛擋風玻璃。第二個模型僅使用手機圖像進行訓練。\n",
      "2）測試圖像：用於評估訓練模型的圖像包括216張人們開車時使用手機的圖像。值得注意的是，由於沒有公共交通攝像頭的訪問權限，測試圖像是在志願者幫助下獲取的，以最佳地代表現實世界應用程序。這些圖像是使用高端和低端相機的混合方式獲取的（表II）。用於獲取這些測試圖像的視頻並未用於對象檢測的訓練。兩步驟方法的測試圖像是從這些相同圖像中僅裁剪出擋風玻璃。 \n",
      "C. 訓練對象檢測器\n",
      "成功檢測使用手機的人的主要挑戰之一是系統檢測具有顯著變化的小物體的能力。為了解決這個問題，訓練並評估了多個對象檢測方法。\n",
      "原文: The same trained models are used to evaluate both the single-\n",
      "step and two-step approaches.A variety of pre-trained base networks (backbones) can be\n",
      "chosen depending on the object detector used, such as ResNet\n",
      "[31], VGG16 [29], Inception [34] and MobileNet [32].YOLO\n",
      "typically uses Darknet53 (YOLOv3) [8] and CSPDarknet53\n",
      "(YOLOv4) [9].All these base networks with the exception\n",
      "of MobileNet, are typically used when running on a GPU\n",
      "翻譯結果: 相同的訓練模型被用於評估單步驟和兩步驟方法。根據使用的物體檢測器，可以選擇各種預訓練基礎網絡（骨幹）例如ResNet[31]、VGG16[29]、Inception[34]和MobileNet[32]，YOLO通常使用Darknet53(YOLOv3)[8]和CSPDarknet53(YOLOv4)[9]。除了MobileNet之外，所有這些基礎網絡通常在GPU上運行。\n",
      "原文: Fig.8: Bespoke RGB and monochrome training images of varying quality obtained speciﬁcally for this work.platform [10].Here, we choose to include a MobileNet base\n",
      "network as one of our trained models to see how effective a\n",
      "low-cost light-weight detector could preform.An application\n",
      "such as this may beneﬁt from running on an edge device\n",
      "[35] where a more light-weight model optimised for smaller\n",
      "computational resources would be preferred.1) YOLOv3 and YOLOv4: Results from other studies [8],\n",
      "[9] concluded that the accuracy on the higher resolutions\n",
      "would yield greater accuracy, and similarly that the lower\n",
      "resolution models would run at a higher frame rate.Based\n",
      "on this, we opt for training both YOLOv3 and YOLOv4 with\n",
      "input resolutions of 512 \u0002512, 416 \u0002416 and 320 \u0002320.For the\n",
      "two-step approach, we would like to see if accuracy would\n",
      "be impacted much on the lower resolution models dealing\n",
      "with: uniform objects such as windscreens, and low resolution\n",
      "cropped images taken from the windscreen.\n",
      "翻譯結果: 圖8：專為本研究製作的RGB和單色訓練圖像，品質各異。在這裡，我們選擇在訓練模型中包含MobileNet基礎網絡，以觀察低成本輕量級檢測器的效果。這樣的應用可能受益於在邊緣設備上運行，因為那裡更適合針對較小計算資源優化的輕量級模型[35]。1）YOLOv3和YOLOv4：其他研究的結果[8]，[9]得出了在更高分辨率下的準確性會產生更高的準確性，同樣地，低分辨率模型會運行在更高的幀速率。基於此，我們選擇以512 x 512，416 x 416和320 x 320的輸入分辨率來訓練YOLOv3和YOLOv4。對於兩步驟方法，我們希望看到處理：像擋風玻璃這樣的均勻物體以及從擋風玻璃裁剪的低分辨率圖像的低分辨率模型是否會受到準確性的影響。\n",
      "原文: 2) Faster R-CNN, SSD, and Centernet: The remaining\n",
      "frameworks are obtained and ﬁne-tuned using the TensorFlow\n",
      "Object Detection API [36], [37], this give us access to many\n",
      "different pre-trained models that could be ﬁne-tune on our\n",
      "custom dataset.D. Evaluating the models\n",
      "To replicate a real-world scenario, the majority of the test\n",
      "images are obtained at a distance of 20-30m from the camera,\n",
      "with a height of approximately 3m.These images were cap-\n",
      "tured during different times of the day under varying weather\n",
      "conditions to enable testing the generalisation capabilities of\n",
      "the system as well as its predictive performance.Test images for evaluation are split into 2 sets, the ﬁrst\n",
      "using 216 full image snapshots taken from the camera, the\n",
      "next with the same images but this time with only the cropped\n",
      "windscreen.This allows us to determine if the model will\n",
      "perform more favourably with the single-step or the two-step\n",
      "system.IoU = ~0.2 IoU = ~0.8False Positive True Positive \n",
      "Fig.9: Example of False Positive (left) and True Positive\n",
      "(right) when IoU threshold set to >0.5.\n",
      "翻譯結果: 2) Faster R-CNN、SSD 及 Centernet：其餘的架構使用 TensorFlow Object Detection API [36]、[37] 獲得並進行微調，這讓我們可以存取許多不同的預訓練模型，可在我們的自訂資料集上進行微調評估模型。\n",
      "\n",
      "D. 評估模型：\n",
      "為了模擬真實世界的情況，大部分的測試影像是從攝影機距離 20-30 公尺處，高度約 3 公尺捕獲得到的。這些影像在不同天氣條件和不同的時間捕捉，使系統具有泛化能力和預測性能進行測試。進行評估的測試影像分為兩組，第一組使用攝影機拍攝的 216 張完整影像，接下來是相同的影像，但這次只使用裁剪的擋風玻璃。這讓我們可以確定該模型是使用單階段還是雙階段系統進行更有利的表現。\n",
      "\n",
      "IoU = ~0.2 IoU = ~0.8 假正向 真正向\n",
      "圖 9：當 IoU 閾值設置為 >0.5 時，假正向（左）和真正向（右）的示例。\n",
      "原文: Green bounding box\n",
      "represents the ground truth, red bounding box is the prediction.1) Evaluation Metrics: A good metric to evaluate object\n",
      "detectors is Mean Average Precision (mAP) [13].However,\n",
      "as we are only concerned with the accuracy of a single class\n",
      "(Phone), we evaluate the approach using Average Precision\n",
      "(AP) [12].The ﬁrst part of this process is collecting the\n",
      "sequences of True Positives (TP) and False Positives (FP) from\n",
      "the predictions made on the test images.In object detection, a\n",
      "TP is determined by the value of the Intersection Over Union\n",
      "(IoU).For example, if the minimum IoU requirement is >0.5\n",
      "(often referred to as mAP 50orAP50, where any predictions\n",
      "with IoU above this threshold are classed as TP and any below\n",
      "are FP [38] (Figure 9).Once these have been collected, they\n",
      "are sorted in descending order by the conﬁdence score, then\n",
      "precision and recall values are calculated using Equation 1.Precision =TP\n",
      "TP+FPRecall =TP\n",
      "TP+FN(1)\n",
      "The AP summarises the shape of the precision/recall curve,\n",
      "and is deﬁned as the mean precision at a set of eleven\n",
      "equally spaced recall levels [0, 0.1,..., 1] [12] (Equation 2).\n",
      "翻譯結果: 綠色框框代表真實範圍，紅色框框則代表預測範圍。1）評估指標：評估物體偵測的好指標是平均精度平均值（mean Average Precision, 簡稱 mAP）[13]。不過，因為我們僅關心單一類別（手機）的準確度，所以我們使用平均精度（Average Precision, 簡稱 AP）[12] 來評估方法。此過程的第一部分是從測試圖像上的預測中收集真正正面(True Positives, TP)和假正面(False Positives, FP) 的序列。在物體偵測中，TP是由交集過聯集(IoU)值來決定的。例如，如果最低的IoU要求是>0.5（通常被稱為 mAP 50或AP50，任何IoU在此門檻之上的預測都被視為 TP，而任何IoU在此門檻之下的則被視為FP [38] (圖9)。收集完TP和FP後，它們會按照置信分數按降序排序，接著使用公式1計算精度和召回率（Recall）的值。精度 = TP/ (TP+FP)，召回率 = TP/ (TP+FN) (1)。AP總結了精度/召回曲線的形狀，並被定義為一組11個等距召回率水平[0, 0.1，...，1]的平均精度[12]（公式2）。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: The precision at each recall level ris interpolated by taking\n",
      "the maximum precision measured for a method for which the\n",
      "corresponding recall exceeds r[12] (Equation 2).In this work, we evaluate our proposed approach using\n",
      "both IoU>0.5 and IoU >0.1, hereby referred to as AP50\n",
      "andAP10respectively.We are attempting to ﬁnd very small\n",
      "objects (mobile phone) with signiﬁcant variation, so the lower\n",
      "IoU threshold would likely be more appropriate.To test the\n",
      "accuracy of our trained models, all the test images are supplied\n",
      "with given annotation ﬁles (ground truth bounding boxes).We\n",
      "then run these images through the object detectors to obtain the\n",
      "predicted bounding boxes.The AP is subsequently calculated\n",
      "based on both IoU thresholds.Next, we discuss evaluating the\n",
      "efﬁciency of the proposed approach.AP=1\n",
      "11X\n",
      "r2f0;0:1;0::;0:9;1gPinterp(r)\n",
      "wherePinterp(r)= max\n",
      "~r:~r\u0014rp(~r)(2)\n",
      "2) Evaluating frame rate: The system has been designed\n",
      "around the ability to take live video from trafﬁc cameras.It is\n",
      "therefore important that it not only detects with high accuracy,\n",
      "but with low latency, especially when dealing with moving\n",
      "vehicles.\n",
      "翻譯結果: 在每個召回水平上的精確度是通過插值得到的，方法是採取相應的召回超過 r[12] 的方法測量的最大精確度（公式2）。在本研究中，我們使用IoU>0.5和IoU>0.1進行了我們提出的方法評估，分別稱為AP50和AP10。我們試圖找到變化很大的非常小的物體（手機），因此較低的IoU閾值可能更適合。為了測試訓練模型的準確性，所有測試圖像都附帶有給定的標註文件（ground truth bounding boxes）。然後，我們通過物體檢測器運行這些圖像以獲取預測的邊界框。根據兩個IoU閾值計算AP。接下來，我們討論評估所提出方法效率的方法。\n",
      "\n",
      "AP=1 \n",
      "11X\n",
      "r2f0;0:1;0::;0:9;1gPinterp(r)\n",
      "其中Pinterp（r）= max\n",
      "~r：~r\u0014rp（~r）（公式2）\n",
      "\n",
      "2）評估幀速率：系統的設計是圍繞著從交通攝像機拍攝現場視頻進行的。因此，高精度和低延遲是非常重要的，特別是在處理移動車輛時。\n",
      "原文: Other studies conﬁrm that AP alone is not enough to\n",
      "evaluate object detectors, particularly when it comes to video\n",
      "object detection [39].Consequently, frames per second (FPS)\n",
      "is another metric used to evaluate the proposed system.To\n",
      "翻譯結果: 其他研究也證實，僅使用平均精度（AP）來評估物體偵測器是不夠的，特別是在視頻物體檢測方面[39]。因此，每秒幀數（FPS）是另一個用來評估所提出系統的指標。\n",
      "原文: test speed of our trained object detectors, we run the same\n",
      "test video for each model and then calculate the average FPS.3) Choosing the best model: Once these metrics are cal-\n",
      "culated for each of the models, we then shortlist the top\n",
      "two for further evaluation.The test images are split into two\n",
      "categories, namely high-quality and low-quality.High-quality\n",
      "images are captured using the high-end cameras (Avigilon and\n",
      "Axis) to represent how the model should perform when the\n",
      "system has been built with a relatively larger budget in mind.The low-quality images are acquired using the low-end camera\n",
      "(ELP) to demonstrate how the model will perform under cost-\n",
      "effective considerations.Details of the cameras are listed in\n",
      "Table II.The speed in which these models perform also needs\n",
      "to be re-evaluated to incorporate the two-step windscreen\n",
      "method as well as the object tracking algorithm.4) Object tracking and data collection: A consideration\n",
      "when building a fully-automated system is how the phone\n",
      "violations are going to be recorded in a way that is useful\n",
      "to the end user.\n",
      "翻譯結果: 測試訓練好的物件檢測器的速度，我們對每個模型運行同一個測試視頻，然後計算平均FPS。3) 選擇最佳模型：一旦計算了每個模型的這些指標，我們就會為進一步評估選出前兩個。測試圖像分為高質量和低質量兩類。使用高端攝像機（Avigilon和Axis）捕捉高質量圖像，以展示當系統的建設相對較大的預算考慮時，模型應該如何運作。使用低端攝像機（ELP）獲取低質量圖像，以展示當系統建設成本效益考慮時，模型的表現如何。攝像機的詳細信息列在表II中。這些模型運行的速度還需要重新評估，以納入兩步玻璃窗口方法以及物件跟踪算法。4）物件跟踪和數據收集：建立全自動系統時需要考慮的一個問題是，手機違規行為將如何被記錄並以對最終用戶有用的方式呈現。\n",
      "原文: To do this, the system would have to be\n",
      "able to distinguish between unique and duplicate detection.For example, a ﬁve second video may show one driver using\n",
      "their phone, but since the detections are done per frame, it\n",
      "may count duplicate violations for every one of these frames.In order to address this, we add DeepSort [14], which is an\n",
      "object tracking algorithm.This will add a unique ID for each\n",
      "detection and then takes each frame to predict if the next\n",
      "detection belongs to the same ID or not.In case of the single-\n",
      "step system, we can check every new detection to see if the ID\n",
      "has been seen before, then if not, can log as a new detection.For the two-step system, a phone violation is only logged once\n",
      "per unique windscreen ID.This same method also allows us\n",
      "to count the number of vehicles, taken from the licence plate\n",
      "on the single-step method and windscreen on the two-step.IV.E XPERIMENTAL RESULTS\n",
      "In this section, we evaluate our models using the experimen-\n",
      "tal setup and the metrics discussed in the previous section.\n",
      "翻譯結果: 為了達到這個目的，系統需要能夠區分唯一的和重複的偵測。例如，一個 5 秒的視頻可能顯示一個駕駛使用他們的手機，但由於偵測是逐幀進行的，這可能會對每個幀計算重複違規。為了解決這個問題，我們添加了 DeepSort [14]，這是一種對象跟踪算法。它會為每個偵測添加一個唯一的 ID，然後對每個幀進行預測，以判斷下一個偵測是否屬於同一個 ID。對於單步系統，我們可以檢查每個新的偵測，以查看是否之前已經見過該 ID，如果沒有，可以將其記錄為新的偵測。對於雙步系統，每個唯一風擋 ID 只記錄一次手機違規。同樣的方法也允許我們從單步方法的車牌和雙步方法的風擋中計算車輛數量。在本節中，我們使用實驗設置和上一節中討論的指標來評估我們的模型。\n",
      "原文: A.System Speciﬁcation\n",
      "Trained object detectors are tested and evaluated using the\n",
      "system speciﬁcation listed in Table III.Type Spec\n",
      "Processor AMD Ryzen 7 3800X\n",
      "GPU Nvidia RTX2080Ti\n",
      "Memory 32GB\n",
      "Operating System Windows 10\n",
      "Programming Language Python 3.8\n",
      "Machine Learning Platform TensorFlow 2.2\n",
      "TABLE III: System Speciﬁcations.B.Average precision\n",
      "For the ﬁrst accuracy test, we present the single-step method\n",
      "where the full test image is used in the model trained to detect\n",
      "the phone.Figure 10 shows the YOLO models outperforming\n",
      "the other object detectors.YOLOv4 with input size 512 is bestperforming on both AP50andAP10, whilst the SSD models\n",
      "are the poorest performing for both IOU thresholds.Next, we test the same models, but this time with the\n",
      "cropped windscreen images which will allow us to determine\n",
      "whether the two-step approach is more appropriate.Figure\n",
      "10 suggests that if accuracy was the main driver, the two-\n",
      "step method will be more favourable to use, giving higher\n",
      "AP in almost all of the trained object detectors.\n",
      "翻譯結果: A. 系統規格\n",
      "經過訓練的物體偵測器使用的系統規格如表 III 所列。\n",
      "型號規格\n",
      "處理器：AMD Ryzen 7 3800X\n",
      "顯示卡：Nvidia RTX2080Ti\n",
      "記憶體：32GB\n",
      "作業系統：Windows 10\n",
      "程式語言：Python 3.8\n",
      "機器學習平台：TensorFlow 2.2\n",
      "表 III：系統規格\n",
      "\n",
      "B. 平均精度\n",
      "在第一次準確度測試中，我們提供了單步驟方法，即在已訓練的模型中使用完整的測試圖像來偵測手機。圖 10 顯示 YOLO 模型的表現優於其他物體偵測器。在 AP50 和 AP10 上，輸入大小為 512 的 YOLOv4 表現最佳，而 SSD 模型則在兩種 IOU 閾值下表現最差。接下來，我們測試了相同的模型，但這次使用切割的擋風玻璃圖像來確定兩步驟方法是否更適合。圖 10 表明，如果準確度是主要的驅動因素，那麼兩步驟方法將更有利於使用，因為幾乎所有已訓練物體偵測器的 AP 都更高。\n",
      "原文: Again, the\n",
      "YOLO models yield the highest accuracy scores.YOLOv3\n",
      "with the larger input size gives the best results with AP50,\n",
      "whilst YOLOv4 with the input resolution of 416 gives the\n",
      "highest accuracy for AP10.Once again, accuracy for all\n",
      "3 SSD frameworks are the lowest by a signiﬁcant margin.Another study [10] points out that, in SSD object detectors,\n",
      "regardless of which base network you use, it will still retain\n",
      "the original characteristics of the SSD.Therefore, accuracy\n",
      "on small objects will not be as accurate compared to other\n",
      "two-stage models such as R-CNN.Review of the predictions made on the test images when\n",
      "IOU threshold is set to >0.5 shows multiple false positives\n",
      "despite the predicted bounding box surrounding the correct\n",
      "object.As mentioned previously, the objects that the model\n",
      "is attempting to predict have signiﬁcant amounts of variation,\n",
      "meaning that it will always be difﬁcult to get a high IOU\n",
      "score.Figure 12 shows a false positive prediction where we\n",
      "have the IOU threshold set to >0.5.\n",
      "翻譯結果: 再次證實，YOLO模型具有最高的準確度得分。在AP50方面，輸入大小較大的YOLOv3結果最佳，而輸入分辨率為416的YOLOv4則在AP10方面具有最高的準確度。相比之下，所有三個SSD框架的準確度都顯著低於其他兩階段模型。另外一項研究[10]指出，在SSD目標檢測器中，無論使用哪種基本網絡，它仍然會保留SSD的原始特徵。因此，在小物體上的準確度不如其他兩階段模型（如R-CNN）。當IOU閾值設置為>0.5時，對測試圖像進行預測的回顧顯示多個假陽性，儘管預測的邊界框圍繞著正確的物體。如先前所述，模型嘗試預測的物體具有顯著的變化量，這意味著要獲得高IOU分數總是很困難的。圖12展示了當IOU閾值設置為>0.5時的一個假陽性預測。\n",
      "原文: We can see that the model\n",
      "is capturing the violation correctly, but narrowly missing the\n",
      "IOU threshold resulting in a false positive.Based on this, we\n",
      "propose an IOU threshold of >0.1 for this application.C. Frame rate\n",
      "Speed of the trained models is evaluated using a 70-second\n",
      "test video.These initial frame rate evaluation tests are done\n",
      "on detection only prior to adding the tracking algorithm and\n",
      "the two-step approach.As seen in Figure 11, the single-stage (YOLO, SSD, Cen-\n",
      "terNet) object detectors are signiﬁcantly faster than the two-\n",
      "stage R-CNN detectors.As expected, the most efﬁcient object\n",
      "detector is the low-cost SSD Mobilenet FPNLite 640 with\n",
      "43 FPS, however lacking in accuracy.Each of the YOLO\n",
      "models seem to perform consistently well with regards to both\n",
      "accuracy and speed with YOLOv3 320 performing the best at\n",
      "29 FPS.D. Output images\n",
      "Figure 13 shows a number of sample results obtained\n",
      "from the YOLOv3-320 model for the two-step method; blue\n",
      "bounding box refers to the ground truth, green is the true\n",
      "positive prediction, and red is the false positive perdition.\n",
      "翻譯結果: 我們可以看到，該模型成功捕捉到違規情況，但僅僅錯失IOU閾值，導致了假陽性的發生。基於此，我們建議對於這個應用範圍設定IOU閾值>0.1。C. 帧速率\n",
      "使用一個70秒的測試視頻評估訓練模型的速度。這些初始幀速率評估測試是在添加跟踪算法以及兩步驟方法之前僅對檢測進行的。\n",
      "如圖11所示，單階段（YOLO，SSD，CenterNet）物體檢測器比兩階段的RCNN檢測器快得多。如預期的那樣，低成本的SSD Mobilenet FPNLite 640是最高效的物體檢測器，其幀率達到了43 FPS，但缺乏準確性。每個YOLO模型在準確性和速度方面都表現出色，每秒29幀的YOLOv3 320表現最佳。D. 輸出圖片\n",
      "圖13展示了使用YOLOv3-320模型進行兩步驟方法的一些樣本結果；藍色邊界框是真實值，綠色是真實陽性預測，紅色是假陽性預測。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: With regards to predictions made on the low-quality camera,\n",
      "although the model performs well, there appears to be a higher\n",
      "chance of false positive predictions.Having a high proportion of false positives for this particular\n",
      "application could result in incorrectly ﬁning individuals, or\n",
      "involve human intervention to manually go through all the\n",
      "violations, which is both costly and defeats the main purpose\n",
      "翻譯結果: 針對在低品質相機下進行預測的情況，儘管模型表現良好，但似乎存在更高的虛警預測機率。對於這個特定應用程序，高比例的虛警可能會導致錯誤罰款個人，或需要人為介入手動審查所有違規行為，這既耗費成本，也使主要目的失敗。\n",
      "原文: Object detector AP50 AP10 AP50cropped AP10cropped FPS (detection)\n",
      "YOLOv4 512 45.98 73.11 59.62 83.32 27.12\n",
      "YOLOv3 512 35.81 46.16 63.27 85.88 25.18\n",
      "YOLOv4 416 40.23 48.23 61.60 87.58 26.12\n",
      "YOLOv3 416 37.41 51.64 58.44 79.99 25.96\n",
      "YOLOv4 320 19.62 52.36 52.65 81.93 26.05\n",
      "YOLOv3 320 37.54 72.45 59.05 84.62 28.97\n",
      "Centernet ResNet101 512 43.81 56.18 50.04 66.6 35.9\n",
      "Faster R-CNN ResNet101 640 37.18 48.57 41.78 55.5 15.4\n",
      "Faster R-CNN ResNet152 640 34.85 44.48 40.89 50.57 11.5\n",
      "SSD Mobilenet FPNLite 640 12.23 18.23 12.64 28.41 42.77\n",
      "SSD ResNet50 FPN 640 2.77 7.77 0.92 4.22 23.66\n",
      "SSD ResNet101 FPN 640 0.6 4.86 0.71 13.9 22.2\n",
      "TABLE IV: Results of all the trained models showing average precision and frames per second.Cropped refers to images of\n",
      "the windscreen only to evaluate performance of the two-step approach.the frame per second (FPS) metric is based on detection\n",
      "only and does not include tracking and the two-step approach.(a) Single-step approach.(b) Two-step approach.Fig.10: Average precision for both IoU thresholds of 0.5 and 0.1 - single-step (left) & two-step (right).\n",
      "翻譯結果: 物件偵測器 AP50 AP10 AP50剪裁版 AP10剪裁版 FPS（偵測）\n",
      "YOLOv4 512 45.98 73.11 59.62 83.32 27.12\n",
      "YOLOv3 512 35.81 46.16 63.27 85.88 25.18\n",
      "YOLOv4 416 40.23 48.23 61.60 87.58 26.12\n",
      "YOLOv3 416 37.41 51.64 58.44 79.99 25.96\n",
      "YOLOv4 320 19.62 52.36 52.65 81.93 26.05\n",
      "YOLOv3 320 37.54 72.45 59.05 84.62 28.97\n",
      "Centernet ResNet101 512 43.81 56.18 50.04 66.6 35.9\n",
      "Faster R-CNN ResNet101 640 37.18 48.57 41.78 55.5 15.4\n",
      "Faster R-CNN ResNet152 640 34.85 44.48 40.89 50.57 11.5\n",
      "SSD Mobilenet FPNLite 640 12.23 18.23 12.64 28.41 42.77\n",
      "SSD ResNet50 FPN 640 2.77 7.77 0.92 4.22 23.66\n",
      "SSD ResNet101 FPN 640 0.6 4.86 0.71 13.9 22.2\n",
      "表格IV：所有訓練模型的平均精度和每秒幀數的結果。剪裁表示僅對擋風玻璃的圖像進行剪裁以評估兩步法的性能。每秒幀數（FPS）指檢測的性能，不包括跟踪和兩步策略。（a）單步策略。（b）兩步策略。圖10：0.5和0.1的兩個IoU閾值的平均精度 - 單步策略（左）和兩步策略（右）。\n",
      "原文: Fig.11: FPS for detection only for the trained object detectors.Fig.12: False positive result for AP50, which demonstrates\n",
      "that for the application in this work, AP10is more appropriate.of this work, which is to automate the pipeline.For the\n",
      "ﬁnal deployed model, it may be appropriate to increase the\n",
      "score threshold for the phone detector step to reduce these\n",
      "false positives.Observation from the predictions of the test\n",
      "set conﬁrm that AP10is appropriate for this application of\n",
      "detecting such small and difﬁcult images, as demonstrated in\n",
      "Figure 12.Based on the results shown in Table IV, the model chosen\n",
      "for deployment is YOLOv3 with the input size of 320 using\n",
      "the two-step method.Although it did not achieve the highest\n",
      "翻譯結果: 的Fig.11：只针对训练过的物体检测器进行检测的FPS。Fig.12：AP50 的假阳性结果，证明在本文的应用中，AP10 更合适。这项工作的目标是自动化管道。对于最终部署的模型，可能需要增加手机检测步骤的分数阈值，以减少这些假阳性。测试集预测的观察结果确认，在检测这种小而复杂图像的应用中，AP10是适当的，如图12所示。基于表格IV中展示的结果，选择用输入大小为320的YOLOv3模型和两步法进行部署。尽管没有达到最高水平\n",
      "原文: Fig.13: Results from YOLOv3 320 with AP10showing high\n",
      "accuracy but high false positives on low-quality images.Blue\n",
      "bounding boxes denote ground truth, green refers to the true\n",
      "positive prediction and red is the false positive perdition.overall accuracy, it came a close third behind YOLOv3-512\n",
      "and YOLOv4-416.The deciding factor is the speed of the\n",
      "model, as it is able to achieve almost 29 FPS (almost 11%\n",
      "faster then the next best performing model).For this type of\n",
      "application, the cameras typically monitors fast-moving trafﬁc.The model should consequently be able to make detections\n",
      "efﬁciently.Having a model with a smaller input size means it\n",
      "will be less expensive with regards to hardware demands.E. Integrating the two-step method and tracking\n",
      "The next stage in our overall system is to re-train the chosen\n",
      "YOLOv3-320 model and modify the code for the two-step\n",
      "approach, where ﬁrst step is trained to detect the windscreen\n",
      "and the next step trained on only the phone images.The\n",
      "DeepSort [14] tracking algorithm is also integrated into this\n",
      "system.\n",
      "翻譯結果: 圖13：使用AP10的YOLOv3 320的結果顯示對於低質量圖像具有高準確度但高假陽性率。藍色邊界框表示真實值，綠色是真陽性預測，紅色是假陽性預測。總體精度表現接近第三而被YOLOv3-512和YOLOv4-416超越。重要因素是模型的速度，它能夠達到近29FPS（比下一個表現最佳的模型快約11％）。對於此類應用，相機通常監測快速移動的交通，因此模型應能有效地進行檢測。擁有較小的輸入大小的模型意味著在硬件要求方面更加經濟實惠。E. 整合兩步驟法與追蹤\n",
      "我們整體系統的下一步是重新訓練選定的YOLOv3-320模型並修改兩步驟方法的代碼，其中第一步驟訓練以檢測擋風玻璃，下一步訓練僅針對手機圖像進行。DeepSort [14]跟踪算法也集成到此系統中。\n",
      "原文: The frame rate is recalculated on the same video, to\n",
      "show the impact of the tracking algorithm and the two-step\n",
      "approach on system efﬁciency.Table V shows that the tracking\n",
      "algorithm reduces FPS by almost 10%, whilst adding the extra\n",
      "step on top of this sees a further reduction of \u001850% giving a\n",
      "frame rate of 13.15 FPS on the YOLOv3-320 model.For the ﬁnal benchmark tests, we split the test images\n",
      "into 2 categories; high-quality and low-quality, with 116 and\n",
      "100 images respectively.Based on our chosen metric of IOU\n",
      "threshold greater than 0.1, for YOLOv3-320, we can achieve\n",
      "an AP of as high as 95.81% on the images taken with only\n",
      "high-quality cameras, whilst still achieving an AP of 74.36%\n",
      "on images taken from the low-quality camera (Table VI).V. D ISCUSSIONS AND FUTURE WORK\n",
      "Our proposed approach delivers very promising results and\n",
      "further enables a fully-automated end-to-end surveillance sys-\n",
      "tem capable of capturing mobile use violations while driving.However, there are still limitations that need to be addressed\n",
      "before tangible impact can be made.\n",
      "翻譯結果: 幀率是在同一個視頻上重新計算的，以展示追蹤算法和兩步法對系統效率的影響。表V顯示，追蹤算法降低了近10%的FPS，而在此基礎上增加額外的步驟則進一步減少了約50%，在YOLOv3-320模型上的幀率為13.15 FPS。對於最終的基準測試，我們將測試影像分成2類：高質量和低質量，分別為116張和100張影像。根據我們選擇的IOU閾值大於0.1的評估指標，在僅使用高質量攝像頭拍攝的影像中，對於YOLOv3-320，我們可以實現高達95.81%的AP，同時在從低質量攝像頭拍攝的影像中仍然可以實現74.36%的AP（表VI）。 \n",
      "\n",
      "V. 討論和未來工作\n",
      "\n",
      "我們提出的方法提供了非常有前途的結果，進一步實現了一個完全自動化的端到端監視系統，能夠捕捉行駛中的移動使用違例。然而，在實現有實質影響之前還需要解決一些限制。\n",
      "原文: The proposed two-step model detects the driver side of the\n",
      "windscreen based on right-hand drive vehicles.When deployed\n",
      "in countries using left-hand drive vehicles, we can simplycrop the opposite side.Alternatively, even this process can be\n",
      "automated by detecting the licence plate to identify country\n",
      "and determine which side is the driver.Section III-B addresses not having access to public roadside\n",
      "cameras, so next step would be to deploy the system with\n",
      "support from local authority/police.Test parameters of this\n",
      "work have been based on 3m mounting height with a distance\n",
      "of 25-30m from the subject, meaning that phone could be\n",
      "hidden when texting close to lap.This could be remedied when\n",
      "deployed on a public road by utilizing a gantry trafﬁc camera\n",
      "which allows for a better view within the vehicle.Although, the two-stage approach achieves a greater ac-\n",
      "curacy, there is a compromise with frame rate.We propose\n",
      "optimizing the model and exploring TensorRT [40] framework\n",
      "to potentially improve the speed of the model.\n",
      "翻譯結果: 本提案的兩步驟模型是基於右駕車輛來偵測駕駛座前方的擋風玻璃。如果應用在左駕車國家，我們可以簡單地裁切相反的那一側。或者，透過偵測車牌以識別國家並確定駕駛席位置，甚至可以自動化此過程。第三節的B部分討論了沒有公共路邊攝影機的情況，因此下一步是在當地當局/警方的支持下部署系統。本研究的測試參數基於距離主體約25-30公尺的3公尺安裝高度，這意味著手機可能會被藏在大腿旁打字。在公共道路上部署時，可以利用橫橋式交通攝影機對車內進行更好的觀察，這一問題可以得到解決。儘管兩段式方法取得了更高的準確度，但是這與幀速率進行了折衷。我們建議優化模型，並探索TensorRT [40]框架以潛在地提高模型的速度。\n",
      "原文: VI.C ONCLUSION\n",
      "In this paper, we have presented a deep learning approach\n",
      "for detecting driver phone violations in all weather conditions\n",
      "without the need for human intervention.A total of 12 object\n",
      "detection models [3], [5], [9]–[11], [31], [32] are ﬁne-tuned\n",
      "and evaluated based on speed and accuracy for both the\n",
      "approaches which are: single-step, where a single frame is\n",
      "used to detect the phone, and the two-step, which ﬁrst detects\n",
      "windscreen and then uses the cropped image of only the driver\n",
      "side to detect the phone.The two-step approach yields higher\n",
      "accuracy but lower frame rate due to having to run two models\n",
      "simultaneously.The model chosen based on both accuracy and\n",
      "speed is YOLOv3 with an input resolution of 320.We also\n",
      "integrate DeepSort, an object tracking algorithm, which allows\n",
      "us to only collect and log unique phone detections from the\n",
      "driver’s side, meaning that this collected data could be made\n",
      "useful for time-series analysis.The trained object detector is\n",
      "able to achieve an accuracy of 84.62% ( AP10) on the 216 test\n",
      "images, with a frame rate of 13.15 FPS during activity and\n",
      "\u001827 with no activity.\n",
      "翻譯結果: VI.C 總結\n",
      "本文提出了一種深度學習方法，可以在各種天氣條件下自動檢測司機使用手機的行為，無需人為介入。基於速度和準確性對12種目標檢測模型進行了微調和評估，這些模型包括[3]，[5]，[9]–[11]，[31]，[32]，其中單步法使用單一幀來檢測手機，而雙步法則先檢測風擋，然後使用只包含司機區域的裁剪圖像來檢測手機。雙步法能夠取得更高的準確性，但由於需要同時運行兩個模型而降低了幀率。基於準確性和速度的綜合考量，我們選擇了 YOLOv3 作為模型，並將 DeepSort 目標跟踪算法集成到模型中，使我們能夠只收集並記錄來自司機側面的唯一手機偵測信息，這些收集的數據可用於時間序列分析。訓練好的目標檢測器在216張測試圖像上達到了84.62%（AP10）的準確度，在活動期間的幀率為13.15 FPS，在無活動期間為27。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: When images were split between high-\n",
      "end camera and low-end camera, we achieve accuracy levels\n",
      "as high as 95.81% on the high-end and 74.36% on the budget\n",
      "cameras.A user interface is also built in order for the user\n",
      "to easily access the information as well as the ability to view\n",
      "snapshots of all violations.We kindly invite the readers to refer to the supplemental\n",
      "video : https:// youtu.be/PErIUr3Cxvg for more information\n",
      "and more results in video format.REFERENCES\n",
      "[1] World Health Organization.Road trafﬁc injuries.https://www.who.in\n",
      "t/news-room/fact-sheets/detail/road-trafﬁc-injuries, 2021.[Online;\n",
      "accessed 5-July-2021].[2] GOV UK.Double penalties for motorists using mobiles.https://www.gov.uk/government/news/double-penalties-for-motorists-using-mobiles,\n",
      "2017.[Online; accessed 7-July-2021].[3] Luis M ´arquez, V ´ıctor Cantillo, and Juli ´an Arellana.Mobile phone\n",
      "use while driving: A hybrid modeling approach.Accident Analysis &\n",
      "Prevention , 78:73–80, 2015.[4] Jacinto C Nascimento and Jorge S Marques.\n",
      "翻譯結果: 當圖像分為高端相機和低端相機時，我們實現了高達95.81％的高端相機和74.36％的低端相機的準確性水平。還建立了用戶界面，以便用戶輕鬆訪問信息以及查看所有違規的快照。 我們誠懇邀請讀者參考補充視頻：https://youtu.be/PErIUr3Cxvg以獲取更多信息和更多以視頻格式呈現的結果。\n",
      "\n",
      "參考文獻\n",
      "[1] 世界衛生組織。道路交通傷害。https://www.who.int/news-room/fact-sheets/detail/road-traff ic-injuries, 2021。[在線;於2021年7月5日訪問]。\n",
      "[2] GOV UK。雙重處罰對使用手機駕駛的汽車駕駛者。https://www.gov.uk/government/news/double-penalties-for-motorists-using-mobiles, 2017。[在線;於2021年7月7日訪問]。\n",
      "[3] Luis Márquez、Víctor Cantillo和Julián Arellana。行駛時使用手機：一種混合建模方法。事故分析與預防，78：73-80，2015。\n",
      "[4] Jacinto C Nascimento和Jorge S Marques.\n",
      "原文: Performance evaluation of\n",
      "object detection algorithms for video surveillance.IEEE Transactions\n",
      "on Multimedia , 8(4):761–774, 2006.\n",
      "翻譯結果: 對於視頻監控的物體檢測演算法進行性能評估。IEEE Transactions\n",
      "on Multimedia，2006年8月4日：761-774。\n",
      "原文: Object detector FPS FPS FPS\n",
      "(detection only) (with tracking) (with tracking & two-step)\n",
      "YOLOv3 320 29.71 26.94 13.15\n",
      "TABLE V: Impact on frame rate for the chosen model YOLOv3-320 when tracking and two-step is added.Frame rate of\n",
      "the two-step has been recorded when there is constant activity in the video.Without activity, frame rate increases to that of\n",
      "detection and tracker.Object detector Model type AP50 AP10 AP50 AP10\n",
      "(high-quality) (high-quality) (low-quality) (low-quality)\n",
      "YOLOv3 320 Two-step 78.29 95.81 41.93 74.36\n",
      "TABLE VI: Average precision results when images are split between high-end and low-end cameras for YOLOv3-320.[5] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.Faster r-\n",
      "cnn: Towards real-time object detection with region proposal networks.Advances in neural information processing systems , 28:91–99, 2015.[6] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott\n",
      "Reed, Cheng-Yang Fu, and Alexander C Berg.Ssd: Single shot multibox\n",
      "detector.In European conference on computer vision , pages 21–37.\n",
      "翻譯結果: 物件偵測器 FPS FPS FPS\n",
      "(僅偵測) (加上追蹤) (加上追蹤和兩步驟)\n",
      "YOLOv3 320 29.71 26.94 13.15\n",
      "表 V：當加入追蹤和兩步驟時，YOLOv3-320 選擇的模型對相干影像的影響。兩步驟的影格率在影片中有不斷的活動時所記錄的。若沒有活動，影格率會增加到偵測和追蹤的水準。\n",
      "物件偵測器 模型類型 AP50 AP10 AP50 AP10\n",
      "(高品質) (高品質) (低品質) (低品質)\n",
      "YOLOv3 320 兩步驟 78.29 95.81 41.93 74.36\n",
      "表 VI：當影像分配給高階和低階攝影機時，YOLOv3-320 的平均精度結果。[5] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-\n",
      "cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28:91–99, 2015. [6] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In European conference on computer vision, pages 21–37.\n",
      "原文: Springer, 2016.[7] Jie Xu.A deep learning approach to building an intelligent video\n",
      "surveillance system.Multimedia Tools and Applications , 80(4):5495–\n",
      "5515, 2021.[8] Joseph Redmon and Ali Farhadi.Yolov3: An incremental improvement.arXiv preprint arXiv:1804.02767 , 2018.[9] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao.Yolov4: Optimal speed and accuracy of object detection.arXiv preprint\n",
      "arXiv:2004.10934 , 2020.[10] Yixuan Kang.Research on ssd base network.In IOP Conference Series:\n",
      "Materials Science and Engineering , volume 768, page 072031.IOP\n",
      "Publishing, 2020.[11] Zhujun Xu, Emir Hrustic, and Damien Vivet.Centernet heatmap propa-\n",
      "gation for real-time video object detection.In European Conference on\n",
      "Computer Vision , pages 220–234.Springer, 2020.[12] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn,\n",
      "and Andrew Zisserman.The pascal visual object classes (voc) challenge.International journal of computer vision , 88(2):303–338, 2010.[13] Paul Henderson and Vittorio Ferrari.\n",
      "翻譯結果: Springer, 2016.[7] Jie Xu.應用深度學習建立智能視頻監控系統。多媒體工具和應用，80(4):5495-5515，2021年。[8] Joseph Redmon和Ali Farhadi。Yolov3:一種增量改進。arXiv預印本arXiv:1804.02767，2018年。[9] Alexey Bochkovskiy，Chien-Yao Wang和Hong-Yuan Mark Liao。Yolov4:物體檢測的最佳速度和準確性。arXiv預印本arXiv:2004.10934，2020年。[10] Yixuan Kang。基於SSD的網絡研究。在IOP會議系列中：材料科學和工程，768卷，第072031頁。IOP出版，2020年。[11] Zhujun Xu，Emir Hrustic和Damien Vivet。Centernet熱圖傳播實現實時視頻物體檢測。在歐洲計算機視覺會議上，頁220-234。Springer，2020年。[12] Mark Everingham，Luc Van Gool，Christopher KI Williams，John Winn和Andrew Zisserman。Pascal視覺對象類別（VOC）挑戰。國際計算機視覺雜誌，88（2）：303-338，2010年。[13] Paul Henderson和Vittorio Ferrari。\n",
      "原文: End-to-end training of object class\n",
      "detectors for mean average precision.In Asian Conference on Computer\n",
      "Vision , pages 198–213.Springer, 2016.[14] Xinyu Hou, Yi Wang, and Lap-Pui Chau.Vehicle tracking using deep\n",
      "sort with low conﬁdence track ﬁltering.In 2019 16th IEEE International\n",
      "Conference on Advanced Video and Signal Based Surveillance (AVSS) ,\n",
      "pages 1–6.IEEE, 2019.[15] G Sreenu and MA Saleem Durai.Intelligent video surveillance: a review\n",
      "through deep learning techniques for crowd analysis.Journal of Big\n",
      "Data , 6(1):1–27, 2019.[16] Clive Norris and Xavier L’Hoiry.Times of crises and the development\n",
      "of the police national automatic number plate recognition system in the\n",
      "uk.In Big Data, Surveillance and Crisis Management , pages 198–221.Routledge, 2017.[17] Paul Viola and Michael Jones.Rapid object detection using a boosted\n",
      "cascade of simple features.In Proceedings of the 2001 IEEE computer\n",
      "society conference on computer vision and pattern recognition.CVPR\n",
      "2001 , volume 1, pages I–I.\n",
      "翻譯結果: 物體類別檢測器的平均精度終端到端訓練。於2016年亞洲計算機視覺會議上，第198-213頁。Springer。[14]侯新宇、王毅和周立培。使用深度排序進行低置信度跟踪過濾的車輛跟踪。於2019年第16屆IEEE高級視頻和信號基於監視（AVSS）國際會議上，第1-6頁。IEEE。[15]G Sreenu 和MA Saleem Durai。基於深度學習技術的群眾分析智能視頻監控回顧。《大數據》雜誌，6(1):1-27，2019。[16]克萊夫·諾里斯和沙維爾·勒瓦利。危機時期和英國國家自動車牌照識別系統的發展。於《大數據、監視和危機管理》一書中，第198-221頁。Routledge，2017年。[17]保羅·維奧拉和邁克爾·瓊斯。使用簡單特徵的增強級聯快速對象檢測。於2001年IEEE計算機協會的計算機視覺和模式識別會議上，CVPR 2001，卷1，I-I頁。\n",
      "原文: Ieee, 2001.[18] Navneet Dalal and Bill Triggs.Histograms of oriented gradients\n",
      "for human detection.In 2005 IEEE computer society conference on\n",
      "computer vision and pattern recognition (CVPR’05) , volume 1, pages\n",
      "886–893.Ieee, 2005.[19] Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva\n",
      "Ramanan.Object detection with discriminatively trained part-based\n",
      "models.IEEE transactions on pattern analysis and machine intelligence ,\n",
      "32(9):1627–1645, 2009.[20] Zhengxia Zou, Zhenwei Shi, Yuhong Guo, and Jieping Ye.Object\n",
      "detection in 20 years: A survey.arXiv preprint arXiv:1905.05055 , 2019.[21] Chenyi Chen, Ming-Yu Liu, Oncel Tuzel, and Jianxiong Xiao.R-cnn for\n",
      "small object detection.In Asian conference on computer vision , pages\n",
      "214–230.Springer, 2016.[22] Ross Girshick.Fast r-cnn.In Proceedings of the IEEE international\n",
      "conference on computer vision , pages 1440–1448, 2015.[23] Jifeng Dai, Yi Li, Kaiming He, and Jian Sun.R-fcn: Object detection\n",
      "via region-based fully convolutional networks.\n",
      "翻譯結果: Ieee, 2001年[18]。Navneet Dalal和Bill Triggs。针对人类检测的梯度方向直方图。在2005年IEEE计算机学会计算机视觉和模式识别会议（CVPR'05）中，第1卷，第886至893页，2005年。[19] Pedro F Felzenszwalb，Ross B Girshick，David McAllester和Deva Ramanan。用有区别地训练的基于部分的模型进行目标检测。IEEE模式分析与机器智能交易，32（9）：1627-1645，2009年。[20]邹正霞，石振伟，郁红，叶杰平。20年中的目标检测：一项调查。arXiv预印本arXiv：1905.05055，2019年。[21]陈谊琛，刘明宇，Oncel Tuzel和肖建雄。用于小物体检测的R-cnn。在亚洲计算机视觉会议上，第214-230页。Springer，2016年。[22] Ross Girshick。快速r-cnn。在IEEE国际会议计算机视觉论文集上，第1440-1448页，2015年。[23] Jifeng Dai，Yi Li，Kaiming He和Jian Sun。R-fcn：通过基于区域的全卷积网络进行目标检测。\n",
      "原文: In Advances in neural\n",
      "information processing systems , pages 379–387, 2016.[24] Jiangmiao Pang, Kai Chen, Jianping Shi, Huajun Feng, Wanli Ouyang,\n",
      "and Dahua Lin.Libra r-cnn: Towards balanced learning for object\n",
      "detection.In 2019 IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition (CVPR) , pages 821–830.IEEE, 2019.[25] Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yangdong Deng,\n",
      "and Jian Sun.Light-head r-cnn: In defense of two-stage object detector.arXiv preprint arXiv:1711.07264 , 2017.[26] Hwejin Jung, Bumsoo Kim, Inyeop Lee, Minhwan Yoo, Junhyun Lee,\n",
      "Sooyoun Ham, Okhee Woo, and Jaewoo Kang.Detection of masses\n",
      "in mammograms using a one-stage object detector based on a deep\n",
      "convolutional neural network.PloS one , 13(9):e0203355, 2018.[27] Bensu Alkan, Burak Balci, Alperen Elihos, and Yusuf Artan.Driver cell\n",
      "phone usage violation detection using license plate recognition camera\n",
      "images.In VEHITS , pages 468–474, 2019.[28] Jannes W Elings.Driver handheld cell phone usage detection.\n",
      "翻譯結果: 在神經信息處理系統的進展中，第379-387頁，2016年。[24] Jiangmiao Pang，Kai Chen，Jianping Shi，Huajun Feng，Wanli Ouyang和Dahua Lin。Libra r-cnn：為目標檢測實現平衡學習。在2019年IEEE / CVF計算機視覺和模式識別（CVPR）會議上，第821-830頁。IEEE，2019年。[25] Zeming Li，Chao Peng，Gang Yu，Xiangyu Zhang，Yangdong Deng和Jian Sun。Light-head r-cnn：為兩階段目標檢測器辯護。arXiv預印本arXiv：1711.07264，2017年。[26] Hwejin Jung，Bumsoo Kim，Inyeop Lee，Minhwan Yoo，Junhyun Lee，Sooyoun Ham，Okhee Woo和Jaewoo Kang。基於深度卷積神經網絡的一階段目標檢測器檢測乳腺攝影中的腫塊。PloS one，13（9）：e0203355，2018年。[27] Bensu Alkan，Burak Balci，Alperen Elihos和Yusuf Artan。使用車牌識別攝像頭圖像檢測駕駛員手機使用違規。在VEHITS，第468-474頁，2019。[28] Jannes W Elings。檢測駕駛員手持手機使用。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: Master’s\n",
      "thesis, 2018.[29] Srikanth Tammina.Transfer learning using vgg-16 with deep convolu-\n",
      "tional neural network for classifying images.International Journal of\n",
      "Scientiﬁc and Research Publications (IJSRP) , 9(10):143–150, 2019.[30] T Hoang Ngan Le, Yutong Zheng, Chenchen Zhu, Khoa Luu, and Marios\n",
      "Savvides.Multiple scale faster-rcnn approach to driver’s cell-phone\n",
      "usage and hands on steering wheel detection.In Proceedings of the\n",
      "IEEE conference on computer vision and pattern recognition workshops ,\n",
      "pages 46–53, 2016.[31] Zifeng Wu, Chunhua Shen, and Anton Van Den Hengel.Wider or\n",
      "deeper: Revisiting the resnet model for visual recognition.Pattern\n",
      "Recognition , 90:119–133, 2019.[32] Wei Wang, Yutao Li, Ting Zou, Xin Wang, Jieyu You, and Yanhong\n",
      "Luo.A novel image classiﬁcation approach via dense-mobilenet models.Mobile Information Systems , 2020, 2020.[33] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan\n",
      "Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci,\n",
      "Alexander Kolesnikov, et al.\n",
      "翻譯結果: 研究生論文，2018年。[29] Srikant Tammina。利用帶有深度卷積神經網絡的VGG-16進行轉移學習用於圖像分類。《國際科學與研究出版》（IJSRP），9（10）：143-150，2019年。[30] T Hoang Ngan Le，Yutong Zheng，Chenchen Zhu，Khoa Luu和Marios Savvides。多尺度更快的RCNN方法用於司機手機使用和握住方向盤偵測。在IEEE計算機視覺和模式識別研討會研討會文集中，頁46-53，2016年。[31] 吳自鋒，沈春華和Anton Van Den Hengel。更寬或更深：重新訪問ResNet模型進行視覺識別。《圖案識別》 ，90：119-133，2019年。[32]王偉，李玉濤，鄭廷，王鑫，游潔宇和羅艷紅。通過密集移動網絡模型的新型圖像分類方法。《移動信息系統》，2020年，2020年。[33] Alina Kuznetsova，Hassan Rom，Neil Alldrin，Jasper Uijlings，Ivan Krasin，Jordi Pont-Tuset，Shahab Kamali，Stefan Popov，Matteo Malloci，Alexander Kolesnikov等。\n",
      "原文: The open images dataset v4.International\n",
      "Journal of Computer Vision , 128(7):1956–1981, 2020.[34] Xiaoling Xia, Cui Xu, and Bing Nan.Inception-v3 for ﬂower classi-\n",
      "ﬁcation.In 2017 2nd International Conference on Image, Vision and\n",
      "Computing (ICIVC) , pages 783–787.IEEE, 2017.[35] Beatriz Blanco-Filgueira, Daniel Garcia-Lesta, Mauro Fern ´andez-\n",
      "Sanjurjo, V ´ıctor Manuel Brea, and Paula L ´opez.Deep learning-\n",
      "based multiple object visual tracking on embedded system for iot and\n",
      "mobile edge computing applications.IEEE Internet of Things Journal ,\n",
      "6(3):5423–5431, 2019.[36] Pirkko Mustamo.Object detection in sports: Tensorﬂow object detection\n",
      "api case study.University of Oulu , 2018.[37] Tensorﬂow.Tensorﬂow 2 detection model zoo.https://github.com/ten\n",
      "sorflow/models/blob/master/research/object detection/g3doc/tf2 detecti\n",
      "onzoo.md, 2021.[Online; accessed 5-May-2021].[38] Shivy Yohanandan.mAP (mean Average Precision) might confuse you!https://towardsdatascience.com/map-mean-average-precision-might-co\n",
      "nfuse-you-5956f1bfa9e2, 2020.\n",
      "翻譯結果: 開放影像數據集v4國際計算機視覺期刊，128（7）：1956-1981，2020年。 [34]夏小鈴，徐翠，南冰。 Inception-v3用於花卉分類。在2017年第二屆國際圖像，視覺和計算會議（ICIVC）中，第783-787頁。 IEEE，2017年。[35] Beatriz Blanco-Filgueira，Daniel Garcia-Lesta，Mauro Fern´andez-Sanjurjo，V´ıctor Manuel Brea和Paula L ´opez。基於深度學習的嵌入式系統多目標視覺跟踪，用於物聯網和移動邊緣計算應用程序。IEEE物聯網期刊，6（3）：5423-5431，2019年。[36] Pirkko Mustamo。運動中的物體檢測：Tensorflow物體檢測API案例研究。奧盧大學，2018年。[37] Tensorflow。TensorFlow 2檢測模型動物園。 https://github.com/tensorflow/models/blob/master/research/objectdetection/g3doc/tf2 detectizoo.md，2021年。[在線;訪問2021年5月5日]。[38] Shivy Yohanandan。 mAP（平均精度）可能會讓您困惑！ https://towardsdatascience.com/map-mean-average-precision-might-confuse-you-5956f1bfa9e2，2020年。\n",
      "原文: [Online; accessed 7-July-2021].[39] Huizi Mao, Xiaodong Yang, and William J Dally.A delay metric\n",
      "for video object detection: What average precision fails to tell.In\n",
      "Proceedings of the IEEE/CVF International Conference on Computer\n",
      "Vision , pages 573–582, 2019.[40] Han Vanholder.Efﬁcient inference with tensorrt, 2016.\n",
      "翻譯結果: [線上; 檢索日期：2021年7月7日]。 [39] Mao Huizi，Yang Xiaodong和William J Dally。影片物件偵測的延遲指標：平均精度無法告知。於《電腦視覺國際會議論文集》中，頁面573-582，2019年。[40] Han Vanholder。使用 TensorRT 的高效推論，2016年。\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(chunks)):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"請你成為文章翻譯的小幫手，請協助翻譯以下技術文件，以繁體中文輸出\"},\n",
    "        {\"role\": \"user\", \"content\": chunks[i]},\n",
    "    ]\n",
    "    )\n",
    "    print('原文:', chunks[i])\n",
    "    print('翻譯結果:',completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93678e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
