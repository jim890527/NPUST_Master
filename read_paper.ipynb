{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19230942",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_path = \"C:/Users/user/Desktop/paper/next/Identification_of_Driver_Phone_Usage_Violations_via_State-of-the-Art_Object_Detection_with_Tracking.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcdd074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "res = requests.get('https://cdn.openai.com/papers/gpt-4.pdf')\n",
    "with open('gpt-4.pdf', 'wb') as f:\n",
    "    f.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f8bd1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "reader = PdfReader(\"gpt-4.pdf\")\n",
    "number_of_pages = len(reader.pages)\n",
    "page = reader.pages[0]\n",
    "text = page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52e7d9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8266306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4 Technical Report\n",
      "OpenAI\u0003\n",
      "Abstract\n",
      "We report the development of GPT-4, a large-scale, multimodal model which can\n",
      "accept image and text inputs and produce text outputs.\n",
      "====================\n",
      "While less capable than\n",
      "humans in many real-world scenarios, GPT-4 exhibits human-level performance\n",
      "on various professional and academic benchmarks, including passing a simulated\n",
      "bar exam with a score around the top 10% of test takers.\n",
      "====================\n",
      "GPT-4 is a Transformer-\n",
      "based model pre-trained to predict the next token in a document.\n",
      "====================\n",
      "The post-training\n",
      "alignment process results in improved performance on measures of factuality and\n",
      "adherence to desired behavior.\n",
      "====================\n",
      "A core component of this project was developing\n",
      "infrastructure and optimization methods that behave predictably across a wide\n",
      "range of scales.\n",
      "====================\n",
      "This allowed us to accurately predict some aspects of GPT-4’s\n",
      "performance based on models trained with no more than 1/1,000th the compute of\n",
      "GPT-4.\n",
      "====================\n",
      "1 Introduction\n",
      "This technical report presents GPT-4, a large multimodal model capable of processing image and\n",
      "text inputs and producing text outputs.\n",
      "====================\n",
      "Such models are an important area of study as they have the\n",
      "potential to be used in a wide range of applications, such as dialogue systems, text summarization,\n",
      "and machine translation.\n",
      "====================\n",
      "As such, they have been the subject of substantial interest and progress in\n",
      "recent years [1–34].\n",
      "====================\n",
      "One of the main goals of developing such models is to improve their ability to understand and generate\n",
      "natural language text, particularly in more complex and nuanced scenarios.\n",
      "====================\n",
      "To test its capabilities\n",
      "in such scenarios, GPT-4 was evaluated on a variety of exams originally designed for humans.\n",
      "====================\n",
      "In\n",
      "these evaluations it performs quite well and often outscores the vast majority of human test takers.\n",
      "====================\n",
      "For example, on a simulated bar exam, GPT-4 achieves a score that falls in the top 10% of test takers.\n",
      "====================\n",
      "This contrasts with GPT-3.5, which scores in the bottom 10%.\n",
      "====================\n",
      "On a suite of traditional NLP benchmarks, GPT-4 outperforms both previous large language models\n",
      "and most state-of-the-art systems (which often have benchmark-speciﬁc training or hand-engineering).\n",
      "====================\n",
      "On the MMLU benchmark [ 35,36], an English-language suite of multiple-choice questions covering\n",
      "57 subjects, GPT-4 not only outperforms existing models by a considerable margin in English, but\n",
      "also demonstrates strong performance in other languages.\n",
      "====================\n",
      "On translated variants of MMLU, GPT-4\n",
      "surpasses the English-language state-of-the-art in 24 of 26 languages considered.\n",
      "====================\n",
      "We discuss these\n",
      "model capability results, as well as model safety improvements and results, in more detail in later\n",
      "sections.\n",
      "====================\n",
      "This report also discusses a key challenge of the project, developing deep learning infrastructure and\n",
      "optimization methods that behave predictably across a wide range of scales.\n",
      "====================\n",
      "This allowed us to make\n",
      "predictions about the expected performance of GPT-4 (based on small runs trained in similar ways)\n",
      "that were tested against the ﬁnal run to increase conﬁdence in our training.\n",
      "====================\n",
      "Despite its capabilities, GPT-4 has similar limitations to earlier GPT models [ 1,37,38]: it is not fully\n",
      "reliable (e.g.\n",
      "====================\n",
      "can suffer from “hallucinations”), has a limited context window, and does not learn\n",
      "\u0003Please cite this work as “OpenAI (2023)\".\n",
      "====================\n",
      "Full authorship contribution statements appear at the end of the\n",
      "document.\n",
      "====================\n",
      "Correspondence regarding this technical report can be sent to gpt4-report@openai.comarXiv:submit/4812508  [cs.CL]  27 Mar 2023\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print('=' * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "935565fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = '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'\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"請你成為文章翻譯的小幫手，請協助翻譯以下技術文件，以繁體中文輸出\"},\n",
    "        {\"role\": \"user\", \"content\": sentences[0]},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca0c7c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPT-4技術報告 \\nOpenAI\\n摘要 \\n本報告介紹GPT-4的開發。GPT-4是一個大型、多模態模型，它可以接收圖片和文本輸入並生成文本輸出。'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee9fe9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences = ''\n",
    "chunks = []\n",
    "for sentence in sentences:\n",
    "    input_sentences += sentence\n",
    "    if len(input_sentences) > 1000:\n",
    "        chunks.append(input_sentences)\n",
    "        input_sentences = ''\n",
    "chunks.append(input_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1475c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"請你成為文章翻譯的小幫手，請協助翻譯以下技術文件，以繁體中文輸出\"},\n",
    "        {\"role\": \"user\", \"content\": chunks[0]},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c71d3329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPT-4技術報告\\nOpenAI\\n摘要：\\n本報告闡述GPT-4的開發過程，這是一種大規模、多模態模型，可以接受圖片和文本輸入並產生文本輸出。儘管在許多現實場景中不如人類強大，但GPT-4在各種專業和學術測試中展現出與人類相當的表現，包括通過模擬的律師考試，成績在前10％的考生中。GPT-4是一個基於Transformer架構的模型，其預先訓練的目標是預測文檔中的下一個標記。後訓練校正過程使其在事實準確性和遵循所需行為方面的表現有所提升。本項目的核心組件是開發基礎設施和優化方法，這些方法能夠在各種規模下表現穩定可靠，從而使我們可以使用不超過GPT-4計算代價的1/1,000來準確預測一些GPT-4表現的方面。\\n\\n1 簡介\\n本技術報告介紹GPT-4，一種大型多模態模型，能夠處理圖片和文本輸入並產生文本輸出。'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "693351c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "pdf_name = paper_path\n",
    "reader = PdfReader(pdf_name)\n",
    "number_of_pages = len(reader.pages)\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for i in range(number_of_pages):\n",
    "    page = reader.pages[i]\n",
    "    text = page.extract_text()\n",
    "    sentences = sent_tokenize(text)\n",
    "    input_sentences = ''\n",
    "  \n",
    "    for sentence in sentences:\n",
    "        input_sentences += sentence\n",
    "        if len(input_sentences) > 1000:\n",
    "            chunks.append(input_sentences)\n",
    "            input_sentences = ''\n",
    "    chunks.append(input_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cce045d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: Identiﬁcation of Driver Phone Usage Violations via\n",
      "State-of-the-Art Object Detection with Tracking\n",
      "Steven Carrell\n",
      "School of Computing\n",
      "Newcastle University\n",
      "Newcastle upon Tyne, United Kingdom\n",
      "steven.carrell.ncl@gmail.comAmir Atapour-Abarghouei\n",
      "Department of Computer Science\n",
      "Durham University\n",
      "Durham, United Kingdom\n",
      "amir.atapour-abarghouei@durham.ac.uk\n",
      "Abstract —The use of mobiles phones when driving have been\n",
      "a major factor when it comes to road trafﬁc incidents and\n",
      "the process of capturing such violations can be a laborious\n",
      "task.Advancements in both modern object detection frameworks\n",
      "and high-performance hardware has paved the way for a more\n",
      "automated approach when it comes to video surveillance.In\n",
      "this work, we propose a custom-trained state-of-the-art object\n",
      "detector to work with roadside cameras to capture driver phone\n",
      "usage without the need for human intervention.The proposed\n",
      "approach also addresses the issues caused by windscreen glare\n",
      "and introduces the steps required to remedy this.\n",
      "翻譯結果: 透過最先進的物件偵測與追蹤技術辨識駕駛手機使用違規行為\n",
      "\n",
      "史蒂芬 · 卡瑞爾（Steven Carrell）\n",
      "新堡大學計算機學院\n",
      "英國新堡\n",
      "steven.carrell.ncl@gmail.com\n",
      "\n",
      "阿米爾·阿塔普爾-阿巴吾伊（Amir Atapour-Abarghouei）\n",
      "杜倫大學計算機科學系\n",
      "英國杜倫\n",
      "amir.atapour-abarghouei@durham.ac.uk\n",
      "\n",
      "摘要：在駕駛時使用手機一直是道路交通事故的主要因素之一。捕捉此類違規行為的過程可能是一個費時的任務。現代物件偵測框架和高性能硬體的進步為視頻監視提供了更自動化的方法。在本文中，我們提出了一種定制的最先進的物件偵測器，可以與路邊攝像頭配合使用，無需人工干預即可捕捉駕駛員使用手機的情況。所提出的方法還解決了風擋反光引起的問題，介紹了解決此問題所需的步驟。\n",
      "原文: Twelve pre-\n",
      "trained models are ﬁne-tuned with our custom dataset using\n",
      "four popular object detection methods: YOLO, SSD, Faster R-\n",
      "CNN, and CenterNet.Out of all the object detectors tested, the\n",
      "YOLO yields the highest accuracy levels of up to \u001896% (AP10)\n",
      "and frame rates of up to \u001830 FPS.DeepSort object tracking\n",
      "algorithm is also integrated into the best-performing model to\n",
      "collect records of only the unique violations, and enable the\n",
      "proposed approach to count the number of vehicles.The proposed\n",
      "automated system will collect the output images of the identiﬁed\n",
      "violations, timestamps of each violation, and total vehicle count.Data can be accessed via a purpose-built user interface.Index Terms —Mobile phone detection, YOLO Object Detec-\n",
      "tion, Intelligent Transportation Systems, Deep Learning\n",
      "I. I NTRODUCTION\n",
      "According to the World Health Organization (WHO), ap-\n",
      "proximately 1.3 million people die each year as a result of\n",
      "road trafﬁc accidents [1].A contributing factor towards this is\n",
      "the use of a handheld mobile devices while operating a motor\n",
      "vehicle.\n",
      "翻譯結果: 使用自定義數據集對12個預先訓練模型進行微調，使用四種流行的物體檢測方法：YOLO、SSD、Faster R-CNN和CenterNet。在所有測試的物體檢測器中，YOLO可以達到高達96%（AP10）的準確度和高達30 FPS的幀率。DeepSort物體跟踪算法也被集成到表現最佳的模型中，以收集唯一違規記錄，使所提出的方法能夠計算車輛數量。所提出的自動系統將收集識別違規的輸出圖像、每個違規的時間戳和總車輛數。數據可以通過專用用戶界面訪問。關鍵詞-手機檢測，YOLO物體檢測，智能交通系統，深度學習。\n",
      "I.引言\n",
      "根據世界衛生組織的統計，每年約有130萬人死於道路交通事故[1]。其中一個導致這種情況的因素是使用手持行動裝置時駕駛機動車輛。\n",
      "原文: Drivers using a mobile phone are approximately four\n",
      "times more likely to be involved in a crash than drivers not\n",
      "using their phone [1].In 2017, the UK Government doubled the penalty for using\n",
      "a mobile phone while driving to 6 points and a £200 ﬁne\n",
      "(up from 3 points and £100) [2].A study carried out in 2015\n",
      "suggests that there is a negative correlation between a higher\n",
      "ﬁne and the likelihood of a person using their phone [3].Typically, catching drivers using their phones involves road-\n",
      "side police performing the laborious task of capturing the\n",
      "violation as it happens.Unless there are signiﬁcant resources\n",
      "dedicated to this task, there is a strong probability that many\n",
      "of these violations will go undetected.This opens the door for\n",
      "a more automated process of capturing these violations.Recent years have seen the development of object detection\n",
      "in video surveillance [4].Object detection frameworks such as\n",
      "(a) Step one.(b) Step two.Fig.1: Example of the proposed two-step approach: the ﬁrst\n",
      "step (left) detects windscreen; the second step (right) ﬁrst crops\n",
      "the driver’s side and only then detects the phone.\n",
      "翻譯結果: 使用手機的司機發生車禍的可能性約為不使用手機的司機的四倍[1]。2017年，英國政府將開車使用手機的罰款加倍至6分和200英鎊罰款（由原來的3分和100英鎊）[2]。一項在2015年進行的研究表明，罰款越高，使用手機的可能性越小[3]。通常，抓到使用手機的司機需要路邊警察執行艱難的任務來捕捉違規行為。除非有大量資源專門用於此項任務，否則很有可能會有許多違規行為未被發現。這為更加自動化的捕捉違規行為開啟了方便之門。近年來，視頻監控中的物體檢測得到了發展[4]。物體檢測框架，例如（a）第一步。（b）第二步。圖1:所提出的兩步方法的示例：第一步（左）檢測擋風玻璃；第二步（右）先裁剪司機一側，然後再檢測手機。\n",
      "原文: Faster Region Based Convolutional Neural Networks (Faster\n",
      "R-CNN) [5] and Single Shot Detector (SSD) [6] have made\n",
      "it possible to take video images and detect objects with both\n",
      "high accuracy and speed [7].In this work, we propose a fully-automated system that will\n",
      "take live video from roadside surveillance cameras and detect\n",
      "if a driver is using a mobile phone whilst the vehicle is in\n",
      "operation.We will explore different quality cameras (high-end\n",
      "and low-end) whilst addressing challenges such as windscreen\n",
      "glare, tinted windows and low-light scenarios.In order for the\n",
      "system to be fully automated, it will need to have the ability\n",
      "to log each unique violation as well as to save the images\n",
      "corresponding to each violation.We propose two methods for achieving this task; ﬁrst, a\n",
      "single-step method focusing on efﬁciency and speed, using a\n",
      "single trained model to detect violations.This method will\n",
      "suffer a trade-off with accuracy due to potential issues caused\n",
      "by having to ﬁnd an extremely small object (phone) within\n",
      "a large image.\n",
      "翻譯結果: 更快速的區域基於卷積神經網絡 (Faster R-CNN) 和單次檢測器 (SSD) 已經使得使用視頻影像進行高精度和高速度的物體檢測成為可能。本文提出了一個完全自動化的系統，它可以從路邊監控攝像頭中輸入實時視頻，並檢測一名駕駛員是否在駕車時使用手機。我們將探討不同質量的攝像頭（高端和低端），同時解決風擋反光、有色玻璃和低光照情況等挑戰。為了實現系統的全自動化，它需要有能力記錄每個唯一的違規行為以及保存對應每個違規行為的影像。我們提出了兩種實現這一任務的方法；第一個是單步方法，重點在於效率和速度，使用一個訓練好的模型來檢測違規行為。這種方法可能會因為必須在大圖像中找到一個極小的物體（手機）而導致精度上的妥協。\n",
      "原文: The second method (two-step) focuses on\n",
      "achieving high accuracy by running two individually trained\n",
      "models simultaneously.The single-step system is trained to detect two classes,\n",
      "namely mobile phone and licence plate with the plate only\n",
      "used as a method of counting the total number of vehicles.The two-step system (Figure 1) ﬁrst detects the windscreen\n",
      "and then uses the cropped image of the driver’s side of the\n",
      "windscreen as the input to the second step to detect the mobile\n",
      "phone.Similar to the licence plate in the ﬁrst (single-step)\n",
      "method, the windscreen is used to count the number of vehiclesarXiv:2109.02119v3  [cs.CV]  8 Oct 2021\n",
      "翻譯結果: 第二種方法（兩步驟）著重於通過同時運行兩個獨立訓練的模型來達到高精度。單步驟系統被訓練為檢測兩個類別，即手機和車牌，其中車牌僅用作計算總車輛數的一種方法。兩步驟系統（圖1）首先檢測擋風玻璃，然後使用司機側擋風玻璃的裁剪圖像作為第二步的輸入，以檢測手機。與第一種（單步驟）方法中的車牌類似，擋風玻璃用於計算車輛數。\n",
      "原文: so the number of detections can be worked out as a proportion\n",
      "to the total number of vehicles.Models are trained using a\n",
      "number of different state-of-the art object detector frameworks.These include YOLO (You Only Look Once) v3 [8] and v4\n",
      "[9], SSD [10], [11], Faster R-CNN [5] and Centernet [11].Model performance is measured via Average Precision (AP)\n",
      "[12], and using both the PASCAL VOC evaluation metric\n",
      "where the Intersection Over Union (IoU) score is >0.5 [13],\n",
      "whilst also testing IoU >0.1 due to the nature of objects being\n",
      "detected in this particular application.The proposed solution\n",
      "will be designed to work with a live video; therefore, we also\n",
      "evaluate the efﬁciency of the proposed method by calculating\n",
      "the frame rate of the output - i.e.frames per second (FPS).The images that are used to train our model on the phone\n",
      "class will predominantly be obtained and created especially\n",
      "for this project.In order for the model to detect mobile phone\n",
      "use violations with a reasonable level of accuracy, the training\n",
      "images will be replications of the real-world scenario of a\n",
      "person using their phone whilst driving (examples of training\n",
      "images can be seen in Figure 8).\n",
      "翻譯結果: 因此，偵測數量可以按照總車輛數的比例計算出來。模型是使用多種最先進的物體檢測框架進行訓練的，這些框架包括YOLO（You Only Look Once）v3【8】和v4【9】、SSD【10】【11】、Faster R-CNN【5】和Centernet【11】。模型的性能是通過平均精度（AP）【12】來衡量的，使用PASCAL VOC評估指標來進行評估，其中交集比聯集（IoU）大於0.5【13】，同時也會測試IoU大於0.1，因為在這特定應用中檢測到的對象的性質。所提出的解決方案將被設計為能適應實時視頻的，因此，我們還通過計算輸出的幀速率 - 即每秒幀數（FPS）來評估提出的方法的效率。用於訓練我們的手機類型模型的圖像主要是為該項目而獲取和創建的。為了使模型能夠以合理的精度檢測出手機使用違規行為，訓練圖像將是人們在開車時使用手機的現實情況的複製品（訓練圖像的示例可以見於圖 8）。\n",
      "原文: Once the ﬁnal model is chosen, we integrate a tracking\n",
      "algorithm [14] into our model to avoid the issue of logging\n",
      "multiple detections for the same violation.This will allow\n",
      "the system to keep track of the total number of violations\n",
      "for a given duration.This data can then be analysed using a\n",
      "purposely designed user interface.This work aims to successfully develop a system that can\n",
      "work with a roadside camera 24 hours a day to automatically\n",
      "detect whether a person is using their mobile phone when in\n",
      "operation of a motor vehicle.In short, the primary contribu-\n",
      "tions of this paper are as follows:\n",
      "\u000fTrain and evaluate multiple object detection methods [5],\n",
      "[8]–[11] to detect mobile phone use violations with a\n",
      "reasonable degree of accuracy and speed by establishing\n",
      "an appropriate trade-off between predictive performance\n",
      "and efﬁciency.\u000fTest the trained models on both full images and cropped\n",
      "windscreens to determine if a single-step or two-step\n",
      "approach is more appropriate.The single-step method\n",
      "operates on the full image only, while the two-step system\n",
      "ﬁrst ﬁnds windscreen and then uses this cropped image\n",
      "to detect the phone (Section III).\n",
      "翻譯結果: 一旦選定最終模型，我們將一個跟踪算法集成到模型中，以避免同一違規的多次檢測問題。這將使系統能夠追踪相同時間段內的所有違規總數。然後可以使用特定的用戶界面分析這些數據。本研究旨在成功開發一個可以與路邊攝像頭24小時配合工作的系統，以自動檢測駕駛行駛時是否使用手機。簡而言之，本文的主要貢獻如下：\n",
      "\u000f使用適當的預測性能和效率之間的權衡，訓練和評估多種物體檢測方法[5]，[8]–[11]，以合理的準確性和速度檢測使用手機違規。\n",
      "\u000f測試訓練好的模型，對於全圖像和裁剪的擋風玻璃，以確定單步或兩步方法更適合。單步方法僅在全圖像上運行，而兩步系統首先找到擋風玻璃，然後使用這個裁剪的圖像來檢測手機（第III部分）。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: \u000fEstablish what can be achieved with on a low-cost\n",
      "budget using a low-end consumer camera solution as well\n",
      "as providing insights on what can be achieved with a\n",
      "reasonable budget using high-end cameras.\u000fEnsure issues such as windscreen glare and poorly-lit\n",
      "environments are addressed so the system can work at\n",
      "any time of the day.\u000fIntegrate a tracking algorithm [14] to identify unique\n",
      "detections in order to log useful data whilst providing\n",
      "snapshots of the violations.Collected data can be ac-\n",
      "cessed through a purposely designed user interface.To better enable reproducibility, the source code for theproject is publicly available1.The remainder of the paper is organised as follows.Related\n",
      "work on mobile phone usage detection within the existing\n",
      "literature is presented in Section II.The proposed approach\n",
      "is described in Section III.Experimental results are examined\n",
      "and discussed in Section IV.We address limitations and future\n",
      "work in Section V, before ﬁnally concluding the paper in\n",
      "Section VI.\n",
      "翻譯結果: • 為了確保系統在任何時間都能夠運作，需要解決風擋玻璃反光和光線不好的環境等問題。\n",
      "• 整合跟踪算法[14]以識別唯一的檢測，從而記錄有用的數據，並提供違規的快照。收集的數據可以通過專門設計的用戶界面進行訪問。\n",
      "• 為了更好地實現再現性，該項目的源代碼是公開可用的1。\n",
      "• 本文的其餘部分如下所述。在第二節中介紹了現有文獻中關於手機使用檢測的相關工作。第三節描述了所提出的方法。第四節檢查和討論了實驗結果。我們在第五節中討論了限制和未來工作，最後在第六節總結了本文。\n",
      "原文: II.R ELATED WORK\n",
      "We consider related work in the context of object detection\n",
      "in general (Section II-A) and speciﬁc systems utilised in\n",
      "distracted driver identiﬁcation applications (Section II-B).A.Object Detection\n",
      "Traditionally, any kind of object identiﬁcation with the use\n",
      "of video surveillance would be done manually and could\n",
      "involve the laborious task of humans either monitoring the\n",
      "live stream of the camera or reviewing historic footage.This\n",
      "difﬁcult and tiresome task meant that a more intelligent ap-\n",
      "proach would eventually be required [15].Object detection is a\n",
      "computer vision task concerned with detecting and classifying\n",
      "objects within an image.The use of this technology has paved\n",
      "the way for a more automated solution into video surveillance\n",
      "applications.Examples of such tasks include Licence Plate\n",
      "Recognition (LPR) [16], people tracking, vehicle counting, or\n",
      "unattended baggage in airports.We can describe two-major historic milestones in the de-\n",
      "velopment of object detection: “traditional object detection\n",
      "period” (pre 2014) [17]–[19] and “learning-based detection\n",
      "period” [20].\n",
      "翻譯結果: II.相關工作\n",
      "我們考慮一般物體檢測（第II-A節）以及在分心駕駛者識別應用中使用的特定系統（第II-B節）中的相關工作。 \n",
      "A. 物體檢測\n",
      "傳統上，使用視頻監控進行任何類型的物體識別都是通過人工進行的，可能涉及人員監控攝像機的直播流或回顧歷史記錄的繁瑣任務。這項艱巨且繁瑣的任務意味著最終需要一種更智能的方法[15]。物體檢測是一項關注於檢測和分類圖像中物體的計算機視覺任務。該技術的使用為視頻監控應用程序開辟了更加自動化的解決方案。此類任務的示例包括車牌識別（LPR）[16]、人員跟踪、車輛計數或機場中無人看管的行李檢測。我們可以描述物體檢測的兩個主要歷史里程碑：傳統物體檢測時期（2014年之前）[17]–[19]和基於學習的檢測時期[20]。\n",
      "原文: It was not until 2015 where object detection\n",
      "could be utilised in real-time video with the development of\n",
      "Faster Regional-Based Neural Network (Faster R-CNN) [5],\n",
      "an improvement from its predecessor R-CNN [21] in both\n",
      "accuracy and speed.Modern-day object detectors can be categorised into two\n",
      "types: one-stage (YOLO [8], [9], SSD [10], and CenterNet\n",
      "[11]) and two-stage (R-CNN series including Fast R-CNN\n",
      "[22], Faster R-CNN [5], R-FCN [23], and Libra R-CNN [24]).Two-stage detectors generally split the overall object detection\n",
      "task as follows: the ﬁrst stage is to generate proposals, and then\n",
      "the second stage focuses on the veriﬁcation and recognition\n",
      "of these proposals [25].The two-stage detectors are typically\n",
      "slower due to their heavy-head design but detect with a higher\n",
      "level of accuracy [25], whilst one-stage approaches tend to be\n",
      "faster but can be more limited in terms of their predictive\n",
      "performance [26].B.Distracted Driver Identiﬁcation\n",
      "Recent years have seen a rise in the number of distracted\n",
      "driver identiﬁcation systems.\n",
      "翻譯結果: 直到2015年，隨著更快速區域神經網路（Faster R-CNN） [5]的開發，物體檢測才能在實時視頻中被利用。Faster R-CNN是其前身R-CNN [21] 在精確度和速度方面的改進。現代物體檢測器可以分為兩種類型：單階段（YOLO [8]，[9]，SSD [10]和CenterNet [11]）和雙階段（包括Fast R-CNN [22]，Faster R-CNN [5]，R-FCN [23]和Libra RCNN [24] 的 R-CNN 系列）。 雙階段檢測器通常將整個物體檢測任務分成以下兩個階段：第一階段是生成提議，然後第二階段專注於驗證和識別這些提議 [25]。由於重量設計較重，雙階段檢測器通常較慢，但具有更高的精確度 [25]，而單階段方法往往更快，但在預測性能方面可能更有限 [26]。B.分心駕駛識別 最近幾年，分心駕駛識別系統的數量不斷增加。\n",
      "原文: One such approach [27] utilises\n",
      "current infrastructure using LPR [16] roadside cameras.The\n",
      "system contains a three-stage approach to detecting whether\n",
      "the driver is using their phone.The ﬁrst stage is to detect\n",
      "the windscreen, which is then cropped and processed in the\n",
      "1https://github.com/carrell-ncl/Windscreen2\n",
      "翻譯結果: 其中一種方法是透過利用道路旁邊的車牌辨識攝影機、使用目前的基礎架構，建立一套系統 [27]。該系統有三個步驟，用以判斷司機是否使用手機。第一個步驟是偵測車輛的擋風玻璃，接著對其進行裁剪和處理。1https://github.com/carrell-ncl/Windscreen2\n",
      "原文: second model to identify whether a person can be clearly\n",
      "seen.This process is in place to ensure that the images with\n",
      "undesirable reﬂection effects are not processed.The ﬁnal\n",
      "stage will then detect for mobile phone usage [27].The\n",
      "authors recognise a limitation of this system [27]; the images\n",
      "could not be acquired during summer days between 12:00 and\n",
      "15:00 due to excessive amount of windscreen glare.Initial\n",
      "testing for this work shows windscreen glare for the majority\n",
      "of the day (as discussed in Section III-A1), suggesting that\n",
      "this would not be appropriate for some territories.Fig.2: The three-stage mobile usage violation detection ap-\n",
      "proach of Alkan et al.[27].Another study builds upon a software already developed\n",
      "by the Dutch Police, which ﬁrst looks for a licence plate,\n",
      "based on which it detects and outputs the driver’s side of the\n",
      "windscreen [28].These images are used as inputs to the trained\n",
      "model where hands, phone and face are detected.The image of\n",
      "the hand (taken from bounding box) is classiﬁed using VGG-\n",
      "16 [29].\n",
      "翻譯結果: 第二個模型的目的是辨別人物是否清晰可見，以確保未被不良反射效果影響的圖像不會被處理。最終階段會檢測手機的使用情況[27]。作者指出了系統的一個局限性[27]，即由於擋風玻璃反光太強，無法在夏季的12:00至15:00期間取得圖像。初步測試顯示大部分時間都有擋風玻璃反光（如第III-A1節所述），因此對某些地區來說可能不適用。圖2：Alkan等人[27]的三階段手機使用違規檢測方法。另一項研究則是在荷蘭警方開發的軟件基礎上進行的，該軟件首先尋找車牌，然後檢測和輸出駕駛員的擋風玻璃[28]。這些圖像作為訓練模型的輸入，檢測手、電話和臉部。手的圖像（從邊界框中取得）使用VGG-16進行分類[29]。\n",
      "原文: The model looks to eliminate the issue of falsely\n",
      "classifying objects such as phone mounts.This is done by\n",
      "checking where the phone is positioned in relation to the head\n",
      "and hand: if the distance is greater than a set threshold, then it\n",
      "is not classiﬁed.The system is dependent on the Dutch Police\n",
      "windscreen detector, which only works for Dutch Plates.Due\n",
      "to the reliance on a third party software to make this work,\n",
      "there is a lack of control on a signiﬁcant portion of the overall\n",
      "approach.There could be issues with support further down the\n",
      "line, or changes to licensing.It seems that more work would\n",
      "be required in order for this system to be deployable.Another work monitors the driver using their phone in\n",
      "addition to hand position [30].This approach uses cameras\n",
      "inside the vehicle positioned towards both the driver and\n",
      "the steering wheel.They propose a Multiple Scale Faster R-\n",
      "CNN [5] to detect both mobile phone and hands.Geometric\n",
      "information is then extracted to determine if the driver is using\n",
      "their phone [30].\n",
      "翻譯結果: 該模型旨在解決誤判像手機支架之類的物體的問題。這是通過檢查手機相對於頭部和手的位置來完成的：如果距離大於設定的閾值，則不進行分類。該系統依賴於荷蘭警察的擋風玻璃檢測器，該檢測器僅適用於荷蘭車牌。由於依賴第三方軟件使其運作，整個方法的控制力受到限制。在支援方面可能會出現問題，或者許可證可能會更改。似乎需要更多的工作才能使該系統可部署。另外一項工作監測司機使用手機以及手的位置[30]。該方法使用放置在車內朝向駕駛員和方向盤的相機。他們提出了一種多尺度更快的R-CNN [5]來檢測手機和手。然後提取幾何信息以判斷駕駛員是否正在使用手機[30]。\n",
      "原文: Mass deployment of this system could prove\n",
      "costly and impractical due to its reliance on cameras within\n",
      "the vehicle as well as the drivers being aware that they are\n",
      "being monitored.Our proposed system looks to solve the limitations of prior\n",
      "work starting with the issues of windscreen glare where the\n",
      "use of a polarising ﬁlter on the camera lens has been explored.The trained object detector is designed to work off a standard\n",
      "video surveillance roadside camera and as a result, limit the\n",
      "cost of deployment.The next section describes our approach\n",
      "for building this system in more detail.Object Detector Backbone Image Resolution\n",
      "YOLOv3 [3] Darknet-53 [3] 320, 416, 512\n",
      "YOLOv4 [9] CSPDarknet-53 [9] 320, 416, 512\n",
      "Faster R-CNN [5] Resnet101 [31] 640\n",
      "Faster R-CNN [5] Resnet152 [31] 640\n",
      "Centernet [11] Resnet101 FPN [31] 512\n",
      "SSD [10] Mobilenetv2 FPNLite [32] 640\n",
      "SSD [10] ResNet50 V1 FPN [31] 640\n",
      "SSD [10] ResNet101 V1 FPN [31] 640\n",
      "TABLE I: Chosen object detectors and pre-trained base models\n",
      "(Backbone) ﬁne-tuned and evaluated as part of this work.\n",
      "翻譯結果: 這個系統的大規模部署可能會因其依賴車輛內部相機和司機知道他們正在接受監控而變得昂貴和不切實際。我們提出的系統旨在解決先前工作的限制，首先解決擋風玻璃反光的問題，其中已經探討了在相機鏡頭上使用偏振濾鏡的方法。經過訓練的物體檢測器設計為運行在標準的視頻監控路邊攝像頭上，從而限制了部署的成本。下一部分詳細描述了我們構建此系統的方法。物體檢測器骨幹圖像分辨率YOLOv3 [3] Darknet-53 [3] 320、416、512YOLOv4 [9] CSPDarknet-53 [9] 320、416、512Faster R-CNN [5] Resnet101 [31] 640Faster R-CNN [5] Resnet152 [31] 640Centernet [11] Resnet101 FPN [31] 512SSD [10] Mobilenetv2 FPNLite [32] 640SSD [10] ResNet50 V1 FPN [31] 640SSD [10] ResNet101 V1 FPN [31] 640表1：本文選擇的物體檢測器和預訓練基礎模型（骨幹）作為本工作的精細調整和評估。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: III.A PPROACH\n",
      "Here, we describe the steps taken to build the proposed so-\n",
      "lution of a fully-automated system to detect driver violations.We propose two methods for achieving this: a single-step and\n",
      "a two-step approach (Figure 3), where by step we refer to\n",
      "a dedicated trained model in the overall system architecture.The single-step model is trained to detect both a licence plate\n",
      "and a person using their phone from a single image input\n",
      "in one forward pass through the model.A key advantage of\n",
      "this approach is that running a single model to complete the\n",
      "entire task at once results in a more light-weight faster system.A potential limitation of this method, however, would be the\n",
      "trade-off with accuracy as the trained model will be attempting\n",
      "to detect a very small object within a large image.To remedy this issue, we also propose a two-step solution,\n",
      "which ﬁrst detects the windscreen of the vehicle, and then uses\n",
      "the cropped image of only the driver side as the input for the\n",
      "next step to search for the phone.\n",
      "翻譯結果: III. 方法\n",
      "\n",
      "在這裡，我們描述建立完全自動化的系統以偵測駕駛員違規的步驟。我們提出了兩種實現方法：一步和二步方法（圖3），其中一步指的是整個系統架構中訓練的一個專用模型。單步模型訓練用於從單個影像輸入中同時檢測車牌和使用手機的人。這種方法的主要優點在於使用單一模型一次完成整個任務會導致系統更輕量級和更快速。然而，這種方法的潛在局限性是模型訓練試圖從大圖像中檢測非常小的對象，因此可能會產生精度方面的折衷。為了解決這個問題，我們還提出了一種二步解決方案，它首先檢測車輛的擋風玻璃，然後使用僅包含駕駛員側面的裁剪圖像作為下一步搜索手機的輸入。\n",
      "原文: An overview of the process\n",
      "of the two-step approach is seen in Figure 3.To enable a rigorous analysis and provide insight into the\n",
      "requirements of such an automated distracted driver identiﬁca-\n",
      "tions system, both the proposed one-step and two-step models\n",
      "will be evaluated for accuracy and speed.To decide upon the best object detection model, we evaluate\n",
      "four popular frameworks with various backbone models and\n",
      "image input sizes [3], [5], [9], [10].In total, we ﬁne-tune\n",
      "12 pre-trained models with our custom dataset, where we\n",
      "have phone and licence plate for the single-step method,\n",
      "and windscreen and phone for the two-step one.The chosen\n",
      "architectures used in this project are listed in Table I.To train and evaluate the models, images are acquired using\n",
      "high-end (Aviglon2, Axis3) and low-end (ELP4) cameras under\n",
      "varying weather conditions.Details of all equipment used can\n",
      "be found in Table II.Figure 4 demonstrates the difference in\n",
      "quality between a high and low-end camera, where all other\n",
      "conditions are identical.\n",
      "翻譯結果: 雙步驟方法的概述如圖3所示。為了進行嚴謹的分析並洞察這樣一個自動化分心駕駛員識別系統的要求，我們將評估提出的一步和雙步模型的準確性和速度。為了選擇最佳的對象檢測模型，我們評估了四個流行的框架，使用不同的backbone模型和圖像輸入尺寸[3]，[5]，[9]，[10]。總共，我們微調了12個預訓練模型與我們的自定數據集，其中在單步方法中有電話和許可證，而在雙步方法中有風檻和電話。本項目中使用的選擇的體系結構列於表I中。為了訓練和評估模型，使用高端（Aviglon2, Axis3）和低端（ELP4）攝像頭在不同的天氣條件下獲取圖像。所有使用的設備的詳細信息可以在表II中找到。圖4演示了高端和低端攝像頭之間的質量差異，其中所有其他條件相同。\n",
      "原文: We discuss the camera and hardware\n",
      "setup in the following section.A.Camera and hardware setup\n",
      "To successfully develop an automated mobile phone use\n",
      "detection system, a practical feasibility study is required to\n",
      "2https://www.avigilon.com/\n",
      "3https://www.axis.com/en-gb\n",
      "4http://www.elpcctv.com/\n",
      "翻譯結果: 以下是相機和硬件設置的討論。\n",
      "\n",
      "A.相機和硬件設置\n",
      "為了成功開發自動手機使用檢測系統，需要進行實際可行性研究。\n",
      "\n",
      "2https://www.avigilon.com/\n",
      "3https://www.axis.com/en-gb\n",
      "4http://www.elpcctv.com/\n",
      "原文: Detect \n",
      "windscreen Input Image Crop driver side \n",
      "of windscreenDetect phoneOutput image with \n",
      "overlayYOLOv3 -\n",
      "320 \n",
      "Model 1\n",
      "YOLOv3 -\n",
      "320 \n",
      "Model 2\n",
      "Fig.3: Two-step approach using YOLOv3 with input size of 320 \u0002320.Each input frame resized to 320 \u0002320 then passed\n",
      "through the ﬁrst model to detect windscreen.Image cropped on driver side of the windscreen is then resized to 320 \u0002320.Cropped image is passed through second model to detect phone.Output image is the original frame with overlay of predicted\n",
      "windscreen and phone bounding boxes.(a) High-end camera example.(b) Low-end camera example.Fig.4: Two images taken under the same conditions, using\n",
      "both high-end and low-end cameras.ensure that images of a high enough quality could be captured\n",
      "through a car windscreen in all weather conditions.It is\n",
      "highlighted in the work of Alkan et al.[27] that images\n",
      "captured in certain hours of the day present the challenge\n",
      "of windscreen glare, whilst night-time and poorly-lit areas\n",
      "can result in dark unusable images.\n",
      "翻譯結果: 偵測車載攝影機畫面中的風擋玻璃，裁切車輛駕駛座旁邊的畫面，並使用 YOLOv3 模型進行辨識。原始影像會加上預測出的風擋玻璃和手機框框的疊加圖層，輸出辨識結果。圖 3 為使用輸入大小為 320*320 的 YOLOv3 兩階段辨識方法，將初始影像調整成 320*320 後，通過第一個模型進行風擋玻璃辨識。然後在風擋玻璃上裁切出駕駛座旁的畫面再縮放成 320*320，再通過第二個模型進行手機辨識。圖 4 則展示了使用高端和低端攝影機拍攝在相同條件下的效果，要確保在所有天氣條件下都能夠拍攝到足夠高質量的影像。Alkan 等人的工作[27]中強調部分時間所拍攝的影像存在著風擋玻璃反射和夜間及照明不佳區域的黑暗無用影像等問題。\n",
      "原文: We also acknowledge\n",
      "that tinted windscreens may result in cameras not having\n",
      "reasonable visibility into the vehicle.While this can simply\n",
      "be resolved by having the camera in night-mode and using\n",
      "excessive amounts of IR, this issue is not common in the UK\n",
      "due to legal restrictions, so it is beyond the scope of this work.The primary objective of this paper is to not just create\n",
      "an object detector that could capture phone usage violations,\n",
      "but one that could do this during all hours of the day.In\n",
      "this section, we address the following challenges and propose\n",
      "solutions:\n",
      "1) Windscreen Glare: One of the most difﬁcult challenges\n",
      "when trying to see inside the vehicle is windscreen glare.This will usually occur when the sun is in a particular part\n",
      "Fig.5: Image during sunny/cloudy day without a polarising\n",
      "ﬁlter to show the adverse effects of windscreen reﬂection.(a) Without polarizing ﬁlter.(b) With polarizing ﬁlter.Fig.6: Images passed through YOLOv3-416 object detector\n",
      "to detect a person.Top image without a polarizing ﬁlter is\n",
      "completely unable to detect the person.\n",
      "翻譯結果: 我們也承認，有色擋風玻璃可能導致相機無法合理地看到車內情況。雖然這可以通過將相機設置為夜視模式並使用大量紅外線來解決，但由於法律限制，這個問題在英國不常見，因此超出了本研究的範圍。本文的主要目標不僅是創造一個可以捕捉手機使用違規行為的物體檢測器，而是一個可以在白天的所有時間做到這一點。在本節中，我們解決以下挑戰並提出解決方案：\n",
      "1）擋風玻璃的反光：當試圖觀察車內情況時，最困難的挑戰之一是擋風玻璃的反光。通常，這會在太陽處於特定位置時發生。圖5顯示了沒有偏振濾鏡的晴天/多雲天的圖像，展示了擋風玻璃反射的不良影響。（a）為未使用偏振濾鏡的圖像。（b）為使用偏振濾鏡的圖像。圖6顯示了通過YOLOv3-416物體檢測器檢測人的圖像。未使用偏振濾鏡的上圖完全無法檢測到人。\n",
      "原文: Bottom image with the\n",
      "polarizing ﬁlter is detecting the person with 91% conﬁdence.of the sky and can be made worse when clouds are present\n",
      "as they can be reﬂected quite signiﬁcantly on the windscreen.An example of how glare and cloud reﬂection can completely\n",
      "obstruct the view into the vehicle from the windscreen can be\n",
      "seen in Figure 5.Our preliminary tests found that the issue of\n",
      "glare would occur during the majority of the day.We can solve this issue by using a polarising ﬁlter which is\n",
      "ﬁxed to the camera lens.The effectiveness of this solution is\n",
      "demonstrated in Figure 6, which shows the same image taken\n",
      "翻譯結果: 使用偏光濾鏡的底圖像可以在91％的信心水平下檢測到人物，但在存在雲層時，它可以變得更糟，因為雲層可以在擋風玻璃上反射相當明顯。如圖5所示，眩光和雲層反射如何完全阻擋了從擋風玻璃進入車輛的視野。我們的初步測試發現，眩光問題在大多數時間內都會發生。我們可以使用固定在相機鏡頭上的偏光濾鏡來解決這個問題。解決方案的有效性在圖6中展示，該圖顯示了拍攝相同圖像的情況。\n",
      "原文: Equipment Make Model number Resolution/Wavelength Lens Origin\n",
      "Camera Avigilon 2.0C-H5A-B1 2MP 4.7 - 84.6mm Canada\n",
      "Camera Axis P1353 1.3MP 5-50mm Sweden\n",
      "Camera ELP ELP-USB-FHD01M-SFV 2MP 5-50mm China\n",
      "Infra-Red Raytec V AR2-i8-1 850nm 10 Degrees UK\n",
      "Infra-Red Raytec V AR2-i8-1-730 730nm 10 Degrees UK\n",
      "TABLE II: List of the cameras, IR and lens setup used in this work.(a) 850nm IR.(b) 730nm IR.Fig.7: Night images with active IR - 850nm vs 730nm.both without and with a polarising ﬁlter.We tested the impact\n",
      "of the polarising ﬁlter on the overall system by running the\n",
      "images through a pre-trained YOLOv3 model to see if it can\n",
      "detect the person inside the vehicle.We see from the images\n",
      "that it cannot detect the person when no ﬁlter is used, but\n",
      "detects the person with 91% conﬁdence when the polarising\n",
      "ﬁlter is added, which points to the importance of including a\n",
      "polarising ﬁlter with the system hardware.2) Low-light conditions: In order for the system to func-\n",
      "tion successfully in low-light conditions, we would need to\n",
      "consider an appropriate light source.\n",
      "翻譯結果: 設備 品牌 型號 解析度/波長 鏡頭 來源\n",
      "攝影機 Avigilon 2.0C-H5A-B1 2MP 4.7-84.6mm 加拿大\n",
      "攝影機 Axis P1353 1.3MP 5-50mm 瑞典\n",
      "攝影機 ELP ELP-USB-FHD01M-SFV 2MP 5-50mm 中國\n",
      "紅外線 Raytec V AR2-i8-1 850nm 10度 英國\n",
      "紅外線 Raytec V AR2-i8-1-730 730nm 10度 英國\n",
      "\n",
      "表格II：本研究所使用的攝影機、紅外線和鏡頭設置列表。(a)850nm紅外線。(b)730nm紅外線。圖7：使用主動紅外線拍攝的夜間圖像 - 850nm vs 730nm。使用偏振濾鏡和不使用時的差異。我們通過運行預先訓練的YOLOv3模型來測試偏振濾鏡對整個系統的影響，以查看是否可以檢測到車內的人物。從圖像中，我們可以看出，在不使用濾鏡時無法檢測到人，但在加入偏振濾鏡後，可以以91%的置信度檢測到人物，這表明在系統硬件中包括偏振濾鏡的重要性。2)低光照條件：為了使系統在低光照條件下成功運作，我們需要考慮適當的照明源。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: Directional white visible\n",
      "light is not used with the camera as it is considered to be a risk\n",
      "of causing glare to the driver.Instead, active Infra Red (IR)5is\n",
      "used, where two different wavelengths are tested: 850 and 730\n",
      "nanometers (nm).Figure 7 demonstrates both wavelengths of\n",
      "IR working well in low-light conditions, however it is clear\n",
      "that the camera is able to capture more details using 730nm.Details of the IR used for this work can be found in Table II.It is noted that IR can also be used during the day with the\n",
      "camera in monochrome setting, though more IR is required\n",
      "due to the higher ambient light levels.Further studies would be\n",
      "useful in determining the optimal IR power, typically measured\n",
      "in\u0016W/cm2, but this is outside the scope of this work.The deployed system will be designed to expect both\n",
      "RGB (day) and monochrome images (night).A proportion\n",
      "of monochrome images taken with IR have been used in the\n",
      "training and testing of the model.5https://www.rayteccctv.comB.Dataset\n",
      "This section describes the custom dataset used to train and\n",
      "test the models used in the proposed system.\n",
      "翻譯結果: 在相機中不會使用定向的白色可見光，因為這被認為會對司機產生眩光風險。相反，使用主動紅外線（IR）5，其中測試了兩種不同的波長：850和730納米（nm）。圖7展示了在低光條件下兩種不同波長的IR均能良好工作，但顯然相機使用730nm可以捕捉更多的細節。關於此工作中使用的IR的詳細信息可以在表II中找到。注意，IR也可以在白天與相機設置為單色模式時使用，但由於環境光線較高，因此需要更多的IR。進一步的研究有助於確定最優的IR功率，通常使用\u0016W/cm2進行測量，但這超出了本工作的範圍。部署的系統將被設計為期望使用RGB（白天）和單色影像（晚上）。使用IR拍攝的部分單色影像已用於模型的培訓和測試中。5 https://www.rayteccctv.com B.數據集 本節描述了用於訓練和測試所提出系統中使用的自定義數據集。\n",
      "原文: 1) Training images: For the single-step approach, the\n",
      "dataset consists of 2,150 images of phone and 2,235 images\n",
      "of licence plates.The licence plate images are obtained from\n",
      "the Google Open Images Dataset [33].For the phone class, we\n",
      "used a small portion of mobile phone stock images in order\n",
      "to pick up simple detections.However, the main proportion\n",
      "of images would need to be obtained/created speciﬁcally for\n",
      "this project with a mixture of quality, weather conditions,\n",
      "and distance to best represent the real-world scenarios (Figure\n",
      "8).Despite the class being labeled “phone”, the majority of\n",
      "what is actually being detected is multiple variations of hand\n",
      "positions holding a phone.In order to ensure that our model\n",
      "is trained to detect these types of images, 1,700 of the 2,150\n",
      "phone images are obtained speciﬁcally for this project.The two-step approach required obtaining 263 images of ve-\n",
      "hicle windscreens, with some of these images having multiple\n",
      "vehicles, so the total number of windscreens annotated is 487.\n",
      "翻譯結果: 1) 訓練圖像：對於單步方法，該數據集包括2,150張手機圖像和2,235張車牌圖像。車牌圖像是從 Google 的開放圖像數據集[33] 中獲得的。對於手機類別，我們使用了手機庫存圖像的一小部分，以便進行簡單的檢測。然而，大部分圖像需要為此項目特別獲得/創建，以混合質量、天氣條件和距離來最好地代表現實世界的情況（圖8）。儘管該類被標記為“手機”，但實際上被檢測的大部分是多種手持手機的不同姿勢變化。為了確保我們的模型訓練能夠檢測這些類型的圖像，其中1,700張手機圖像是特別為此項目獲得的。兩步驟方法需要獲取263張車輛擋風玻璃的圖像，其中一些圖像有多輛車，因此註釋的總數為487。\n",
      "原文: These images are used to train the ﬁrst model in the two-step\n",
      "pipeline, which will be used to crop the vehicle windscreens.The second model is trained using only the phone images.2) Test images: The images used to evaluate the trained\n",
      "models consist of 216 images of a person using their phone\n",
      "whilst driving.It is important to note that due to there being no\n",
      "access to public trafﬁc camera footage, the test images were\n",
      "taken with the aid of volunteers to best represent that of a\n",
      "real-world application.These were obtained using a mixture\n",
      "of high-end and low-end cameras (Table II).The videos used\n",
      "to obtain these test images were not used in the training of the\n",
      "object detectors.Test images for the two-step approach were\n",
      "obtained by cropping out only the windscreen of these same\n",
      "images.C. Training the object detectors\n",
      "One of the main challenges for successfully detecting a\n",
      "person using their phone is the ability for the system to\n",
      "detect small objects with signiﬁcant variation.To address this,\n",
      "multiple object detection methods are trained and evaluated.\n",
      "翻譯結果: 這些圖像用於訓練兩步驟管線中的第一個模型，該模型將用於裁剪車輛擋風玻璃。第二個模型僅使用手機圖像進行訓練。2）測試圖像：用於評估訓練模型的圖像包括216張人們開車時使用手機的圖像。值得注意的是，由於無法訪問公共交通攝像頭的影像，因此測試圖像是由志願者協助拍攝，以最好地代表現實應用的情況。這些圖像是使用高端和低端相機（表II）混合獲取的。用於獲取這些測試圖像的視頻未用於訓練物體檢測器。兩步驟方法的測試圖像是通過裁剪出相同圖像的擋風玻璃獲取的。C.訓練物體檢測器檢測一個人使用手機的主要挑戰之一是系統能夠檢測具有顯著變化的小物體。為了解決這個問題，多種物體檢測方法正在得到訓練和評估。\n",
      "原文: The same trained models are used to evaluate both the single-\n",
      "step and two-step approaches.A variety of pre-trained base networks (backbones) can be\n",
      "chosen depending on the object detector used, such as ResNet\n",
      "[31], VGG16 [29], Inception [34] and MobileNet [32].YOLO\n",
      "typically uses Darknet53 (YOLOv3) [8] and CSPDarknet53\n",
      "(YOLOv4) [9].All these base networks with the exception\n",
      "of MobileNet, are typically used when running on a GPU\n",
      "翻譯結果: 相同的訓練模型被用於評估單步和雙步方法。根據使用的物體檢測器，可以選擇多種預訓練基礎網絡（backbones），例如ResNet [31]、VGG16 [29]、Inception [34]和MobileNet [32]。YOLO通常使用Darknet53（YOLOv3）[8]和CSPDarknet53（YOLOv4）[9]。除了MobileNet之外，所有這些基礎網絡通常在運行GPU時使用。\n",
      "原文: Fig.8: Bespoke RGB and monochrome training images of varying quality obtained speciﬁcally for this work.platform [10].Here, we choose to include a MobileNet base\n",
      "network as one of our trained models to see how effective a\n",
      "low-cost light-weight detector could preform.An application\n",
      "such as this may beneﬁt from running on an edge device\n",
      "[35] where a more light-weight model optimised for smaller\n",
      "computational resources would be preferred.1) YOLOv3 and YOLOv4: Results from other studies [8],\n",
      "[9] concluded that the accuracy on the higher resolutions\n",
      "would yield greater accuracy, and similarly that the lower\n",
      "resolution models would run at a higher frame rate.Based\n",
      "on this, we opt for training both YOLOv3 and YOLOv4 with\n",
      "input resolutions of 512 \u0002512, 416 \u0002416 and 320 \u0002320.For the\n",
      "two-step approach, we would like to see if accuracy would\n",
      "be impacted much on the lower resolution models dealing\n",
      "with: uniform objects such as windscreens, and low resolution\n",
      "cropped images taken from the windscreen.\n",
      "翻譯結果: 圖8：特別為此項工作獲得的不同品質的定制RGB和單色訓練圖像。在這裡，我們選擇使用MobileNet基礎網絡作為我們訓練模型之一，以了解低成本輕量級檢測器的有效性。這樣的應用程序可能會受益於在邊緣設備上運行，[35]這裡使用優化為更小計算資源的輕量級模型。1）YOLOv3和YOLOv4：其他研究[8] [9]的結果表明，高分辨率的準確度會更高，同樣，低分辨率模型的幀率更高。基於此，我們選擇使用512×512、416×416和320×320的輸入分辨率來訓練YOLOv3和YOLOv4。對於兩步驟方法，我們希望看到對於處理均勻物體（如擋風玻璃）和從擋風玻璃上拍攝的低分辨率裁剪圖像的低分辨率模型，精度是否會受到太大影響。\n",
      "原文: 2) Faster R-CNN, SSD, and Centernet: The remaining\n",
      "frameworks are obtained and ﬁne-tuned using the TensorFlow\n",
      "Object Detection API [36], [37], this give us access to many\n",
      "different pre-trained models that could be ﬁne-tune on our\n",
      "custom dataset.D. Evaluating the models\n",
      "To replicate a real-world scenario, the majority of the test\n",
      "images are obtained at a distance of 20-30m from the camera,\n",
      "with a height of approximately 3m.These images were cap-\n",
      "tured during different times of the day under varying weather\n",
      "conditions to enable testing the generalisation capabilities of\n",
      "the system as well as its predictive performance.Test images for evaluation are split into 2 sets, the ﬁrst\n",
      "using 216 full image snapshots taken from the camera, the\n",
      "next with the same images but this time with only the cropped\n",
      "windscreen.This allows us to determine if the model will\n",
      "perform more favourably with the single-step or the two-step\n",
      "system.IoU = ~0.2 IoU = ~0.8False Positive True Positive \n",
      "Fig.9: Example of False Positive (left) and True Positive\n",
      "(right) when IoU threshold set to >0.5.\n",
      "翻譯結果: 2) Faster R-CNN、SSD 和 Centernet: 其餘的框架是使用 TensorFlow 物件檢測 API [36]、[37] 獲得並進行微調，這使我們可以使用許多不同的預先訓練模型，在我們的自定義數據集上進行微調。D. 評估模型為了複製真實世界的情況，大部分的測試圖像是以距離攝像機20-30米的高度約為3米的高度獲得的。這些圖像是在不同時間拍攝的，天氣條件也不同，以便測試系統的一般化能力以及其預測性能。評估用的測試圖像分為2組，第一組使用從攝像機拍攝的216個完整圖像快照，下一組使用相同的圖像，但這次只使用截取的擋風玻璃。這樣可以確定模型在單步或雙步系統中的表現更好。IoU = ~0.2 IoU = ~0.8假陽性真陽性\n",
      "圖9: IoU閾值設置為>0.5時，假陽性（左）和真陽性（右）的示例。\n",
      "原文: Green bounding box\n",
      "represents the ground truth, red bounding box is the prediction.1) Evaluation Metrics: A good metric to evaluate object\n",
      "detectors is Mean Average Precision (mAP) [13].However,\n",
      "as we are only concerned with the accuracy of a single class\n",
      "(Phone), we evaluate the approach using Average Precision\n",
      "(AP) [12].The ﬁrst part of this process is collecting the\n",
      "sequences of True Positives (TP) and False Positives (FP) from\n",
      "the predictions made on the test images.In object detection, a\n",
      "TP is determined by the value of the Intersection Over Union\n",
      "(IoU).For example, if the minimum IoU requirement is >0.5\n",
      "(often referred to as mAP 50orAP50, where any predictions\n",
      "with IoU above this threshold are classed as TP and any below\n",
      "are FP [38] (Figure 9).Once these have been collected, they\n",
      "are sorted in descending order by the conﬁdence score, then\n",
      "precision and recall values are calculated using Equation 1.Precision =TP\n",
      "TP+FPRecall =TP\n",
      "TP+FN(1)\n",
      "The AP summarises the shape of the precision/recall curve,\n",
      "and is deﬁned as the mean precision at a set of eleven\n",
      "equally spaced recall levels [0, 0.1,..., 1] [12] (Equation 2).\n",
      "翻譯結果: 綠色方框表示實際值，紅色方框表示預測值。1）評估指標：評估物體檢測器的良好評估指標是平均精度均值（mAP）[13]。但是，由於我們只關心單個類別（手機）的準確性，因此我們使用平均精度（AP）[12]來評估方法。此過程的第一部分是從對測試圖像進行的預測中收集真陽性（TP）和假陽性（FP）的序列。在物體檢測中，TP由交集比聯集（IoU）的值確定。例如，如果最小IoU要求是>0.5（經常稱為mAP 50或AP50），則任何IoU超過此閾值的預測都被視為TP，任何IoU低於此值的預測都是FP[38]（圖9）。一旦這些被收集起來，它們就會根據信心得分按降序進行排序，然後使用公式1計算精確度和召回率值。精確度= TP /（TP + FP）， 召回率= TP /（TP + FN）（1）AP總結了精度/召回率曲線的形狀，並定義為一組等間隔召回率級別[0，0.1，...，1]的平均精度[12]（公式2）。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: The precision at each recall level ris interpolated by taking\n",
      "the maximum precision measured for a method for which the\n",
      "corresponding recall exceeds r[12] (Equation 2).In this work, we evaluate our proposed approach using\n",
      "both IoU>0.5 and IoU >0.1, hereby referred to as AP50\n",
      "andAP10respectively.We are attempting to ﬁnd very small\n",
      "objects (mobile phone) with signiﬁcant variation, so the lower\n",
      "IoU threshold would likely be more appropriate.To test the\n",
      "accuracy of our trained models, all the test images are supplied\n",
      "with given annotation ﬁles (ground truth bounding boxes).We\n",
      "then run these images through the object detectors to obtain the\n",
      "predicted bounding boxes.The AP is subsequently calculated\n",
      "based on both IoU thresholds.Next, we discuss evaluating the\n",
      "efﬁciency of the proposed approach.AP=1\n",
      "11X\n",
      "r2f0;0:1;0::;0:9;1gPinterp(r)\n",
      "wherePinterp(r)= max\n",
      "~r:~r\u0014rp(~r)(2)\n",
      "2) Evaluating frame rate: The system has been designed\n",
      "around the ability to take live video from trafﬁc cameras.It is\n",
      "therefore important that it not only detects with high accuracy,\n",
      "but with low latency, especially when dealing with moving\n",
      "vehicles.\n",
      "翻譯結果: 在每個召回水平的精度是通過使用插值來取得的，即通過取得對應召回率大於 r[12] 的方法的最大精度（公式 2）來進行插值。在這項工作中，我們使用 IoU>0.5 和 IoU>0.1 這兩個閾值來評估我們提出的方法，分別稱為 AP50 和 AP10。我們試圖找到非常小但變化巨大的物體（手機），因此較低的 IoU 閾值可能更適合。為了測試我們訓練模型的準確性，所有測試圖像都附帶有給定的注釋文件（真實的邊界框）。然後，我們通過目標檢測器運行這些圖像以獲取預測的邊界框，接著根據兩個 IoU 閾值計算 AP。接下來，我們討論評估所提出方法的效率。AP = 1\n",
      "11X\n",
      "r2f0;0:1;0::;0:9;1gPinterp(r)\n",
      "其中，Pinterp(r) = max ~r : ~r ≤ rp(~r) (公式 2)。\n",
      "\n",
      "2）評估幀速率：該系統的設計是圍繞著從交通攝像頭中拍攝實時視頻的能力。因此，它不僅需要以高精度檢測，還需要低延遲，特別是在處理移動車輛時。\n",
      "原文: Other studies conﬁrm that AP alone is not enough to\n",
      "evaluate object detectors, particularly when it comes to video\n",
      "object detection [39].Consequently, frames per second (FPS)\n",
      "is another metric used to evaluate the proposed system.To\n",
      "翻譯結果: 此外，其他研究也證實僅以平均精度(AP)為衡量物體偵測器的表現並不足夠，特別是在視頻物體偵測方面[39]。因此，每秒幀數(FPS)是另一個用於評估所提出系統的指標。\n",
      "原文: test speed of our trained object detectors, we run the same\n",
      "test video for each model and then calculate the average FPS.3) Choosing the best model: Once these metrics are cal-\n",
      "culated for each of the models, we then shortlist the top\n",
      "two for further evaluation.The test images are split into two\n",
      "categories, namely high-quality and low-quality.High-quality\n",
      "images are captured using the high-end cameras (Avigilon and\n",
      "Axis) to represent how the model should perform when the\n",
      "system has been built with a relatively larger budget in mind.The low-quality images are acquired using the low-end camera\n",
      "(ELP) to demonstrate how the model will perform under cost-\n",
      "effective considerations.Details of the cameras are listed in\n",
      "Table II.The speed in which these models perform also needs\n",
      "to be re-evaluated to incorporate the two-step windscreen\n",
      "method as well as the object tracking algorithm.4) Object tracking and data collection: A consideration\n",
      "when building a fully-automated system is how the phone\n",
      "violations are going to be recorded in a way that is useful\n",
      "to the end user.\n",
      "翻譯結果: 測試我們訓練過的物件偵測器的速度，我們會對每個模型進行相同的測試，然後計算平均FPS。3）選擇最佳模型：一旦這些指標都被計算出來，我們就會將前兩個筆記到進一步評估的名單中。測試圖片分為兩類，即高質量和低質量。高質量圖片使用高端相機（Avigilon和Axis）拍攝，以代表當系統建造時預算相對較大時模型應該表現的方式。低質量圖像使用低端攝像頭（ELP）獲得，以演示模型在成本效益考慮下的表現。攝像頭的詳細信息列在表II中。這些模型的運行速度還需要重新評估，以納入兩步式擋風玻璃方法以及物體跟踪算法。4）物體跟踪和數據收集：建立一個完全自動化的系統時，考慮到如何以對最終用戶有用的方式記錄手機違規行為是必要的。\n",
      "原文: To do this, the system would have to be\n",
      "able to distinguish between unique and duplicate detection.For example, a ﬁve second video may show one driver using\n",
      "their phone, but since the detections are done per frame, it\n",
      "may count duplicate violations for every one of these frames.In order to address this, we add DeepSort [14], which is an\n",
      "object tracking algorithm.This will add a unique ID for each\n",
      "detection and then takes each frame to predict if the next\n",
      "detection belongs to the same ID or not.In case of the single-\n",
      "step system, we can check every new detection to see if the ID\n",
      "has been seen before, then if not, can log as a new detection.For the two-step system, a phone violation is only logged once\n",
      "per unique windscreen ID.This same method also allows us\n",
      "to count the number of vehicles, taken from the licence plate\n",
      "on the single-step method and windscreen on the two-step.IV.E XPERIMENTAL RESULTS\n",
      "In this section, we evaluate our models using the experimen-\n",
      "tal setup and the metrics discussed in the previous section.\n",
      "翻譯結果: 為了達成此目的，系統必須能夠區分獨特和重複的偵測。例如，五秒鐘的視頻可能顯示一個司機使用手機，但由於偵測是每一幀進行的，它可能會對每一個幀都計算重複的違規行為。為了解決這個問題，我們添加了DeepSort [14]，這是一個物體跟蹤算法。這將為每個偵測添加一個唯一的ID，然後將每個幀預測下一個偵測是否屬於相同的ID。對於單步驟系統，我們可以檢查每個新的偵測是否已經見過該ID，如果沒有，則可以記錄為新的偵測。對於雙步驟系統，每個唯一的擋風玻璃ID只記錄一次手機違規行為。同樣的方法也允許我們計算車輛數量，從單步驟方法的車牌和雙步驟方法的擋風玻璃識別。在本節中，我們使用實驗設置和前一節中討論的指標來評估我們的模型。\n",
      "原文: A.System Speciﬁcation\n",
      "Trained object detectors are tested and evaluated using the\n",
      "system speciﬁcation listed in Table III.Type Spec\n",
      "Processor AMD Ryzen 7 3800X\n",
      "GPU Nvidia RTX2080Ti\n",
      "Memory 32GB\n",
      "Operating System Windows 10\n",
      "Programming Language Python 3.8\n",
      "Machine Learning Platform TensorFlow 2.2\n",
      "TABLE III: System Speciﬁcations.B.Average precision\n",
      "For the ﬁrst accuracy test, we present the single-step method\n",
      "where the full test image is used in the model trained to detect\n",
      "the phone.Figure 10 shows the YOLO models outperforming\n",
      "the other object detectors.YOLOv4 with input size 512 is bestperforming on both AP50andAP10, whilst the SSD models\n",
      "are the poorest performing for both IOU thresholds.Next, we test the same models, but this time with the\n",
      "cropped windscreen images which will allow us to determine\n",
      "whether the two-step approach is more appropriate.Figure\n",
      "10 suggests that if accuracy was the main driver, the two-\n",
      "step method will be more favourable to use, giving higher\n",
      "AP in almost all of the trained object detectors.\n",
      "翻譯結果: A. 系統規格\n",
      "訓練完的物件偵測器是使用表III的系統規範進行測試和評估的。\n",
      "種類 規格\n",
      "處理器 AMD Ryzen 7 3800X\n",
      "GPU Nvidia RTX2080Ti\n",
      "記憶體 32GB\n",
      "作業系統 Windows 10\n",
      "程式語言 Python 3.8\n",
      "機器學習平台 TensorFlow 2.2\n",
      "表III: 系統規格\n",
      "\n",
      "B. 平均精準度\n",
      "在第一次精確度測試中，我們提供單步方法，其中完整的測試圖像用於訓練檢測手機的模型。\n",
      "圖10顯示了YOLO模型優於其他物件偵測器。 YOLOv4使用512作為輸入大小，在AP50和AP10上表現最佳，而SSD模型在兩個IOU閾值下表現最差。\n",
      "接下來，我們測試相同的模型，但這次使用裁剪過的擋風玻璃圖像，這將允許我們確定兩步方法是否更適合。圖10表明，如果精度是主要的驅動因素，則兩步法將更適合使用，在幾乎所有訓練過的物件偵測器中都具有更高的AP。\n",
      "原文: Again, the\n",
      "YOLO models yield the highest accuracy scores.YOLOv3\n",
      "with the larger input size gives the best results with AP50,\n",
      "whilst YOLOv4 with the input resolution of 416 gives the\n",
      "highest accuracy for AP10.Once again, accuracy for all\n",
      "3 SSD frameworks are the lowest by a signiﬁcant margin.Another study [10] points out that, in SSD object detectors,\n",
      "regardless of which base network you use, it will still retain\n",
      "the original characteristics of the SSD.Therefore, accuracy\n",
      "on small objects will not be as accurate compared to other\n",
      "two-stage models such as R-CNN.Review of the predictions made on the test images when\n",
      "IOU threshold is set to >0.5 shows multiple false positives\n",
      "despite the predicted bounding box surrounding the correct\n",
      "object.As mentioned previously, the objects that the model\n",
      "is attempting to predict have signiﬁcant amounts of variation,\n",
      "meaning that it will always be difﬁcult to get a high IOU\n",
      "score.Figure 12 shows a false positive prediction where we\n",
      "have the IOU threshold set to >0.5.\n",
      "翻譯結果: 再一次，YOLO模型取得了最高的準確度分數。在AP50方面，輸入大小較大的YOLOv3給出了最佳結果，而輸入解析度為416的YOLOv4給出了AP10的最高準確度。同樣地，所有3個SSD框架的準確度都明顯低於其他模型。另一份研究[10]指出，在SSD物體偵測器中，不論使用哪個基礎網絡，它仍然會保留SSD的原始特徵。因此，對於小物體的準確度不會像其他二階段模型（如R-CNN）一樣高。當IOU閾值設置為>0.5時，對測試圖像進行的預測檢查顯示出多個假陽性，儘管預測的邊界框圍繞著正確的物體。如前所述，模型嘗試預測的物體具有顯著的變化，這意味著要獲得高IOU分數總是很困難的。圖12顯示了一個假陽性預測，其中IOU閾值設置為>0.5。\n",
      "原文: We can see that the model\n",
      "is capturing the violation correctly, but narrowly missing the\n",
      "IOU threshold resulting in a false positive.Based on this, we\n",
      "propose an IOU threshold of >0.1 for this application.C. Frame rate\n",
      "Speed of the trained models is evaluated using a 70-second\n",
      "test video.These initial frame rate evaluation tests are done\n",
      "on detection only prior to adding the tracking algorithm and\n",
      "the two-step approach.As seen in Figure 11, the single-stage (YOLO, SSD, Cen-\n",
      "terNet) object detectors are signiﬁcantly faster than the two-\n",
      "stage R-CNN detectors.As expected, the most efﬁcient object\n",
      "detector is the low-cost SSD Mobilenet FPNLite 640 with\n",
      "43 FPS, however lacking in accuracy.Each of the YOLO\n",
      "models seem to perform consistently well with regards to both\n",
      "accuracy and speed with YOLOv3 320 performing the best at\n",
      "29 FPS.D. Output images\n",
      "Figure 13 shows a number of sample results obtained\n",
      "from the YOLOv3-320 model for the two-step method; blue\n",
      "bounding box refers to the ground truth, green is the true\n",
      "positive prediction, and red is the false positive perdition.\n",
      "翻譯結果: 我們可以看到這個模型正確地捕捉到違規行為，但卻僅僅錯失了IOU閾值，從而產生了假陽性。基於此，我們建議此應用的IOU閾值應為>0.1。\n",
      "C. 帧速率\n",
      "使用70秒的測試視頻評估受訓模型的速度。在添加跟踪算法和兩步驟方法之前，僅對檢測進行了初始幀率評估測試。如圖11所示，單級（YOLO、SSD、CenterNet）物體檢測器比兩步驟R-CNN檢測器快得多。如預期，最有效的物體檢測器是低成本SSD Mobilenet FPNLite 640，其分辨率為43 FPS，但缺乏準確性。每個YOLO模型似乎在準確性和速度方面表現一致，其中YOLOv3 320以29 FPS的最佳表現最好。\n",
      "D. 輸出圖像\n",
      "圖13顯示了從YOLOv3-320模型中獲取的兩步驟方法的多個樣本結果；藍色邊界框是真實值，綠色是真陽性預測，紅色是假陽性預測。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: With regards to predictions made on the low-quality camera,\n",
      "although the model performs well, there appears to be a higher\n",
      "chance of false positive predictions.Having a high proportion of false positives for this particular\n",
      "application could result in incorrectly ﬁning individuals, or\n",
      "involve human intervention to manually go through all the\n",
      "violations, which is both costly and defeats the main purpose\n",
      "翻譯結果: 針對低品質相機所做的預測，在模型表現良好的情況下，似乎存在較高的假陽性預測機率。對於此特定應用程式，高比例的假陽性可能會導致錯誤罰款個人，或需要人工介入手動檢查所有違規情況，這既耗費成本又違背主要目的。\n",
      "原文: Object detector AP50 AP10 AP50cropped AP10cropped FPS (detection)\n",
      "YOLOv4 512 45.98 73.11 59.62 83.32 27.12\n",
      "YOLOv3 512 35.81 46.16 63.27 85.88 25.18\n",
      "YOLOv4 416 40.23 48.23 61.60 87.58 26.12\n",
      "YOLOv3 416 37.41 51.64 58.44 79.99 25.96\n",
      "YOLOv4 320 19.62 52.36 52.65 81.93 26.05\n",
      "YOLOv3 320 37.54 72.45 59.05 84.62 28.97\n",
      "Centernet ResNet101 512 43.81 56.18 50.04 66.6 35.9\n",
      "Faster R-CNN ResNet101 640 37.18 48.57 41.78 55.5 15.4\n",
      "Faster R-CNN ResNet152 640 34.85 44.48 40.89 50.57 11.5\n",
      "SSD Mobilenet FPNLite 640 12.23 18.23 12.64 28.41 42.77\n",
      "SSD ResNet50 FPN 640 2.77 7.77 0.92 4.22 23.66\n",
      "SSD ResNet101 FPN 640 0.6 4.86 0.71 13.9 22.2\n",
      "TABLE IV: Results of all the trained models showing average precision and frames per second.Cropped refers to images of\n",
      "the windscreen only to evaluate performance of the two-step approach.the frame per second (FPS) metric is based on detection\n",
      "only and does not include tracking and the two-step approach.(a) Single-step approach.(b) Two-step approach.Fig.10: Average precision for both IoU thresholds of 0.5 and 0.1 - single-step (left) & two-step (right).\n",
      "翻譯結果: 物件探測器 AP50 AP10 AP50裁剪 AP10裁剪 FPS（偵測）\n",
      "YOLOv4 512 45.98 73.11 59.62 83.32 27.12\n",
      "YOLOv3 512 35.81 46.16 63.27 85.88 25.18\n",
      "YOLOv4 416 40.23 48.23 61.60 87.58 26.12\n",
      "YOLOv3 416 37.41 51.64 58.44 79.99 25.96\n",
      "YOLOv4 320 19.62 52.36 52.65 81.93 26.05\n",
      "YOLOv3 320 37.54 72.45 59.05 84.62 28.97\n",
      "Centernet ResNet101 512 43.81 56.18 50.04 66.6 35.9\n",
      "更快的 R-CNN ResNet101 640 37.18 48.57 41.78 55.5 15.4\n",
      "更快的 R-CNN ResNet152 640 34.85 44.48 40.89 50.57 11.5\n",
      "SSD Mobilenet FPNLite 640 12.23 18.23 12.64 28.41 42.77\n",
      "SSD ResNet50 FPN 640 2.77 7.77 0.92 4.22 23.66\n",
      "SSD ResNet101 FPN 640 0.6 4.86 0.71 13.9 22.2\n",
      "表 IV：所有訓練模型的結果，顯示平均精度和每秒幀數。裁剪是指僅測試風擋玻璃圖像以評估兩步驟方法的性能。每秒幀數（FPS）指的是僅基於檢測的指標，不包括跟踪和兩步驟方法。（a）單步驟方法。（b）兩步驟方法。圖10：IoU閾值為0.5和0.1的平均精度-單步驟（左）和兩步驟（右）。\n",
      "原文: Fig.11: FPS for detection only for the trained object detectors.Fig.12: False positive result for AP50, which demonstrates\n",
      "that for the application in this work, AP10is more appropriate.of this work, which is to automate the pipeline.For the\n",
      "ﬁnal deployed model, it may be appropriate to increase the\n",
      "score threshold for the phone detector step to reduce these\n",
      "false positives.Observation from the predictions of the test\n",
      "set conﬁrm that AP10is appropriate for this application of\n",
      "detecting such small and difﬁcult images, as demonstrated in\n",
      "Figure 12.Based on the results shown in Table IV, the model chosen\n",
      "for deployment is YOLOv3 with the input size of 320 using\n",
      "the two-step method.Although it did not achieve the highest\n",
      "翻譯結果: 譯文：\n",
      "\n",
      "圖11：經過訓練的物件檢測器僅進行檢測的FPS。圖12：AP50的假陽性結果，顯示對於本研究的應用，在這種情況下，AP10更合適。本研究的目的是自動化流程。最終的部署模型可能需要增加手機檢測步驟的分數閾值，以減少這些假陽性。從測試集的預測結果中觀察到，對於這種檢測小而難以辨識的圖像的應用，AP10是合適的，如圖12所示。根據表IV顯示的結果，選擇用於部署的模型是YOLOv3，使用輸入尺寸為320的兩步方法。儘管未達到最高的可能效果，但這是最佳的選擇。\n",
      "原文: Fig.13: Results from YOLOv3 320 with AP10showing high\n",
      "accuracy but high false positives on low-quality images.Blue\n",
      "bounding boxes denote ground truth, green refers to the true\n",
      "positive prediction and red is the false positive perdition.overall accuracy, it came a close third behind YOLOv3-512\n",
      "and YOLOv4-416.The deciding factor is the speed of the\n",
      "model, as it is able to achieve almost 29 FPS (almost 11%\n",
      "faster then the next best performing model).For this type of\n",
      "application, the cameras typically monitors fast-moving trafﬁc.The model should consequently be able to make detections\n",
      "efﬁciently.Having a model with a smaller input size means it\n",
      "will be less expensive with regards to hardware demands.E. Integrating the two-step method and tracking\n",
      "The next stage in our overall system is to re-train the chosen\n",
      "YOLOv3-320 model and modify the code for the two-step\n",
      "approach, where ﬁrst step is trained to detect the windscreen\n",
      "and the next step trained on only the phone images.The\n",
      "DeepSort [14] tracking algorithm is also integrated into this\n",
      "system.\n",
      "翻譯結果: 圖13顯示使用YOLOv3 320進行測試的結果，AP10顯示出高準確度但對於低質量圖像有較高的假陽性。藍色邊框表示實際結果，綠色表示真陽性預測，紅色表示假陽性預測。整體準確性排名第三，僅次於YOLOv3-512和YOLOv4-416，但其決定性因素是模型的速度，能夠達到近29FPS（幾乎比下一最佳表現模型快11%）。對於這種應用，攝像頭通常監視快速移動的交通。因此，模型應該能夠有效地進行檢測。擁有較小輸入尺寸的模型意味著在硬件需求方面更具成本效益。E. 整合兩步驟方法和跟踪\n",
      "我們整體系統的下一階段是重新訓練選定的YOLOv3-320模型，並修改代碼以實現兩步法，其中第一步被訓練以檢測擋風玻璃，下一步則僅訓練手機圖像。DeepSort [14]跟踪算法也被整合到該系統中。\n",
      "原文: The frame rate is recalculated on the same video, to\n",
      "show the impact of the tracking algorithm and the two-step\n",
      "approach on system efﬁciency.Table V shows that the tracking\n",
      "algorithm reduces FPS by almost 10%, whilst adding the extra\n",
      "step on top of this sees a further reduction of \u001850% giving a\n",
      "frame rate of 13.15 FPS on the YOLOv3-320 model.For the ﬁnal benchmark tests, we split the test images\n",
      "into 2 categories; high-quality and low-quality, with 116 and\n",
      "100 images respectively.Based on our chosen metric of IOU\n",
      "threshold greater than 0.1, for YOLOv3-320, we can achieve\n",
      "an AP of as high as 95.81% on the images taken with only\n",
      "high-quality cameras, whilst still achieving an AP of 74.36%\n",
      "on images taken from the low-quality camera (Table VI).V. D ISCUSSIONS AND FUTURE WORK\n",
      "Our proposed approach delivers very promising results and\n",
      "further enables a fully-automated end-to-end surveillance sys-\n",
      "tem capable of capturing mobile use violations while driving.However, there are still limitations that need to be addressed\n",
      "before tangible impact can be made.\n",
      "翻譯結果: 本技術文件重新計算同一個影片的幀率，以展示追蹤演算法和兩階段方法對系統效率的影響。表V顯示，追蹤演算法將FPS降低了近10％，而在此基礎上增加額外步驟則進一步減少了50％，使得YOLOv3-320模型的幀率為13.15 FPS。對於最終的基準測試，我們將測試影像分為高質量和低質量兩類，分別為116張和100張影像。根據我們選定的IOU閾值大於0.1的度量標準，在僅使用高質量攝像頭拍攝的影像中，對於YOLOv3-320模型，我們可以達到高達95.81％的AP，而在低質量攝像頭拍攝的影像中仍然可以實現74.36％的AP（表VI）。 \n",
      "\n",
      "討論與未來工作：我們提出的方法取得了非常有前途的結果，進一步實現了完全自動化的端到端監控系統，能夠捕捉駕駛中的移動使用違規行為。然而，在可以實現有形效果之前，仍然需要解決一些限制。\n",
      "原文: The proposed two-step model detects the driver side of the\n",
      "windscreen based on right-hand drive vehicles.When deployed\n",
      "in countries using left-hand drive vehicles, we can simplycrop the opposite side.Alternatively, even this process can be\n",
      "automated by detecting the licence plate to identify country\n",
      "and determine which side is the driver.Section III-B addresses not having access to public roadside\n",
      "cameras, so next step would be to deploy the system with\n",
      "support from local authority/police.Test parameters of this\n",
      "work have been based on 3m mounting height with a distance\n",
      "of 25-30m from the subject, meaning that phone could be\n",
      "hidden when texting close to lap.This could be remedied when\n",
      "deployed on a public road by utilizing a gantry trafﬁc camera\n",
      "which allows for a better view within the vehicle.Although, the two-stage approach achieves a greater ac-\n",
      "curacy, there is a compromise with frame rate.We propose\n",
      "optimizing the model and exploring TensorRT [40] framework\n",
      "to potentially improve the speed of the model.\n",
      "翻譯結果: 提出的兩步模型是根據右駕車輛來偵測風擋鏡的駕駛員側位置。當應用在左駕車輛的國家時，我們可以簡單地裁切相反的一側。或者，可以通過檢測車牌來識別國家並確定駕駛員側位置，甚至可以自動化此過程。第 III-B 部分介紹了在沒有公共路邊攝像機的情況下，下一步將是在當地政府/警察的支持下部署系統。這個工作的測試參數是基於 3 米安裝高度和與被測主體的距離為 25-30 米，這意味著當發送短信時，手機可能被藏在腿上附近。當在公共道路上部署時，可以通過利用含拱形交通攝像機來獲得更好的車內視野。儘管兩階段方法實現了更高的準確性，但在幀速率方面存在折衷。我們建議優化模型並探索 TensorRT [40] 框架，以潛在地提高模型的速度。\n",
      "原文: VI.C ONCLUSION\n",
      "In this paper, we have presented a deep learning approach\n",
      "for detecting driver phone violations in all weather conditions\n",
      "without the need for human intervention.A total of 12 object\n",
      "detection models [3], [5], [9]–[11], [31], [32] are ﬁne-tuned\n",
      "and evaluated based on speed and accuracy for both the\n",
      "approaches which are: single-step, where a single frame is\n",
      "used to detect the phone, and the two-step, which ﬁrst detects\n",
      "windscreen and then uses the cropped image of only the driver\n",
      "side to detect the phone.The two-step approach yields higher\n",
      "accuracy but lower frame rate due to having to run two models\n",
      "simultaneously.The model chosen based on both accuracy and\n",
      "speed is YOLOv3 with an input resolution of 320.We also\n",
      "integrate DeepSort, an object tracking algorithm, which allows\n",
      "us to only collect and log unique phone detections from the\n",
      "driver’s side, meaning that this collected data could be made\n",
      "useful for time-series analysis.The trained object detector is\n",
      "able to achieve an accuracy of 84.62% ( AP10) on the 216 test\n",
      "images, with a frame rate of 13.15 FPS during activity and\n",
      "\u001827 with no activity.\n",
      "翻譯結果: VI.C 結論\n",
      "在本論文中，我們提出了一種深度學習方法，可以在所有天氣條件下自動檢測駕駛員使用手機的行為，無需人工介入。我們對12種物體檢測模型[3]、[5]、[9]-[11]、[31]、[32]進行了微調和評估，並基於速度和準確性對兩種方法進行了評估，即單步驟方法和兩步驟方法。其中，單步驟方法使用單幅圖像來檢測手機，而兩步驟方法則首先檢測風擋玻璃，然後僅使用駕駛員一側的裁剪圖像來檢測手機。雖然兩步驟方法具有較高的準確性，但由於需要同時運行兩個模型，因此幀率較低。我們基於準確性和速度選擇的模型是 YOLOv3，輸入分辨率為320。我們還將 DeepSort 集成到模型中，這是一種物體跟踪算法，它允許我們僅收集和記錄來自駕駛員一側的獨特手機檢測，這意味著這些收集的數據可以用於時間序列分析。訓練後的物體檢測器能夠在216張測試圖像上達到84.62%（AP10）的準確率，在活動期間的幀率為13.15 FPS，無活動時為27。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: When images were split between high-\n",
      "end camera and low-end camera, we achieve accuracy levels\n",
      "as high as 95.81% on the high-end and 74.36% on the budget\n",
      "cameras.A user interface is also built in order for the user\n",
      "to easily access the information as well as the ability to view\n",
      "snapshots of all violations.We kindly invite the readers to refer to the supplemental\n",
      "video : https:// youtu.be/PErIUr3Cxvg for more information\n",
      "and more results in video format.REFERENCES\n",
      "[1] World Health Organization.Road trafﬁc injuries.https://www.who.in\n",
      "t/news-room/fact-sheets/detail/road-trafﬁc-injuries, 2021.[Online;\n",
      "accessed 5-July-2021].[2] GOV UK.Double penalties for motorists using mobiles.https://www.gov.uk/government/news/double-penalties-for-motorists-using-mobiles,\n",
      "2017.[Online; accessed 7-July-2021].[3] Luis M ´arquez, V ´ıctor Cantillo, and Juli ´an Arellana.Mobile phone\n",
      "use while driving: A hybrid modeling approach.Accident Analysis &\n",
      "Prevention , 78:73–80, 2015.[4] Jacinto C Nascimento and Jorge S Marques.\n",
      "翻譯結果: 當圖像分為高端相機和低端相機時，我們在高端相機上實現高達95.81％的準確性水平，在低端相機上為74.36％。我們還建立了用戶界面，以便用戶輕鬆訪問信息以及查看所有違規的快照。我們誠邀讀者參考補充視頻：https://youtu.be/PErIUr3Cxvg了解更多信息和視頻格式的更多結果。\n",
      "\n",
      "參考資料：\n",
      "[1] 世界衛生組織。道路交通傷害。https://www.who.int/news-room/fact-sheets/detail/road-traf fi c-injuries，2021年。[在線;2021年7月5日訪問]。\n",
      "[2] 英國政府。使用移動電話的駕駛人雙重處罰。https://www.gov.uk/government/news/double-penalties-for-motorists-using-mobiles，2017年。[在線;2021年7月7日訪問]。\n",
      "[3] Luis M ´arquez，V´ıctor Cantillo和Juli´an Arellana。駕駛時使用手機：一種混合建模方法。事故分析和預防，78:73-80，2015年。\n",
      "[4] Jacinto C Nascimento和Jorge S Marques。\n",
      "原文: Performance evaluation of\n",
      "object detection algorithms for video surveillance.IEEE Transactions\n",
      "on Multimedia , 8(4):761–774, 2006.\n",
      "翻譯結果: 視訊監控中物件偵測演算法性能評估。IEEE多媒體期刊，第8卷第4期，2006年，761-774頁。\n",
      "原文: Object detector FPS FPS FPS\n",
      "(detection only) (with tracking) (with tracking & two-step)\n",
      "YOLOv3 320 29.71 26.94 13.15\n",
      "TABLE V: Impact on frame rate for the chosen model YOLOv3-320 when tracking and two-step is added.Frame rate of\n",
      "the two-step has been recorded when there is constant activity in the video.Without activity, frame rate increases to that of\n",
      "detection and tracker.Object detector Model type AP50 AP10 AP50 AP10\n",
      "(high-quality) (high-quality) (low-quality) (low-quality)\n",
      "YOLOv3 320 Two-step 78.29 95.81 41.93 74.36\n",
      "TABLE VI: Average precision results when images are split between high-end and low-end cameras for YOLOv3-320.[5] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.Faster r-\n",
      "cnn: Towards real-time object detection with region proposal networks.Advances in neural information processing systems , 28:91–99, 2015.[6] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott\n",
      "Reed, Cheng-Yang Fu, and Alexander C Berg.Ssd: Single shot multibox\n",
      "detector.In European conference on computer vision , pages 21–37.\n",
      "翻譯結果: 物體偵測器 FPS FPS FPS\n",
      "(僅偵測) (加上追蹤) (加上追蹤和兩步驟)\n",
      "YOLOv3 320 29.71 26.94 13.15\n",
      "\n",
      "表 V：當加入追蹤和兩步驟時，所選模型 YOLOv3-320 在幀率上的影響。當視頻保持不間斷的活動時，紀錄了兩步驟的幀率。沒有活動時，幀率會增加到偵測和追蹤的程度。\n",
      "\n",
      "物體偵測器 模型類型 AP50 AP10 AP50 AP10\n",
      "(高品質) (高品質) (低品質) (低品質)\n",
      "YOLOv3 320 兩步驟 78.29 95.81 41.93 74.36\n",
      "\n",
      "表 VI：當圖像分為高端和低端相機時，YOLOv3-320 的平均準確率結果。[5] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r- cnn：Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28:91–99, 2015. [6] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd：Single shot multibox detector. In European conference on computer vision, pages 21–37。\n",
      "原文: Springer, 2016.[7] Jie Xu.A deep learning approach to building an intelligent video\n",
      "surveillance system.Multimedia Tools and Applications , 80(4):5495–\n",
      "5515, 2021.[8] Joseph Redmon and Ali Farhadi.Yolov3: An incremental improvement.arXiv preprint arXiv:1804.02767 , 2018.[9] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao.Yolov4: Optimal speed and accuracy of object detection.arXiv preprint\n",
      "arXiv:2004.10934 , 2020.[10] Yixuan Kang.Research on ssd base network.In IOP Conference Series:\n",
      "Materials Science and Engineering , volume 768, page 072031.IOP\n",
      "Publishing, 2020.[11] Zhujun Xu, Emir Hrustic, and Damien Vivet.Centernet heatmap propa-\n",
      "gation for real-time video object detection.In European Conference on\n",
      "Computer Vision , pages 220–234.Springer, 2020.[12] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn,\n",
      "and Andrew Zisserman.The pascal visual object classes (voc) challenge.International journal of computer vision , 88(2):303–338, 2010.[13] Paul Henderson and Vittorio Ferrari.\n",
      "翻譯結果: Springer, 2016.[7] Jie Xu.一種深度學習方法建立智能視頻監控系統。多媒體工具及應用，80(4)：5495-5515，2021年。[8] Joseph Redmon和Ali Farhadi。Yolov3：一個漸進的改進。arXiv預印本arXiv:1804.02767，2018年。[9] Alexey Bochkovskiy、Chien-Yao Wang和Hong-Yuan Mark Liao。Yolov4：物體檢測的最佳速度和精度。arXiv預印本arXiv:2004.10934，2020年。[10] Yixuan Kang。關於ssd基礎網絡的研究。在IOP會議系列中：材料科學與工程，768卷，第072031頁。IOP出版，2020年。[11] Zhujun Xu、Emir Hrustic和Damien Vivet。用於實時視頻物體檢測的Centernet熱圖傳播。在歐洲計算機視覺會議上，第220-234頁。Springer，2020年。[12] Mark Everingham、Luc Van Gool、Christopher KI Williams、John Winn和Andrew Zisserman。Pascal視覺物體類別（VOC）挑戰賽。國際計算機視覺期刊，88(2)：303-338，2010年。[13] Paul Henderson和Vittorio Ferrari。\n",
      "原文: End-to-end training of object class\n",
      "detectors for mean average precision.In Asian Conference on Computer\n",
      "Vision , pages 198–213.Springer, 2016.[14] Xinyu Hou, Yi Wang, and Lap-Pui Chau.Vehicle tracking using deep\n",
      "sort with low conﬁdence track ﬁltering.In 2019 16th IEEE International\n",
      "Conference on Advanced Video and Signal Based Surveillance (AVSS) ,\n",
      "pages 1–6.IEEE, 2019.[15] G Sreenu and MA Saleem Durai.Intelligent video surveillance: a review\n",
      "through deep learning techniques for crowd analysis.Journal of Big\n",
      "Data , 6(1):1–27, 2019.[16] Clive Norris and Xavier L’Hoiry.Times of crises and the development\n",
      "of the police national automatic number plate recognition system in the\n",
      "uk.In Big Data, Surveillance and Crisis Management , pages 198–221.Routledge, 2017.[17] Paul Viola and Michael Jones.Rapid object detection using a boosted\n",
      "cascade of simple features.In Proceedings of the 2001 IEEE computer\n",
      "society conference on computer vision and pattern recognition.CVPR\n",
      "2001 , volume 1, pages I–I.\n",
      "翻譯結果: 對於平均精度均值的目標類別偵測器的端對端訓練。發表於2016年亞洲計算機視覺會議，頁198至213，Springer [14]。侯新宇、王毅和周立培。使用具有低置信度軌跡過濾的深度剖分進行車輛跟踪。在2019年第16屆IEEE高級視頻和信號識別監視國際會議（AVSS）上，頁1-6，IEEE，2019 [15]。G Sreenu和MA Saleem Durai。智能視頻監控:藉由深度學習技術進行群眾分析的一個回顧。大數據期刊，6（1）：1-27，2019 [15]。克萊夫·諾里斯和澤維爾·魯瓦。危機時期和英國警察國家自動號牌識別系統的發展。在《大數據、監控和危機管理》中，頁198至221，Routledge，2017 [16]。Paul Viola和Michael Jones。使用簡單特徵的提升級聯進行快速對象檢測。在2001年IEEE電腦視覺和模式識別計算機協會會議論文集CVPR 2001中，第1卷，第I-I頁。\n",
      "原文: Ieee, 2001.[18] Navneet Dalal and Bill Triggs.Histograms of oriented gradients\n",
      "for human detection.In 2005 IEEE computer society conference on\n",
      "computer vision and pattern recognition (CVPR’05) , volume 1, pages\n",
      "886–893.Ieee, 2005.[19] Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva\n",
      "Ramanan.Object detection with discriminatively trained part-based\n",
      "models.IEEE transactions on pattern analysis and machine intelligence ,\n",
      "32(9):1627–1645, 2009.[20] Zhengxia Zou, Zhenwei Shi, Yuhong Guo, and Jieping Ye.Object\n",
      "detection in 20 years: A survey.arXiv preprint arXiv:1905.05055 , 2019.[21] Chenyi Chen, Ming-Yu Liu, Oncel Tuzel, and Jianxiong Xiao.R-cnn for\n",
      "small object detection.In Asian conference on computer vision , pages\n",
      "214–230.Springer, 2016.[22] Ross Girshick.Fast r-cnn.In Proceedings of the IEEE international\n",
      "conference on computer vision , pages 1440–1448, 2015.[23] Jifeng Dai, Yi Li, Kaiming He, and Jian Sun.R-fcn: Object detection\n",
      "via region-based fully convolutional networks.\n",
      "翻譯結果: Ieee，2001年 [18] Navneet Dalal 和 Bill Triggs.直方圖梯度導向用於人體檢測。在2005年IEEE計算機學會的計算機視覺和模式識別會議(CVPR'05)，卷1，頁886-893。Ieee，2005年[19] Pedro F Felzenszwalb、Ross B Girshick、David McAllester和Deva Ramanan.基於部件訓練的鑑別性模型的目標檢測。模式識別與機器智能IEEE交易，32(9)：1627-1645，2009年。[20] Zhengxia Zou、Zhenwei Shi、Yuhong Guo和Jieping Ye。20年的目標檢測：一項調查。arXiv預印本arXiv:1905.05055，2019年。[21] Chenyi Chen、Ming-Yu Liu、Oncel Tuzel和Jianxiong Xiao。R-cnn用於小目標檢測。在亞洲計算機視覺會議上，頁214-230.Springer，2016。[22] Ross Girshick.快速R-cnn.在IEEE國際計算機視覺會議的論文集中，頁1440-1448，2015年。[23] Jifeng Dai、Yi Li、Kaiming He和Jian Sun.R-fcn:基於區域的完全卷積網絡實現目標檢測。\n",
      "原文: In Advances in neural\n",
      "information processing systems , pages 379–387, 2016.[24] Jiangmiao Pang, Kai Chen, Jianping Shi, Huajun Feng, Wanli Ouyang,\n",
      "and Dahua Lin.Libra r-cnn: Towards balanced learning for object\n",
      "detection.In 2019 IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition (CVPR) , pages 821–830.IEEE, 2019.[25] Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yangdong Deng,\n",
      "and Jian Sun.Light-head r-cnn: In defense of two-stage object detector.arXiv preprint arXiv:1711.07264 , 2017.[26] Hwejin Jung, Bumsoo Kim, Inyeop Lee, Minhwan Yoo, Junhyun Lee,\n",
      "Sooyoun Ham, Okhee Woo, and Jaewoo Kang.Detection of masses\n",
      "in mammograms using a one-stage object detector based on a deep\n",
      "convolutional neural network.PloS one , 13(9):e0203355, 2018.[27] Bensu Alkan, Burak Balci, Alperen Elihos, and Yusuf Artan.Driver cell\n",
      "phone usage violation detection using license plate recognition camera\n",
      "images.In VEHITS , pages 468–474, 2019.[28] Jannes W Elings.Driver handheld cell phone usage detection.\n",
      "翻譯結果: 在神經資訊處理系統的進展中，第379-387頁，2016年[24]，彭江淼、陳凱、石建平、馮華軍、歐陽萬里和林大華的Libra r-cnn：為物體檢測實現平衡學習。在2019年IEEE/CVF計算機視覺和模式識別(CVPR)大會上，第821-830頁。IEEE，2019年[25]。李澤明，彭超，余剛，張向宇，鄧陽東和孫劍的輕頭部r-cnn：為兩階段的物體檢測辯護。arXiv預印本arXiv:1711.07264，2017年[26]。Jung、Bumsoo Kim、Lee、Minhwan Yoo、Junhyun Lee、Sooyoun Ham、Woo和Kang的基於深層卷積神經網絡的單階段物體檢測中的乳腺X線攝影中的質量檢測。PloS one，13(9)：e0203355，2018年[26]。Bensu Alkan、Burak Balci、Alperen Elihos和Yusuf Artan的司機手機使用違規檢測，使用許可證板識別攝像機圖像。在VEHITS，第468-474頁，2019年[27]。Elings的司機手持手機使用檢測。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: Master’s\n",
      "thesis, 2018.[29] Srikanth Tammina.Transfer learning using vgg-16 with deep convolu-\n",
      "tional neural network for classifying images.International Journal of\n",
      "Scientiﬁc and Research Publications (IJSRP) , 9(10):143–150, 2019.[30] T Hoang Ngan Le, Yutong Zheng, Chenchen Zhu, Khoa Luu, and Marios\n",
      "Savvides.Multiple scale faster-rcnn approach to driver’s cell-phone\n",
      "usage and hands on steering wheel detection.In Proceedings of the\n",
      "IEEE conference on computer vision and pattern recognition workshops ,\n",
      "pages 46–53, 2016.[31] Zifeng Wu, Chunhua Shen, and Anton Van Den Hengel.Wider or\n",
      "deeper: Revisiting the resnet model for visual recognition.Pattern\n",
      "Recognition , 90:119–133, 2019.[32] Wei Wang, Yutao Li, Ting Zou, Xin Wang, Jieyu You, and Yanhong\n",
      "Luo.A novel image classiﬁcation approach via dense-mobilenet models.Mobile Information Systems , 2020, 2020.[33] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan\n",
      "Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci,\n",
      "Alexander Kolesnikov, et al.\n",
      "翻譯結果: 2018年碩士論文[29]。Srikanth Tammina。使用具有深度卷積神經網絡的 VGG-16 進行轉移學習以分類圖像。國際科學與研究出版物(IJSRP)，第9卷第10期:143-150，2019年[30]。T Hoang Ngan Le、Yutong Zheng、Chenchen Zhu、Khoa Luu和Marios Savvides。多尺度快速-rcnn方法用於司機手機使用和握方向盤檢測。在IEEE計算機視覺和模式識別研討會文集中，第46-53頁，2016年[31]。Zifeng Wu、Chunhua Shen和Anton Van Den Hengel。寬或深：重新考慮 ResNet 模型的視覺識別。模式識別，90：119-133，2019年[32]。Wei Wang、Yutao Li、Ting Zou、Xin Wang、Jieyu You和Yanhong Luo。一種新的基於密集 MobileNet 模型的圖像分類方法。移動信息系統，2020，2020年[33]。Alina Kuznetsova、Hassan Rom、Neil Alldrin、Jasper Uijlings、Ivan Krasin、Jordi Pont-Tuset、Shahab Kamali、Stefan Popov、Matteo Malloci、Alexander Kolesnikov等。\n",
      "原文: The open images dataset v4.International\n",
      "Journal of Computer Vision , 128(7):1956–1981, 2020.[34] Xiaoling Xia, Cui Xu, and Bing Nan.Inception-v3 for ﬂower classi-\n",
      "ﬁcation.In 2017 2nd International Conference on Image, Vision and\n",
      "Computing (ICIVC) , pages 783–787.IEEE, 2017.[35] Beatriz Blanco-Filgueira, Daniel Garcia-Lesta, Mauro Fern ´andez-\n",
      "Sanjurjo, V ´ıctor Manuel Brea, and Paula L ´opez.Deep learning-\n",
      "based multiple object visual tracking on embedded system for iot and\n",
      "mobile edge computing applications.IEEE Internet of Things Journal ,\n",
      "6(3):5423–5431, 2019.[36] Pirkko Mustamo.Object detection in sports: Tensorﬂow object detection\n",
      "api case study.University of Oulu , 2018.[37] Tensorﬂow.Tensorﬂow 2 detection model zoo.https://github.com/ten\n",
      "sorflow/models/blob/master/research/object detection/g3doc/tf2 detecti\n",
      "onzoo.md, 2021.[Online; accessed 5-May-2021].[38] Shivy Yohanandan.mAP (mean Average Precision) might confuse you!https://towardsdatascience.com/map-mean-average-precision-might-co\n",
      "nfuse-you-5956f1bfa9e2, 2020.\n",
      "翻譯結果: 開放圖像數據集v4。國際計算機視覺雜誌，128（7）：1956-1981，2020年。[34]夏小玲，徐翠和冰楠。Inception-v3用於花卉分類。在2017年第二屆國際圖像，視覺和計算（ICIVC）會議論文集中，第783-787頁。IEEE，2017年。[35] Beatriz Blanco-Filgueira，Daniel Garcia-Lesta，Mauro Fern ´ andez- Sanjurjo，V MANUEL Brea 和 Paula L´opez。基於深度學習的嵌入式系統上的多目標視覺跟踪，用於iot和移動邊緣計算應用。IEEE物聯網雜誌，6（3）：5423-5431，2019年。[36] Pirkko Mustamo。體育中的目標檢測：TensorFlow物體檢測API案例研究。奧盧大學，2018年。[37] Tensorflow。Tensorflow 2檢測模型動物園。https://github.com/tensorflow/models/blob/master/research/object detection/g3doc/tf2 detection zoo.md，2021年。[在線;於2021年5月5日訪問]。[38] Shivy Yohanandan。mAP（平均平均精度）可能會讓你困惑！https://towardsdatascience.com/map-mean-average-precision-might-co nfuse-you-5956f1bfa9e2，2020年。\n",
      "原文: [Online; accessed 7-July-2021].[39] Huizi Mao, Xiaodong Yang, and William J Dally.A delay metric\n",
      "for video object detection: What average precision fails to tell.In\n",
      "Proceedings of the IEEE/CVF International Conference on Computer\n",
      "Vision , pages 573–582, 2019.[40] Han Vanholder.Efﬁcient inference with tensorrt, 2016.\n",
      "翻譯結果: [線上; 2021年7月7日瀏覽]。[39] 毛暉梓、楊曉東和William J Dally。一種用於視頻對象檢測的延遲度量：平均精度所無法告知的事項。於計算機視覺國際會議IEEE/CVF國際會議論文集中，頁面573-582，2019年。[40] Han Vanholder。TensorRT的有效推論，2016年。\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(chunks)):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"請你成為文章翻譯的小幫手，請協助翻譯以下技術文件，以繁體中文輸出\"},\n",
    "        {\"role\": \"user\", \"content\": chunks[i]},\n",
    "    ]\n",
    "    )\n",
    "    print('原文:', chunks[i])\n",
    "    print('翻譯結果:',completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1307e47a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
