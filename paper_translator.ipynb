{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3515bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_path = r\"D:\\data\\paper\\Becteria_paper\\Multilayer outperforms single-layer slide scanning in AI-based classification of whole slide images with low-burden acid-fast mycobacteria\\Multilayer outperforms single-layer slide scanning in AI-based classification of whole slide images with low-burden acid-fast mycobacteria (AFB).pdf\"\n",
    "paper_path = paper_path.replace('\\\\', '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1af71a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/data/paper/Becteria_paper/Multilayer outperforms single-layer slide scanning in AI-based classification of whole slide images with low-burden acid-fast mycobacteria/Translator_Multilayer outperforms single-layer slide scanning in AI-based classification of whole slide images with low-burden acid-fast mycobacteria (AFB).txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m path:\n\u001b[0;32m      6\u001b[0m     p \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 7\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTranslator_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(f)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\paper\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/data/paper/Becteria_paper/Multilayer outperforms single-layer slide scanning in AI-based classification of whole slide images with low-burden acid-fast mycobacteria/Translator_Multilayer outperforms single-layer slide scanning in AI-based classification of whole slide images with low-burden acid-fast mycobacteria (AFB).txt'"
     ]
    }
   ],
   "source": [
    "path = paper_path.split('/')\n",
    "name = path[-1].split('.')[0]\n",
    "path = path[0:-1]\n",
    "p = ''\n",
    "for i in path:\n",
    "    p += i + '/'\n",
    "f = open(p+\"Translator_\"+name+\".txt\", \"w\", encoding=\"utf-8\")\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcd76bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "111fff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03434dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "pdf_name = paper_path\n",
    "reader = PdfReader(pdf_name)\n",
    "number_of_pages = len(reader.pages)\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for i in range(number_of_pages):\n",
    "    page = reader.pages[i]\n",
    "    text = page.extract_text()\n",
    "    sentences = sent_tokenize(text)\n",
    "    input_sentences = ''\n",
    "  \n",
    "    for sentence in sentences:\n",
    "        input_sentences += sentence\n",
    "        if len(input_sentences) > 1000:\n",
    "            chunks.append(input_sentences)\n",
    "            input_sentences = ''\n",
    "    chunks.append(input_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "781bda98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation\n",
      "Samuel G. M ¨uller\n",
      "University of Freiburg\n",
      "muellesa@cs.uni-freiburg.deFrank Hutter\n",
      "University of Freiburg &\n",
      "Bosch Center for Artificial Intelligence, Germany\n",
      "fh@cs.uni-freiburg.de\n",
      "Abstract\n",
      "Automatic augmentation methods have recently become\n",
      "a crucial pillar for strong model performance in vision\n",
      "tasks.While existing automatic augmentation methods\n",
      "need to trade off simplicity, cost and performance, we\n",
      "present a most simple baseline, TrivialAugment, that out-\n",
      "performs previous methods for almost free.TrivialAugment\n",
      "is parameter-free and only applies a single augmentation\n",
      "to each image.Thus, TrivialAugment's effectiveness is very\n",
      "unexpected to us and we performed very thorough exper-\n",
      "iments to study its performance.First, we compare Triv-\n",
      "ialAugment to previous state-of-the-art methods in a variety\n",
      "of image classification scenarios.Then, we perform mul-\n",
      "tiple ablation studies with different augmentation spaces,\n",
      "augmentation methods and setups to understand the crucial\n",
      "requirements for its performance.\n",
      "翻譯結果: TrivialAugment：不需要調校，卻能達到最先進的資料增強技術\n",
      "\n",
      "摘要\n",
      "\n",
      "自動增強技術最近成為視覺任務中強模型表現的重要支柱。雖然現有的自動增強方法需要在簡單性，成本和性能之間作出取捨，但我們提出了一個非常簡單的基線TrivialAugment，幾乎免費卻優於以前的方法。TrivialAugment無需參數，只對每個圖像應用一個單一增強。因此，TrivialAugment的效果對我們來說非常出乎意料，我們進行了非常徹底的實驗以研究其性能。首先，我們在各種圖像分類方案中將TrivialAugment與以前的最先進方法進行比較。然後，我們使用不同的增強空間，增強方法和設置進行多個消融研究，以了解其表現的關鍵要求。\n",
      "原文: Additionally, we provide\n",
      "a simple interface to facilitate the widespread adoption of\n",
      "automatic augmentation methods, as well as our full code\n",
      "base for reproducibility1.Since our work reveals a stag-\n",
      "nation in many parts of automatic augmentation research,\n",
      "we end with a short proposal of best practices for sustained\n",
      "future progress in automatic augmentation methods.Sample strength\n",
      " Input image\n",
      "Sample augmentation\n",
      "and apply it\n",
      "Figure 1: A visualization of TA.For each image, TA (uni-\n",
      "formly) samples an augmentation strength and an augmen-\n",
      "tation.This augmentation is then applied to the image with\n",
      "the sampled strength.Method Search CIFAR-10 CIFAR-100 SVHN ImageNet\n",
      "Overhead ShakeShake WRN WRN ResNet\n",
      "AA 40-800× 98.0 82.9 98.9 77.6\n",
      "RA 4-80× 98.0 83.3 99.0 77.6\n",
      "Fast AA 1× 98.0 82.7 98.8 77.6\n",
      "TA (ours) 0× 98.2 84.3 98.9 78.1\n",
      "Table 1: TrivialAugment compares very favourably to\n",
      "previous augmentation methods.In this table we sum-\n",
      "marize some results from Table 2 and present augmentation\n",
      "search overhead estimates.\n",
      "翻譯結果: 此外，我們提供了簡單的介面，以促進自動增強方法的廣泛應用，以及我們的完整代碼庫以進行再現性。由於我們的研究揭示了自動增強研究的許多部分停滯不前的情況，因此我們提出了維持未來進展的最佳實踐建議。樣本強度、輸入圖像、樣本增強並應用它。圖1：TA的可視化。對於每個圖像，TA（均勻）採樣增強強度和增強。然後將此增強應用於具有採樣強度的圖像。方法、搜索CIFAR-10、CIFAR-100、SVHN、ImageNet。ShakeShake WRN WRN ResNet、AA 40-800× 98.0 82.9 98.9 77.6、RA 4-80× 98.0 83.3 99.0 77.6、快速AA 1× 98.0 82.7 98.8 77.6、TA（我們的）0× 98.2 84.3 98.9 78.1。表1：TrivialAugment與以前的增強方法相比表現非常優秀。在這個表中，我們彙總了表2的一些結果並提供了增強搜索的開銷估計。\n",
      "原文: 1.Introduction\n",
      "Data Augmentation is a very popular approach to in-\n",
      "crease generalization of machine learning models by gen-\n",
      "erating additional data.It is applied in many areas, such\n",
      "as machine translation [4], object detection [6] or semi-\n",
      "supervised learning [20].In this work, we focus on the\n",
      "application of data augmentation to image classification\n",
      "[3, 12].Image augmentations for image classification generate\n",
      "novel images based on images in a dataset, which are likely\n",
      "to still belong to the same classification category.This way\n",
      "the dataset can grow based on the biases that come with the\n",
      "augmentations.While data augmentations can yield consid-\n",
      "erable performance improvements, they do require domain\n",
      "knowledge.An example of an augmentation, with a likely\n",
      "class-preserving behaviour, is the rotation of an image by\n",
      "some small number of degrees.The image’s class is still\n",
      "recognized by humans and so this allows the model to gen-\n",
      "eralize in a way humans expect it to generalize.Automatic augmentation methods are a set of methods\n",
      "that design augmentation policies automatically.\n",
      "翻譯結果: 1.簡介\n",
      "數據增強是一種非常流行的方法，通過生成額外的數據來增加機器學習模型的泛化能力。它被應用在許多領域，如機器翻譯[4]、物體檢測[6]或半監督學習[20]。在這篇文章中，我們專注於將數據增強應用於圖像分類[3,12]。圖像分類的圖像增強是基於數據集中的圖像生成新圖像，這些新圖像可能仍然屬於同一個分類類別。這樣，數據集可以根據增強的偏見而擴大。雖然數據增強可以產生相當大的性能改進，但它們需要領域知識。一個例子是圖像以小幅度旋轉，這個增強應該是有利於保留圖像類別的性質。圖像的類別仍然可以被人類識別，因此這使模型能夠以人類期望的方式進行泛化。自動增強方法是一組自動設計增強策略的方法。\n",
      "原文: They have\n",
      "been shown to improve model performance significantly\n",
      "across tasks [2, 23, 20].Automatic augmentation methods have flourished espe-\n",
      "cially for image classification in recent years [2, 13, 14, 9]\n",
      "with many different approaches that learn policies over aug-\n",
      "1.https://github.com/automl/trivialaugment\n",
      "翻譯結果: 這些方法已被證明在各項任務中顯著提高模型性能[2、23、20]。自動增強的方法在近年來尤其在圖像分類中蓬勃發展[2、13、14、9]，有許多不同的方法可以學習增強策略。1.https://github.com/automl/trivialaugment\n",
      "原文: mentation combinations.The promise of this field is to\n",
      "learn custom augmentation policies that are strong for a par-\n",
      "ticular model and dataset.While the application of an aug-\n",
      "mentation policy found automatically is cheap, the search\n",
      "for it can be much more expensive than the training itself.In this work, we challenge the belief that the resulting\n",
      "augmentation policies of current automatic augmentation\n",
      "methods are actually particularly well fit to the model and\n",
      "dataset.We do this by introducing a trivial baseline method\n",
      "that performs comparably to more expensive augmentation\n",
      "methods without learning a specific augmentation policy\n",
      "per task.Our method does not even combine augmentations\n",
      "in any way.We fittingly call it TrivialAugment (TA).The contributions of this paper are threefold:\n",
      "• We analyze the minimal requirements for well-\n",
      "performing automatic augmentation methods and pro-\n",
      "pose TrivialAugment (TA), a trivial augmentation\n",
      "baseline that poses state-of-the-art performance in\n",
      "most setups.\n",
      "翻譯結果: 自動圖像增強已成為近年來計算機視覺中的一個熱門研究領域，可以通過應用不同的圖像增強技術來擴展數據集，尤其是在具有有限數據的情況下。這個領域的承諾是學習定制的增強策略，對於特定的模型和數據集非常有效。雖然自動增強策略的應用是便宜的，但是尋找它的過程可能比訓練本身更昂貴。在這項工作中，我們挑戰了當前自動增強方法所得到的增強策略是否真的非常適合模型和數據集的信念。我們通過引入一種簡單的基線方法來實現這一目的，這種方法可以在不為每個任務學習特定的增強策略的情況下，表現出與更昂貴的增強方法相當的性能。我們適時地稱之為TrivialAugment (TA)。本文的貢獻有三個方面：\n",
      "• 我們分析了自動增強方法的最低要求，提出了TrivialAugment (TA)，這是一種簡單的增強基線，在大多數設置中都具有最先進的性能。\n",
      "原文: At the same time, TA is the most prac-\n",
      "tical automatic augmentation method to date.• We comprehensively analyze the performance of TA\n",
      "and multiple other automatic augmentation methods in\n",
      "many setups, using a unified open-source codebase to\n",
      "compare apples to apples.• We make recommendations on the practical usage of\n",
      "automatic augmentation methods and collect best prac-\n",
      "tices for automatic augmentation research.Addition-\n",
      "ally, we provide our code for easy application and fu-\n",
      "ture research.2.Related Work\n",
      "Many automatic augmentation methods have been pro-\n",
      "posed in recent years with multiple different setups.Still,\n",
      "all automatic augmentation methods we consider share one\n",
      "property: They work on augmentation spaces that consist\n",
      "of i) a set of prespecified augmentations Aand ii) a set of\n",
      "possible strength settings with which augmentations in A\n",
      "can be called (in this work {0, . . . , 30}).One member of A\n",
      "might, for example, be the aforementioned rotation opera-\n",
      "tion, where the strength would correspond to the number of\n",
      "degrees.\n",
      "翻譯結果: 同時，TA是迄今最實用的自動增強方法。我們全面分析了TA和其他多種自動增強方法在多種設置中的性能，使用統一的開源代碼庫進行比較。我們提出了關於自動增強方法的實際用法建議，並收集了自動增強研究的最佳實踐。此外，我們提供了簡單應用和未來研究的代碼。\n",
      "\n",
      "2.相關工作\n",
      "近年來，已經提出了許多自動增強方法，並在多種不同的設置中進行了多種嘗試。仍然，我們考慮的所有自動增強方法都具有一個共同的特徵：它們在由i）預先指定的增強集合A和ii）可能使用的強度設置組成的增強空間中工作（在此工作中，為{0，…，30}）。例如，A的一個成員可能是前述的旋轉操作，其中強度將對應於角度數量。\n",
      "原文: Automatic augmentation methods now learn how\n",
      "to use these augmentations together on training data to yield\n",
      "a well-performing final classifier.In this section, we provide a thorough overview of rel-\n",
      "evant previous methods.As the compute requirements for\n",
      "automatic augmentation methods can dominate the training\n",
      "costs, we order this recount by the total cost of each method.We begin with the first automatic augmentation method,\n",
      "AutoAugment (AA) [1], which also happens to be the most\n",
      "expensive, spending over half a GPU-year of compute to\n",
      "yield a classifier on CIFAR-10.AA uses a recurrent neuralnetwork (RNN), which is trained with reinforcement learn-\n",
      "ing methods, to predict a parameterization of augmentation\n",
      "policies.Reward is given for the validation accuracy of a\n",
      "particular model trained on a particular dataset with the pre-\n",
      "dicted policy.AA makes use of multiple sub-policies each\n",
      "consisting of multiple augmentations, which in turn are ap-\n",
      "plied sequentially to an input image.Additionally, augmen-\n",
      "tations are left out with a specified probability.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻譯結果: 現在自動擴增方法已經學會如何在訓練數據中結合使用這些擴增方式，以產生表現優良的最終分類器。本節將全面介紹相關先前方法。由於自動擴增方法的運算要求可能會主導訓練成本，因此我們按照每種方法的總成本來排序，首先介紹的是第一種自動擴增方法AutoAugment（AA）[1]，也是最昂貴的方法之一，花費半個GPU年的計算時間才能在CIFAR-10上得到一個分類器。AA使用一種循環神經網絡（RNN），並使用強化學習方法對其進行訓練，以預測擴增策略的參數化。對於使用預測策略在特定數據集上訓練的特定模型的驗證準確率，給予獎勵。AA利用多個子策略，每個子策略由多個擴增組成，這些擴增方式依次應用於輸入圖像上。此外，還可以通過特定概率留出某些擴增方式。\n",
      "原文: This allows\n",
      "one sub-policy to represent multiple combinations of aug-\n",
      "mentations.Since AA is costly it uses not the task at hand\n",
      "for augmentation search, but a reduced dataset and a smaller\n",
      "model variant.The second most expensive method is Augmentation-\n",
      "wise Sharing for AutoAugment (AWS) [19].It builds on\n",
      "the same optimization procedure as AA, but uses a simpler\n",
      "search space.The search space consists of a distribution\n",
      "over pairs of augmentations that are applied together.Dif-\n",
      "ferent from AA, AWS learns the augmentation policy for\n",
      "the last few epochs of training only.It does this on the full\n",
      "dataset with a small model.A very different approach, called Population-based Aug-\n",
      "mentation (PBA) [9], is to learn the augmentation policy\n",
      "online as the training goes.PBA does so by using multi-\n",
      "ple workers that each use a different policy and are updated\n",
      "in an evolutionary fashion.It uses yet another policy pa-\n",
      "rameterization: a vector of augmentations where each aug-\n",
      "mentation has an attached strength and leave-out probabil-\n",
      "ity.\n",
      "翻譯結果: 這可以讓一個子政策代表多個增強組合。由於自動增強（AA）的成本較高，它不會使用當前任務進行增強搜索，而是使用簡化的數據集和較小的模型變體。第二昂貴的方法是Augmentation-wise Sharing for AutoAugment（AWS）[19]。它建立在與AA相同的優化過程上，但使用了一個更簡單的搜索空間。搜索空間由一對一起應用的增強的分佈組成。與AA不同，AWS僅在訓練的最後幾個epoch中對全數據集使用小型模型學習增強策略。一種非常不同的方法稱為Populational-based Augmentation（PBA）[9]，是在訓練過程中學習增強策略。PBA通過使用多個工作程序，每個工作程序都使用不同的策略並以進化方式進行更新。它使用另一個策略​​參數化：增強向量，其中每個增強都有附加的強度和遺漏概率。\n",
      "原文: From this vector augmentations are sampled uniformly\n",
      "at random and applied with the given strength or left out,\n",
      "depending on the leave-out probability.Another method based on multiple parallel workers is\n",
      "Online Hyper-Parameter Learning for Auto-Augmentation\n",
      "(OHL) [14].Here, the policy is defined like for AWS and\n",
      "its parameters are trained using reinforcement learning.The\n",
      "major difference with AWS is that its reward is the accu-\n",
      "racy on held-out data after a part of training like for PBA,\n",
      "rather than final accuracy.As an additional way of tuning\n",
      "the neural network weights in the parallel run, the weights\n",
      "of the worker with maximal accuracy are used to initialize\n",
      "all workers in the next part of training.Adversarial AutoAugment (Adv.AA) [22] is another\n",
      "slightly cheaper method that uses multiple workers and\n",
      "learns the augmentation policy online.It trains only a sin-\n",
      "gle model, though.Here, a single batch is copied to eight\n",
      "different workers and each worker applies its own policies\n",
      "to it, similar to the work by Hoffer et al.\n",
      "翻譯結果: 從這個向量增強方法隨機均勻地抽樣並應用于給定強度或留空，具體取決于遺漏概率。另一種基于多個平行工作器的方法是自動增強的在線超參數學習（OHL）[14]。在這里，策略與AWS的策略相似，其參數使用強化學習進行訓練。 AWS的主要不同之處在於，其獎勵是類似PBA的一部分訓練後的保留數據的精度，而不是最終精度。作為并行運行中調整神經網絡權重的附加方式，使用具有最大精度的工作器的權重來初始化下一部分訓練中的所有工作器。對抗自動增強（Adv.AA）[22]是另一種略微更便宜的方法，它使用多個工作器，在線學習增強策略。但是，它僅訓練一個單模型。在這里，一個批次被複製到八個不同的工作器，每個工作器對其應用自己的策略，類似於Hoffer等人的工作。\n",
      "原文: [10].The worker\n",
      "policies are sampled at the beginning of each epoch from\n",
      "a policy distribution.The policy distribution has a simi-\n",
      "lar form to that of AA.After each epoch, Adv.AA makes a\n",
      "reinforcement-learning based update and rewards the policy\n",
      "yielding the lowest accuracy training accuracy, causing the\n",
      "policy distribution to shift towards progressively stronger\n",
      "augmentations over the course of training.\n",
      "翻譯結果: [10].工作人員政策\n",
      "在每個時代的開頭從政策分配中進行抽樣。政策分配形式與AA相似。在每個時代後，Adv.AA進行強化學習更新，獎勵產生最低訓練精度的政策，使政策分配隨著訓練進程向著愈來愈強的增強移動。\n",
      "原文: Recently, Cubuk et al.proposed RandAugment (RA) [2].It is much simpler, but only slightly cheaper, compared\n",
      "to the previous methods.RA only tunes two scalar pa-\n",
      "rameters for each task: (i) a single augmentation strength\n",
      "m∈ {0, . . . , 30}which is applied to all augmentations and\n",
      "(ii) the number of augmentations to combine for each image\n",
      "n∈ {1,2,3}.RA therefore reduces the number of hyper-\n",
      "parameters from all the weights of an RNN (for AA) or a\n",
      "distribution over more than a thousand augmentation com-\n",
      "binations (for AWS and OHL) to just two.This radical sim-\n",
      "plification, contrary to expectations, does not hurt accuracy\n",
      "scores compared to many other methods.The authors give\n",
      "indication that the strong performance might be due to the\n",
      "fact that nandmare tuned for the exact task at hand and not\n",
      "for a pruned dataset, as is done, for example, in AA.The big\n",
      "downside of RA is that it ends up performing an exhaustive\n",
      "search over a set of options for nandmincurring up to 80×\n",
      "overhead over a single training2.\n",
      "翻譯結果: 最近，Cubuk等人提出了RandAugment（RA）[2]。与先前方法相比，它要简单得多，但只是略微更便宜。对于每个任务，RA只调整两个标量参数：（i）应用于所有增强的单个增强强度m∈ {0，...，30}和（ii）将合并到每个图像的增强数量n∈ {1,2,3}。因此，相对于使用RNN的所有权重（对于AA）或分布在一千多种增强组合之上（对于AWS和OHL），RA将超参数数量减少到了仅仅两个。与预期相反，这种根本简化并不会损害与许多其他方法相比的准确性得分。作者提出的一个迹象表明，强大的表现可能是由于n和m针对特定的任务进行调整，而不是像在AA中那样对剪枝数据集进行调整的结果。 RA的一个巨大的缺点是它最终在n和m的选项集上执行耗时庞大的穷举搜索，使单次训练时间开销高达80倍。\n",
      "原文: Fast AutoAugment (Fast AA) [13] is the cheapest of\n",
      "the learned methods.It is based on AA, but does not\n",
      "directly search for policies with strong validation perfor-\n",
      "mance.Rather, it searches for augmentation policies by\n",
      "finding well-performing inference augmentation policies\n",
      "for networks trained on a split of raw, non-augmented, im-\n",
      "ages.All inference augmentations found on different splits\n",
      "are then joined to build a training time augmentation policy.The intuition behind this can be summarized as follows: If a\n",
      "neural network trained on real data generalizes to examples\n",
      "augmented with some policy then this policy produces im-\n",
      "ages that lie in the domain of the class, as approximated by\n",
      "the neural network.The augmentations therefore are class-\n",
      "preserving and useful.This objective stands in contrasts\n",
      "to the approach followed by Adv.AA.Fast AA tries to find\n",
      "augmentations that yield high accuracy when applied to val-\n",
      "idation data, while Adv.AA tries to find augmentations that\n",
      "yield low accuracy when applied to training data.\n",
      "翻譯結果: 快速自動增強（Fast AA）[13] 是學習方法中最便宜的一種。它基於 AA，但不直接尋找具有強大驗證表現的策略。相反，它通過找到在非增強的原始圖像拆分上訓練網絡的表現良好的推理增強策略來搜索增強策略。在不同的拆分上找到的所有推理增強都會合併起來建立訓練時間增強策略。其背後的直覺可以概括如下：如果在實際數據上訓練的神經網絡對某些增強策略加強版的範例進行泛化，則該策略產生的圖像位於類別域中，由神經網絡逼近。因此，增強是類保留的且有用的。這個目標與 Adv.AA 的方法相反。快速 AA 試圖找到應用於驗證數據時產生高精度的增強，而 Adv.AA 則試圖找到應用於訓練數據時產生低精度的增強。\n",
      "原文: Finally, in an unpublished arXiv paper, Lingchen et al.[15] very recently suggested UniformAugment (UA), which\n",
      "works almost like RA.Unlike RA, it fixes the number of\n",
      "augmentations to N= 2and drops each augmentation with\n",
      "a fixed probability of 0.5.Furthermore, the strength mis\n",
      "sampled uniformly at random for each applied operation.In contrast to all above methods methods, we propose\n",
      "TrivialAugment (TA), an augmentation algorithm that is\n",
      "parameter-free like UA, but even simpler.At the same time,\n",
      "TA performs better than any of the comparatively cheap\n",
      "augmentation strategies, making it the most practical auto-\n",
      "matic augmentation method to date.Different from all of the work discussed above, which\n",
      "improves final in-distribution test performance with aug-\n",
      "mentation strategies, AugMix [8] aims to improve model\n",
      "robustness by combining multiple augmentations in appli-\n",
      "cation chains, mixing their outputs, and applying a consis-Figure 2: An exemplary visualization of a 2-D dataset with\n",
      "two classes, crosses and circles, separated by a decision\n",
      "boundary, the dotted line.\n",
      "翻譯結果: 最近，Lingchen等人在未發表的arXiv論文中提出了UniformAugment (UA)，它的工作方式與RA幾乎相同。與RA不同的是，它將增強次數固定為N = 2，然後以0.5的固定概率刪除每個增強。此外，每個應用操作都會隨機均勻地選擇強度。與上述所有方法不同，我們提出了TrivialAugment (TA)，一種無需參數像UA一樣簡單的增強算法，但表現比任何比較便宜的增強策略都要好，使其成為迄今為止最實用的自動增強方法。不同於以上所有提高增強策略的內部分佈測試性能的工作，AugMix旨在通過結合多個增強應用鏈，混合其輸出並應用一致的方法來提高模型的魯棒性。圖2：展示了一個2D資料集的例子，它有兩個類別，十字和圓圈，由決策邊界分開，該邊界為虛線。\n",
      "原文: The colored crosses represent\n",
      "deterministic augmentations of the cross class.TA now uni-\n",
      "formly samples from all crosses.tency loss to several augmented images.The only metric we\n",
      "evaluated for which AugMix was evaluated, too, is ResNet-\n",
      "50 performance on the ImageNet test set.Here, TA outper-\n",
      "forms AugMix.3.TrivialAugment\n",
      "In this section, we present the simplest augmentation al-\n",
      "gorithm we could come up with that still performs well:\n",
      "TrivialAugment (TA) .TA employs the same augmentation\n",
      "style that previous work [2, 15] used: An augmentation is\n",
      "defined as a function amapping an image xand a discrete\n",
      "strength parameter mto an augmented image.The strength\n",
      "parameter is not used by all augmentations, but most use it\n",
      "to define how strongly to distort the image.TA works as follows.It takes an image xand a set\n",
      "of augmentations Aas input.It then simply samples an\n",
      "augmentation from Auniformly at random and applies this\n",
      "augmentation to the given image xwith a strength m, sam-\n",
      "pled uniformly at random from the set of possible strengths\n",
      "{0, . . . , 30}, and returns the augmented image.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻譯結果: 有色交叉表示交叉類的確定性擴增。現在TA從所有交叉中均勻抽樣。我們評估的唯一指標是ResNet-50在ImageNet測試集上的性能，該指標也用於評估AugMix，此處，TA的性能優於AugMix。在本節中，我們介紹了一種簡單的增強算法：TrivialAugment（TA）。TA採用先前工作[2,15]使用的相同增強風格：增強被定義為一個將圖像x和離散強度參數m映射到擴增圖像的函數。並非所有增強都使用強度參數，但大多數都使用它來定義如何強烈扭曲圖像。TA的工作原理如下：它以圖像x和一組增強A作為輸入，然後從A中均勻隨機抽取一個增強，並將此增強應用於給定的圖像x，強度m隨機均勻地從可能強度的集合{0，...，30}中抽樣，然後返回增強圖像。\n",
      "原文: We outline\n",
      "this very simple and parameter-free procedure as pseudo-\n",
      "code in Algorithm 1 and visualize it in Figure 1.We em-\n",
      "phasize that TA is nota special case of RandAugment (RA),\n",
      "since RA uses a fixed optimized strength for all images\n",
      "while TA samples this strength anew for each image.While previous methods used multiple subsequent aug-\n",
      "mentations, TA only applies a single augmentation to each\n",
      "image.This allows viewing the distribution of the TA-\n",
      "augmented dataset as an average of the |A|data distribu-\n",
      "tions generated by each of the augmentations applied to the\n",
      "full dataset.In Figure 2 we visualize this notion for deter-\n",
      "ministic augmentations without a strength parameter.Un-\n",
      "2.In the original setups, the authors also used a different choice of n\n",
      "andmfor the search on each task.This can be hard to do for new tasks or\n",
      "with less intuition for a task.\n",
      "翻譯結果: 我們在算法1的偽代碼中概述了這個非常簡單且無需參數的過程，並在圖1中將其可視化。我們強調TA不是RandAugment（RA）的特殊情況，因為RA對所有圖像使用固定優化的強度，而TA會為每個圖像重新選擇強度。而先前的方法使用多個連續的增強，TA僅對每個圖像應用單個增強。這使得TA增強後的數據集的分佈可以視為應用於完整數據集的每個增強產生的| A |數據分佈的平均值。在圖2中，我們將無強度參數的確定性增強的概念可視化。在原始設置中，作者還使用了不同的n和m的選擇來搜索每個任務。這對於新的任務或對該任務的理解較少可能很困難。\n",
      "原文: Algorithm 1 TrivialAugment Procedure\n",
      "1:procedure TA(x: image)\n",
      "2: Sample an augmentation afromA\n",
      "3: Sample a strength mfrom{0, . . . , 30}\n",
      "4: Return a(x, m )\n",
      "5:end procedure\n",
      "like previous work, we do not generate complex distribu-\n",
      "tions out of stochastic combinations of augmentation meth-\n",
      "ods, but simply mean the data distributions of the augmen-\n",
      "tations applied to the given dataset.4.Experiments\n",
      "In this section, we empirically demonstrate TA's surpris-\n",
      "ingly strong performance, as well as its behaviour across\n",
      "many ablation settings.In all non-ablation experiments we\n",
      "use either the RA augmentation space (RA), i.e., the set of\n",
      "augmentations and their strength parameterization from the\n",
      "RA paper [2], or the wide augmentation space (Wide) for\n",
      "TA.We list the augmentations and their arguments for all\n",
      "augmentation spaces in the appendix in Table 8.We run\n",
      "each experiment ten times, if not stated otherwise.In ad-\n",
      "dition to the average over runs we report a confidence in-\n",
      "terval, which will contain the true mean with probability\n",
      "p= 95% , under the assumption of normal distributed accu-\n",
      "racies.\n",
      "翻譯結果: 算法1 簡單擴充過程\n",
      "1：過程TA(x: image)\n",
      "2：從A中取樣一個擴充a\n",
      "3：從{0, …, 30}中取樣強度m\n",
      "4：返回a(x, m)\n",
      "5：結束過程\n",
      "\n",
      "像以前的工作一樣，我們不會使用擴充方法的隨機組合產生複雜的分佈，而是簡單地將應用於給定數據集的擴充數據分布平均。 \n",
      "\n",
      "4.實驗\n",
      "在本節中，我們通過實際演示TA的出奇制勝的表現及其在許多消融設置下的行為，進行實驗。在所有非消融實驗中，我們使用RA擴充空間（RA），即RA論文中的擴充和它們的強度參數化，或TA的廣泛擴充空間（Wide）。我們在附錄中的第8 表中列出了所有擴充及其引數的擴充空間。如果未另外說明，我們每個實驗運行十次。除了平均運行時間外，我們還報告了置信區間，它在正態分佈的精度假設下，將以p = 95％ 的概率包含真實平均值。\n",
      "原文: In our code we provide a function to compute this\n",
      "interval.Results that lie within the confidence interval of\n",
      "the best performer for each task are typeset in bold font.We evaluate our method on five different datasets.i)\n",
      "CIFAR-10 and CIFAR-100 [11] are standard datasets for\n",
      "image classification and each contain 50K training images.We trained Wide-ResNets [21] as well as a ShakeShake\n",
      "model [5].We follow previous work [1, 2] with our setup.ii) SVHN [17] consists of images of house numbers.It\n",
      "comes with a core set of 73K training images, but offers an\n",
      "additional 531K simpler images as extension of the dataset.We perform experiments with and without the additional\n",
      "images on a Wide-ResNet-28-10.iii) Finally, we perform\n",
      "experiments on ImageNet, a very large image classification\n",
      "corpus with 1000 classes and over 1.2 million images.This\n",
      "experiment is particularly interesting, since it was shown\n",
      "previously that there are augmentations, such as cutout, that\n",
      "do not generalize well to ImageNet.We train a ResNet-\n",
      "50 [7] following the setup of [1].\n",
      "翻譯結果: 我們的程式碼提供了一個計算此區間的函數。每個任務中落在最佳表現者信心區間內的結果會以粗體字顯示。我們在五個不同的資料集上評估我們的方法。i)CIFAR-10和CIFAR-100[11]是用於圖像分類的標準資料集，每個資料集都包含50K培訓圖像。我們訓練了Wide-ResNets[21]和ShakeShake模型[5]。我們遵循前人的工作[1,2]進行實驗。ii)SVHN[17]包括住宅號碼的圖像。它帶有一組核心訓練圖像共73K張，但還提供了531K個簡單的圖像作為資料集的延伸。我們在Wide-ResNet-28-10上進行有和無額外圖像的實驗。iii)最後，我們在ImageNet上進行實驗，它是一個非常大的圖像分類語料庫，包含1000個類別和超過120萬個圖像。這個實驗特別有趣，因為先前已經顯示有些增強，比如cutout，無法很好地推廣到ImageNet。我們訓練一個ResNet-50[7]，並按照[1]的設置進行訓練。\n",
      "原文: We use warmup and 32\n",
      "workers due to cluster limitations, which is less than [1].We scale the learning rate appropriately.See Appendix A\n",
      "for more details.4.1.Comparison to State-of-the-Art\n",
      "It is non-trivial to compare automatic augmentation\n",
      "methods fairly.We therefore compare our method with the\n",
      "previous state-of-the-art in three different setups.In Section 4.1.1, we follow the majority of previous work\n",
      "[1, 2, 9, 13, 15] and perform a comparison with other meth-\n",
      "ods that use the same model and training pipeline.This\n",
      "setup allows for different search costs of different methods\n",
      "and compares methods with the same inference and training\n",
      "costs.In Section 4.1.2, we compare in a similar way as above,\n",
      "but against reproductions of other methods in our codebase.This avoids confounding factors, making sure that the meth-\n",
      "ods, and not setup details, explain the differences between\n",
      "results.We reproduced a total of four other methods in our\n",
      "codebase, including the cheapest three previous methods.In Section 4.1.3, we compare the total cost of each\n",
      "method, both search and model training, with the final ac-\n",
      "curacy.\n",
      "翻譯結果: 由於集群限制，我們使用暖身和32位工作人員，這比[1]更少。我們適當地調整學習率。詳細信息請參見附錄A。4.1. 與現有技術的比較。公正地比較自動增強方法是不簡單的。因此，我們以三種不同的設置與先前的最新技術進行比較。在第4.1.1節中，我們按照大多數先前的工作[1、2、9、13、15]的方式，與使用相同模型和訓練管道的其他方法進行比較。此設置允許不同方法具有不同的搜索成本，並比較具有相同推論和訓練成本的方法。在第4.1.2節中，我們以類似的方式進行比較，但與我們代碼庫中的其他方法進行比較。這樣可以避免混淆因素，確保方法而不是設置細節解釋結果之間的差異。我們在代碼庫中重現了共四種其他方法，包括成本最低的三種先前方法。在第4.1.3節中，我們比較每種方法的總成本，既包括搜索和模型培訓，也包括最終的準確性。\n",
      "原文: This comparison has the upside that it can consider\n",
      "work with different pipelines and models more fairly.4.1.1 Comparison to Published Results\n",
      "In Table 2, we compare TA to all methods that used the\n",
      "setup of AutoAugment [1] or a very similar setup in terms\n",
      "of hyper-parameters, number of epochs and models.TA performs as well or better than previous methods in\n",
      "almost all tasks.The SVHN datasets are the only exception,\n",
      "with RA performing somewhat better.This might, however,\n",
      "be due to our training pipeline, since, as we show in Sec-\n",
      "tion 4.1.2, we were not able to reproduce RA's performance\n",
      "for SVHN Core with our pipeline and the original training\n",
      "pipeline is not available.For ImageNet, TA outperformed all other methods in\n",
      "terms of both top-1 accuracy and top-5 accuracy.We used\n",
      "an image width of 244 like RA [2], but even with a lower\n",
      "width of 224 (as was used for AA [1]), TA outperformed the\n",
      "previously best methods (with a 77.97 ±.21 top-1 accuracy\n",
      "and 93.98 ±.07 top-5 accuracy; not listed in the table).\n",
      "翻譯結果: 這種比較具有優勢，因為它可以更公正地考慮不同流程和模型的工作。4.1.1 與發布結果的比較在表2中，我們將TA與所有使用AutoAugment [1]設置或非常相似的超參數，時期數以及模型方面進行比較。在幾乎所有任務中，TA表現得至少與以前的方法相當，甚至更好。 SVHN數據集是唯一的例外，RA表現略微優於TA。但是，這可能是由於我們的培訓流程，在第4.1.2節中，我們無法重現RA在SVHN Core的表現，並且原始的培訓流程也不可用。對於ImageNet，TA在前1位準確性和前5位準確性方面均優於所有其他方法。我們使用了244的圖像寬度，如RA [2]所示，但即使使用較低的寬度224（AA [1]使用），TA仍然優於以前最好的方法（前1位準確率為77.97 ± .21，前5位準確率為93.98 ± .07花費，未在表格中列出）。\n",
      "原文: In this comparison, we cannot compare to all previous\n",
      "methods, since some use different setups.The best-known\n",
      "setup we had to leave out is Adv.AA.Therefore, we per-\n",
      "form an extra set of experiments following its setup closely.Adv.AA uses eight times the compute for its final train-\n",
      "ing compared to other methods and therefore has a signif-\n",
      "icant advantage compared to other methods.Adv.AA is\n",
      "based on batch augmentation [10], where a set of work-\n",
      "ers in a data parallel setting each compute gradients with\n",
      "respect to the same batch of examples, but apply differ-\n",
      "ent augmentations to the images in it.We re-created this\n",
      "setup, including all hyper-parameters and batch augmenta-\n",
      "tion, for TA.In Table 3, we compare TA with Adv.AA with\n",
      "a Wide-ResNet-28-10 and a ShakeShake-26-2x96d for both\n",
      "CIFAR-10 and CIFAR-100.We show that TA's trivial uni-\n",
      "form sampling of a single augmentation achieves the same\n",
      "performance as their complex (and unavailable) reinforce-\n",
      "ment learning pipeline.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻譯結果: 在這個比較中，由於有些方法使用不同的設置，因此我們無法將所有先前的方法進行比較。我們忽略了最知名的Adv.AA設置。因此，我們進行了一組遵循其設置的額外實驗。Adv.AA的最終訓練使用的計算量比其他方法多8倍，因此與其他方法相比具有明顯的優勢。Adv.AA基於批次增強[10]，在數據並行設置中，一組工作者分別計算與同一批例子相關的梯度，但對其中的圖像應用不同的增強。我們重新創建了此設置，包括所有超參數和批次增強，適用於TA。在表3中，我們比較了TA和Adv.AA在CIFAR-10和CIFAR-100中使用Wide-ResNet-28-10和ShakeShake-26-2x96d的情況。我們展示了TA的簡單統一採樣單一增強與其複雜（且無法獲取）的強化學習流程實現了相同的表現。\n",
      "原文: Default PBA Fast AA AA RA UA TA (Wide)\n",
      "CIFAR-10\n",
      "Wide-ResNet-40-2 96.16±.08 - 96.4 96.3 - 96.25 96.32±.05\n",
      "Wide-ResNet-28-10 97.03±.07 97.4 97.3 97.4 97.3 97.33 97.46±.06\n",
      "ShakeShake-26-2x96d 97.54±.07 98.0 98.0 98.0 98.0 98.10 98.21±.06\n",
      "PyramidNet 97.95±.05 98.5 98.5 98.3 98.5 98.5 98.58±.04\n",
      "CIFAR-100\n",
      "Wide-ResNet-40-2 78.42±.31 - 79.4 79.3 - 79.01 79.86±.19\n",
      "Wide-ResNet-28-10 82.22±.25 83.3 82.7 82.9 83.3 82.82 84.33±.17\n",
      "ShakeShake-26-2x96d 83.28±.14 84.7 85.4 85.7 - 85.00 86.19±.15\n",
      "SVHN Core\n",
      "Wide-ResNet-28-10 97.12±.05 - - 98.0 98.3 - 98.11±.03\n",
      "SVHN\n",
      "Wide-ResNet-28-10 98.67±.02 98.9 98.8 98.9 99.0 - 98.9±.02\n",
      "ImageNet\n",
      "ResNet-5077.20±.32\n",
      "(93.43±.11)-77.6\n",
      "(93.7)77.6\n",
      "(93.8)77.6\n",
      "(93.8)77.63\n",
      "(-)78.07±.27\n",
      "(93.92±.09)\n",
      "Table 2: The average test accuracies from ten runs, besides for ImageNet, where we used five runs.The 95% confidence\n",
      "interval is noted with ±.The trivial TA is in all benchmarks among the top-performers.The only exception is the comparison\n",
      "to RA's performance on the SVHN benchmarks, but this difference was non-existent in our reimplementation in 4.1.2.\n",
      "翻譯結果: 預設 PBA 快速 AA AA RA UA TA (寬)\n",
      "CIFAR-10\n",
      "Wide-ResNet-40-2 96.16±.08 - 96.4 96.3 - 96.25 96.32±.05\n",
      "Wide-ResNet-28-10 97.03±.07 97.4 97.3 97.4 97.3 97.33 97.46±.06\n",
      "ShakeShake-26-2x96d 97.54±.07 98.0 98.0 98.0 98.0 98.10 98.21±.06\n",
      "PyramidNet 97.95±.05 98.5 98.5 98.3 98.5 98.5 98.58±.04\n",
      "CIFAR-100\n",
      "Wide-ResNet-40-2 78.42±.31 - 79.4 79.3 - 79.01 79.86±.19\n",
      "Wide-ResNet-28-10 82.22±.25 83.3 82.7 82.9 83.3 82.82 84.33±.17\n",
      "ShakeShake-26-2x96d 83.28±.14 84.7 85.4 85.7 - 85.00 86.19±.15\n",
      "SVHN Core\n",
      "Wide-ResNet-28-10 97.12±.05 - - 98.0 98.3 - 98.11±.03\n",
      "SVHN\n",
      "Wide-ResNet-28-10 98.67±.02 98.9 98.8 98.9 99.0 - 98.9±.02\n",
      "ImageNet\n",
      "ResNet-5077.20±.32\n",
      "(93.43±.11)-77.6\n",
      "(93.7)77.6\n",
      "(93.8)77.6\n",
      "(93.8)77.63\n",
      "(-)78.07±.27\n",
      "(93.92±.09)\n",
      "表2：十次運行的平均測試準確度，除了 ImageNet 外，我們使用了五次運行。95％的置信區間被標記為 ±。在所有基準測試中，微不足道的 TA 都是表現最佳的人之一。唯一的例外是與 RA 在 SVHN 基準測試中的表現相比，但這種差異在我們在 4.1.2 中的重新實現中是不存在的。\n",
      "原文: Adv.AA TA (Wide)\n",
      "CIFAR-10\n",
      "Wide-ResNet-28-10 98.10±.15 98.04±.06\n",
      "ShakeShake-26-2x96d 98.15±.12 98.12±.12\n",
      "CIFAR-100\n",
      "Wide-ResNet-28-10 84.51±.18 84.62±.14\n",
      "ShakeShake-26-2x96d 85.90±.15 86.02±.13\n",
      "Table 3: A comparison of TA with Adv.AA in the aug-\n",
      "mented batch setting on a Wide-ResNet-28-10.We report\n",
      "the average over five runs.We conclude from this section that, for almost all consid-\n",
      "ered benchmarks across datasets, models and even the way\n",
      "augmentations are applied, TA is among the top-performing\n",
      "methods.4.1.2 Comparison of Reproduced Results in a Fixed\n",
      "Training Setup\n",
      "While in the previous section, we tried to mitigate con-\n",
      "founding factors by comparing results obtained with very\n",
      "similar setups with each other, in this section, we go one\n",
      "step further.We reproduce the results of four methods and\n",
      "compare our baseline method with these reproductions in\n",
      "order to yield a true apples-to-apples comparison.As we present a very cheap and simple augmentation\n",
      "method we picked RA, Fast AA and UA as other cheap\n",
      "and simple augmentation methods for our comparison.\n",
      "翻譯結果: Adv.AA TA（寬廣）\n",
      "CIFAR-10\n",
      "Wide-ResNet-28-10 98.10±.15 98.04±.06\n",
      "ShakeShake-26-2x96d 98.15±.12 98.12±.12\n",
      "CIFAR-100\n",
      "Wide-ResNet-28-10 84.51±.18 84.62±.14\n",
      "ShakeShake-26-2x96d 85.90±.15 86.02±.13\n",
      "表3：在Wide-ResNet-28-10上的增強批次設置中，TA和Adv.AA的比較。我們報告五個運行的平均值。從這一部分可以得出結論，對於幾乎所有考慮的基準，跨數據集、模型甚至增強應用的方式，TA都是表現最好的方法之一。\n",
      "4.1.2在固定的培訓設置中重現結果的比較\n",
      "在上一節中，我們試圖通過比較非常相似的設置獲得的結果來減輕混淆因素，而在本節中，我們更進一步地再現了四種方法的結果，並將我們的基線方法與這些再現進行比較，以得出真正的相互比較。由於我們提出了一種非常簡單和便宜的增強方法，因此我們選擇了RA、Fast AA和UA作為其他簡單和便宜的增強方法進行比較。\n",
      "原文: Ad-\n",
      "ditionally, we compare to AA, as an important, common\n",
      "baseline.Moreover, for all of these methods relevant infor-\n",
      "mation for reproduction was published3.For RA, AA and Fast AA we used the published policies\n",
      "and did not search for an augmentation policy from scratch.We based both our RA and AA implementations on a pub-\n",
      "lic codebase4by the authors of both RA and AA that im-\n",
      "plements AA for the CIFAR datasets.Likewise, for Fast\n",
      "AA we based our implementation on a public codebase.No code is published for UA, and there are multiple hyper-\n",
      "parameters missing in the paper; in these cases, we used the\n",
      "hyper-parameters from RA.For our reproduction of UA,\n",
      "we also adopted the same discretization of the augmenta-\n",
      "tion strengths into 31 values used by the other methods.In\n",
      "addition to the original augmentation space of UA we also\n",
      "perform experiments with the RA augmentation space.We reran experiments for CIFAR-10, CIFAR-100 and\n",
      "SVHN Core, and present the results in Table 4.For each\n",
      "method we ran the benchmarks included in the original\n",
      "work.\n",
      "翻譯結果: 此外，我們會將結果與AA進行比較，因為AA是一個重要且常見的基準。此外，對於所有這些方法，都有相關的重現資訊。對於RA、AA和Fast AA，我們使用了公佈的政策，並未從頭開始搜尋擴充政策。我們基於作者編寫的公開代碼，實現了RA和AA的兩個版本，該代碼庫用於CIFAR數據集上的AA的實現。同樣，對於Fast AA，我們也基於一個公開的代碼庫進行了實現。由於UA沒有發佈代碼，而論文中缺乏多個超參數，因此在這些情況下，我們使用了RA的超參數。對於我們的UA重現，我們還採用了其他方法使用的將增強強度離散化為31個值的方法。除了UA的原始增強空間外，我們還對RA增強空間進行了實驗。我們重新運行了CIFAR-10、CIFAR-100和SVHN Core的實驗，並在表4中呈現了結果。對於每種方法，我們運行了原始工作中包含的基準測試。\n",
      "原文: Generally, we could reproduce most results or even\n",
      "improve upon published results.The only severe exception\n",
      "is RA for which we tried multiple changes to the setup, but\n",
      "were not able to reach their published scores – neither for\n",
      "CIFAR nor for SVHN Core.In this evaluation, TA (Wide) performed best across all\n",
      "methods for each benchmark with a Wide-Resnet-28-10,\n",
      "and TA (RA) performed best for both Wide-Resnet-40-2\n",
      "benchmarks.In addition to the reproductions of published policies, we\n",
      "applied RandAugment to the Wide-ResNet-40-2 on CIFAR-\n",
      "3.See Table 12 in the appendix for an overview of the published mate-\n",
      "rials of different methods\n",
      "4.https://github.com/tensorflow/models/tree/\n",
      "fd34f711f319d8c6fe85110d9df6e1784cc5a6ca/\n",
      "research/autoaugment\n",
      "翻譯結果: 一般來說，我們可以重現大多數結果，甚至可以改進已發表的結果。唯一的例外是RA，我們嘗試了多次更改設置，但無法達到他們公佈的得分-在CIFAR和SVHN Core都是如此。在這個評估中，使用Wide-Resnet-28-10，TA（Wide）在每個基準測試中表現最佳；使用Wide-Resnet-40-2，TA（RA）則在兩個基準測試中表現最佳。除了重現已發表的政策外，我們還將RandAugment應用於CIFAR-3上的Wide-ResNet-40-2。有關不同方法發表材料的概述，請參見附錄中的表12。\n",
      "\n",
      "4. https://github.com/tensorflow/models/tree/\n",
      "fd34f711f319d8c6fe85110d9df6e1784cc5a6ca/\n",
      "research/autoaugment\n",
      "原文: 10, which was originally not considered in the RA pa-\n",
      "per.We therefore had to search for a policy first.De-\n",
      "pending on the task, Cubuk et al .[2] considered differ-\n",
      "ent subsets of the full range of the augmentation strengths\n",
      "M⊂ {1, . . . , 30}and the number of consecutive augmen-\n",
      "tations N⊂ {1, . . . , 3}.In order to avoid missing the best\n",
      "candidates and to not require human intuition we searched\n",
      "on all 90 resulting combinations of RA's parameters.We\n",
      "split up a validation set of 10000 examples like in the orig-\n",
      "inal RandAugment method to evaluate the settings.We\n",
      "then picked the best setting and compared it to TA.Table\n",
      "5 clearly shows that TA performs better than the costly RA\n",
      "setup, even though the RA setup in total required 91 full\n",
      "trainings, compared to a single training for TA.Finally, we consider three more evaluations in the ap-\n",
      "pendix: (i) We show that TA performs comparably or better\n",
      "on the same augmentation space with other automatic aug-\n",
      "mentation methods (see Appendix B), (ii) we show that TA\n",
      "generalizes to more peculiar datasets (see Appendix C) and\n",
      "(iii) we show TA's effectiveness with the EfficientNet Ar-\n",
      "chitecture [18] (see Appendix D).\n",
      "翻譯結果: 在RA論文中原本沒有考慮到的是10，因此我們必須先尋找一個政策。根據任務的不同，Cubuk等人[2]考慮了不同的增強強度子集M ⊂{1, ...，30}和連續增強次數N ⊂{1, ...，3}。為了避免錯過最佳候選項並不需要人類直覺 ，我們尋找了所有90種RA參數的組合。我們像原始的RandAugment方法一樣，分開了10000個示例的驗證集來評估設置，然後選擇最佳設置並將其與TA進行比較。表5清楚地顯示，即使RA總共需要進行91個完整的訓練，而TA只需要進行一次訓練，但TA仍然優於花費高昂的RA設置。最後，我們在附錄中進行了三個更多的評估：（i）我們展示了TA在相同的增強空間中與其他自動增強方法相當或更好（參見附錄B）；（ii）我們展示了TA適用於更奇特的數據集（參見附錄C）；（iii）我們展示了TA在EfficientNet Architecture [18]中的有效性（參見附錄D）。\n",
      "原文: WRN-28-10 CIFAR-10 CIFAR-100\n",
      "AA 97.31±.22 (-.09) 82.91 ±.41 (+.01)\n",
      "FAA 97.43±.09 (+.13) 83.27 ±.13 (+.57)\n",
      "RA 97.12±.14 (-.18) 83.1 ±.32 (-.20)\n",
      "UA (UA) 97.46±.14 (+.13) 83.08 ±.27 (+.26)\n",
      "UA (RA) 97.44±.09 83.36 ±.18\n",
      "TA (RA) 97.46±.09 83.54 ±.12\n",
      "TA (Wide) 97.46±.06 84.33±.17\n",
      "(a)\n",
      "WRN-40-2 CIFAR-10 CIFAR-100\n",
      "AA 96.38±.10 (+.08) 79.66 ±.17 (+.36)\n",
      "FAA 96.39±.06 (-.01) 79.79 ±.21 (+.39)\n",
      "UA (UA) 96.42±.04 (+.17) 79.74 ±.15 (+.73)\n",
      "UA (RA) 96.45±.06 79.95±.20\n",
      "TA (RA) 96.62±.09 79.99±.16\n",
      "TA (Wide) 96.32±.05 79.86±.19\n",
      "(b)\n",
      "WRN-28-10 SVHN Core\n",
      "AA 97.99±.06 (-.01)\n",
      "RA 98.06±.04 (-.24)\n",
      "TA (RA) 98.05±.02\n",
      "TA (Wide) 98.11±.03\n",
      "(c)\n",
      "Table 4: A reproduction of the results of previous work with\n",
      "a Wide-ResNet-28-10 on CIFAR (a) and SVHN Core (c),\n",
      "and with a Wide-ResNet-40-2 on CIFAR (b).We report the\n",
      "relative performance difference to the published results in\n",
      "parentheses.Method Acc.Brute-Force RA 96.42±.09\n",
      "TA (RA) 96.62±.09\n",
      "Table 5: Average over ten runs on CIFAR-10 with a Wide-\n",
      "ResNet-40-2.TA performs better than the over 80-times\n",
      "more expensive exhaustive search over RA's parameters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻譯結果: WRN-28-10 CIFAR-10 CIFAR-100\n",
      "AA 97.31±.22 (-.09) 82.91±.41 (+.01)\n",
      "FAA 97.43±.09 (+.13) 83.27±.13 (+.57)\n",
      "RA 97.12±.14 (-.18) 83.1±.32 (-.20)\n",
      "UA (UA) 97.46±.14 (+.13) 83.08±.27 (+.26)\n",
      "UA (RA) 97.44±.09 83.36±.18\n",
      "TA (RA) 97.46±.09 83.54±.12\n",
      "TA (Wide) 97.46±.06 84.33±.17\n",
      "(a)\n",
      "WRN-40-2 CIFAR-10 CIFAR-100\n",
      "AA 96.38±.10 (+.08) 79.66±.17 (+.36)\n",
      "FAA 96.39±.06 (-.01) 79.79±.21 (+.39)\n",
      "UA (UA) 96.42±.04 (+.17) 79.74±.15 (+.73)\n",
      "UA (RA) 96.45±.06 79.95±.20\n",
      "TA (RA) 96.62±.09 79.99±.16\n",
      "TA (Wide) 96.32±.05 79.86±.19\n",
      "(b)\n",
      "WRN-28-10 SVHN Core\n",
      "AA 97.99±.06 (-.01)\n",
      "RA 98.06±.04 (-.24)\n",
      "TA (RA) 98.05±.02\n",
      "TA (Wide) 98.11±.03\n",
      "(c)\n",
      "表4：使用Wide-ResNet-28-10對CIFAR（a）和SVHN Core（c）以及使用Wide-ResNet-40-2對CIFAR（b）的先前工作結果的複製。括號中的相對表現差異與已發布的結果有關。方法 Acc. Brute-Force RA 96.42±.09 TA (RA) 96.62±.09\n",
      "表5：使用Wide-ResNet-40-2在CIFAR-10上進行10次平均。 TA的表現優於RA參數上的超過80倍的費力搜尋。\n",
      "原文: 4.1.3 Comparison by Total Compute Costs\n",
      "In the previous sections, we compared different augmenta-\n",
      "tion methods for a fixed training setup.We now consider the\n",
      "other extreme, comparing all methods across models and\n",
      "setups by their compute requirements.In Figure 3, we plot this comparison for many CIFAR-\n",
      "100 setups across the literature.The question this plot an-\n",
      "swers is: given some compute budget, what method should\n",
      "we choose for the best final accuracy?For this plot, we used\n",
      "the accuracy numbers published in the literature and esti-\n",
      "mated the compute costs in RTX 2080 Ti GPU-hours.See\n",
      "Appendix E for a detailed account of the information used\n",
      "to calculate the compute cost approximations for all setups.We had to restrict the set of models we considered to the set\n",
      "of models for which we know from our experiments how\n",
      "expensive they are to run, namely all Wide-ResNet setups\n",
      "and the ShakeShake-26-2x96d.We tried to be as conserva-\n",
      "tive as possible regarding the compute requirements of other\n",
      "methods, to not give TA an unfair advantage.\n",
      "翻譯結果: 4.1.3 總計算成本比較\n",
      "在前面的章節中，我們比較了不同的增強方法在固定的訓練配置下的表現。現在我們考慮另一種極端情況，通過它們的計算成本比較所有方法跨模型和設置。在圖3中，我們將此比較繪製在許多CIFAR-100設置中。這個圖回答的問題是：在一定的計算預算下，我們應該選擇哪種方法來獲得最佳的最終準確性？對於這個圖，我們使用了發表在文獻中的準確性數據，並估計了在RTX 2080 Ti GPU小时的計算成本。有關所有設置的計算成本近似值的計算信息的詳細説明，請參見附錄E。我們不得不限制我們考慮的模型集合，這些模型是根據我們的實驗知道他們運行的花費的所有Wide-ResNet設置和ShakeShake-26-2x96d。為了不給TA帶來不公平的優勢，我們盡可能保守地考慮了其他方法的計算要求。\n",
      "原文: In the figure, for all considered budgets, TA and its vari-\n",
      "10 100 1,0007880828486\n",
      "GPU hoursAccuracy (%)AA Fast AA\n",
      "RA AWS\n",
      "UA AWS x8\n",
      "TA (RA) Adv.AA x8\n",
      "TA (Wide) TA (Wide) x8\n",
      "Figure 3: Comparison of the final test accuracy on CIFAR-\n",
      "100 in comparison to RTX2080ti GPU-hours compute in-\n",
      "vested for augmentation search and final model training\n",
      "across a set of models.Methods marked with x8use batch\n",
      "augmentations[10].\n",
      "翻譯結果: 在這個圖表中，對於所有考慮的預算，TA及其變體產生的GPU計算時間分別是：10、100、1,000。比較起RTX2080ti GPU應用於擴增搜索和最終模型訓練所需的計算時間，我們可以看到最終的CIFAR-100測試準確性。其中，標有x8的方法使用了批次擴增[10]。RA AWS和UA AWS分別代表AWS的常規實例和增強實例，TA (RA) Adv.AA x8和TA (Wide) TA (Wide) x8分別代表不同方法的變體。\n",
      "原文: Augmentation space SVHN Core CIFAR-10\n",
      "Full 97.63±.06 97.24 ±.03\n",
      "AA 98.04±.02 97.47 ±.11\n",
      "AA -{Invert} 97.97±.08 97.55±.06\n",
      "RA 98.05±.02 97.46 ±.09\n",
      "Wide 98.11±.03 97.46 ±.06\n",
      "UA 98.06±.04 97.42 ±.07\n",
      "OHL 98.10±.02 97.45 ±.05\n",
      "Table 6: Evaluation of TA on SVHN Core and CIFAR-10\n",
      "with a set of 7 different augmentation spaces.Note that\n",
      "RA =AA− {SamplePairing ,Invert ,Cutout }and UA =\n",
      "AA− {SamplePairing }.ant with augmented batch (TA x8) perform among the best\n",
      "methods.TA also has a clear benefit compared to the popu-\n",
      "lar cheap methods Fast AA and RA for all compute budgets;\n",
      "finally, it is dramatically cheaper than AA.4.2.Understanding the Minimal Requirements of\n",
      "TA\n",
      "While so far, we have demonstrated that in many circum-\n",
      "stances TA's approach of only using a single augmentation\n",
      "per image is enough or yields even better performance than\n",
      "more complicated methods, in this section we will dissect\n",
      "other properties of TA.We first analyse how TA behaves across augmentation\n",
      "spaces from the literature.We then look at its perfor-\n",
      "mance after we apply random changes to its augmentation\n",
      "space.\n",
      "翻譯結果: 表格6:在一組7種不同的增強空間上評估SVHN Core和CIFAR-10上的TA。需要注意的是，RA = AA- {SamplePairing, Invert, Cutout}，UA = AA- {SamplePairing}。使用增強批次（TA x8）的TA方法表現出了最佳方法之一。TA對於所有運算預算均明顯優於流行且便宜的方法Fast AA和RA;最後，它比AA便宜得多。 \n",
      "\n",
      "4.2.了解TA的最低要求\n",
      "到目前為止，我們已經證明，在許多情況下，TA的方法只使用單個圖像進行增強就足夠或甚至比更複雜的方法造成更好表現。在本節中，我們將剖析TA的其他屬性。首先，我們分析了TA在來自文獻中的增強空間上的行為。然後，我們觀察TA在其增強空間隨機更改後的表現。\n",
      "原文: Finally, we consider sets of different augmentation\n",
      "strengths from which TA samples.4.2.1 TA with Different Hand-Picked Augmentation\n",
      "Spaces\n",
      "For this evaluation, we carefully reimplemented the aug-\n",
      "mentation spaces of AA, UA and OHL, besides the one of\n",
      "RA.Additionally, we consider a larger augmentation space\n",
      "(Full), which is a super set of AA, and additionally contains\n",
      "a blur, a smooth, a horizontal and a vertical flip.Especially\n",
      "the vertical flip is likely not useful for very many classifica-\n",
      "tion tasks.See Table 8 in the appendix for an overview of\n",
      "the augmentation spaces.Table 6 indeed shows that TA performs worse on the full\n",
      "augmentation space than on all other augmentation spaces\n",
      "for a Wide-ResNet-28-10 on both SVHN Core and CIFAR-\n",
      "10.We also included another augmentation space not con-\n",
      "sidered in the previous literature: a variant of the AA aug-\n",
      "mentation space, where we removed the extreme invert op-\n",
      "eration, which maps each pixel xto255−x.We can see that\n",
      "this augmentation space performs very well for CIFAR-10,\n",
      "but not great for SVHN Core.\n",
      "翻譯結果: 最後，我們考慮使用不同強度的增強集來進行TA樣本。4.2.1使用不同手選增強空間的TA\n",
      "\n",
      "對於這個評估，除了RA的增強空間之外，我們仔細重新實現了AA，UA和OHL的增強空間。此外，我們考慮了一個更大的增強空間（Full），它是AA的超集，並且還包含模糊、平滑、水平和垂直翻轉。特別是垂直翻轉對於非常多的分類任務可能沒有用。有關增強空間的概述，請參見附錄中的表8。表6確實顯示TA在Wide-ResNet-28-10在SVHN Core和CIFAR-10上的表現比所有其他增強空間都差。我們還包括了另一個先前文獻中沒有考慮的增強空間：AA增強空間的一個變體，其中我們刪除了極端反轉操作，它將每個像素x映射到255-x。我們可以看到這個增強空間在CIFAR-10上表現非常好，但在SVHN Core上卻不是很好。\n",
      "原文: This aligns well with ob-\n",
      "servations made by earlier work, indicating that the invert4 8 10 11 12 13 1495.759696.2596.596.75\n",
      "Number of AugmentationsAccuracy (%)\n",
      "Figure 4: The performance of WRN-40-2 models depend-\n",
      "ing on the size of sampled subsets of the RA augmentations\n",
      "on CIFAR-10.We performed 10 evaluations per subset size.augmentation fosters generalization on SVHN, but not on\n",
      "the other datasets[1].A peculiarity of the OHL augmenta-\n",
      "tion space is that it only uses three strengths, unlike all other\n",
      "methods which consider 31 strengths.Interestingly, this is\n",
      "not harmful and OHL yields the best score for SVHN Core.We can see that the performance of TA is rather stable\n",
      "between augmentation spaces, but still there seems to be\n",
      "room for improvement by a more sophisticated method to\n",
      "choose the augmentation space for TA depending on the\n",
      "task.4.2.2 TA's Behavior With Randomly Pruned Augmen-\n",
      "tation Spaces\n",
      "While we assessed performance with different hand-crafted\n",
      "augmentation spaces above, now we want to analyze how\n",
      "performance is impacted if we only use random subsets of\n",
      "the 14 augmentations in the RA augmentation space (which\n",
      "we used in the other experiments unless otherwise stated).\n",
      "翻譯結果: 這與以前的研究所觀察到的情況相符，表明增強技術能在SVHN上促進泛化，但對其他數據集沒有幫助[1]。OHL增強空間的一個特點是它只使用三個強度，而其他所有方法都考慮31個強度。有趣的是，這並不會造成傷害，而且OHL能夠獲得SVHN Core的最好分數。我們可以看到，TA的表現在增強空間之間相當穩定，但仍然有改進的空間，需要更複雜的方法來選擇TA的增強空間，具體取決於任務。\n",
      "\n",
      "\n",
      "在上述使用不同手工製作的增強空間進行性能評估後，我們現在想分析如果我們只使用RA增強空間的14個增強中的隨機子集，對性能的影響如何（除非另有說明，否則我們在其他實驗中使用RA增強空間）。\n",
      "原文: In Figure 4, we analyze the performance and its variance\n",
      "for multiple augmentation subset sizes for a Wide-ResNet-\n",
      "40-2 on CIFAR-10.We performed 10 evaluations per sam-\n",
      "ple size, where in each evaluation we picked a random\n",
      "sample of augmentations.While performance decreases as\n",
      "fewer and fewer augmentations are considered, we can see\n",
      "that it drops very slowly.We can throw away 4 of 14 aug-\n",
      "mentations and still obtain performance close to the original\n",
      "performance.Another trend is that with fewer augmenta-\n",
      "tions the variance increases.This is likely due to the ran-\n",
      "domness of the subset choice per run, which increases for\n",
      "smaller subsets.4.2.3 The Impact of the Set of Strengths on TA's Per-\n",
      "formance\n",
      "Before, we mostly considered the impact of different sets\n",
      "of augmentations; now we consider the other component of\n",
      "the augmentation space: the set of strengths.\n",
      "翻譯結果: 在圖4中，我們分析對CIFAR-10上Wide-ResNet-40-2的多個增強子集大小的性能及其變異性。我們對每個樣本大小進行了10次評估，在每次評估中，我們選擇一個隨機的增強樣本。性能隨著考慮的增強數量的減少而降低，但可以看出下降速度非常慢。我們可以放棄14個增強中的4個，但仍然可以獲得接近原始性能的表現。另一個趨勢是，隨著增強數量的減少，變異性增加。這可能是由於每次運行的子集選擇的隨機性增加了。4.2.3 強度集對TA性能的影響\n",
      "\n",
      "之前，我們主要考慮了不同增強集的影響；現在我們考慮增強空間的另一個組成部分：強度集。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: Strengths CIFAR-10 CIFAR-100 SVHN Core\n",
      "{30} 97.45±.05 82.98 ±.22 98.16±.03\n",
      "{0,30} 97.51±.08 83.46±.10 98.02 ±.02\n",
      "{0,15,30} 97.46±.06 83.43±.24 98.04 ±.03\n",
      "{0, . . . , 30} 97.46±.09 83.54±.12 98.05 ±.02\n",
      "Table 7: A comparison of the performance of TA (RA) on\n",
      "different datasets with a Wide-ResNet-28-10 using different\n",
      "subsets of strengths.In Table 7, we analyze the performance of TA with a\n",
      "Wide-ResNet-28-10 and different subsets of the original set\n",
      "of possible strengths {0, . . . , 30}on the RA augmentation\n",
      "space.We can see that the CIFAR-10 setup seems to be\n",
      "relatively agnostic to the set of strengths.Performance on\n",
      "CIFAR-100, on the other hand, is very negatively impacted\n",
      "by choosing the subset {30}.In general, performance im-\n",
      "proves on CIFAR-100 with larger sets.For SVHN Core, the\n",
      "opposite is the case: performance improves when only con-\n",
      "sidering {30}.A reason for this could be that the majority\n",
      "of the augmentations are color based and changing the col-\n",
      "ors of a single-color background and a single-color number\n",
      "drastically, still in most cases yields valid house numbers.\n",
      "翻譯結果: 表格7：使用不同強度子集的Wide-ResNet-28-10在不同資料集上進行TA（RA）表現比較。\n",
      "在表格7中，我們使用Wide-ResNet-28-10和原始可能強度集{0，...，30}的不同子集分析了TA的表現。可以看出，CIFAR-10設置似乎對強度集不太敏感。另一方面，選擇子集{30}對CIFAR-100的表現非常負面影響。通常來說，CIFAR-100隨著集合的增大而性能提高。對於SVHN核心而言，情況剛好相反：僅考慮{30}時的表現有所提高。原因可能是大多數擴增基於顏色，改變單色背景和單色數字的顏色，在大多數情況下仍然可以產生有效的房屋號碼。\n",
      "原文: Another observation we made is that it does not matter\n",
      "so much for any setup whether we reduce to three or just\n",
      "two augmentation strengths, compared to all 31.This seems\n",
      "to point towards the importance of a mixture of strong\n",
      "and weak augmentations.At the same time three differ-\n",
      "ent strengths, compared to 31, seem to be enough for these\n",
      "settings.5.Automatic Augmentation Methods in\n",
      "Practice\n",
      "While there are many expensive or hard to reproduce\n",
      "automatic augmentation methods, it is important that aug-\n",
      "mentation methods are practical: the impact of automatic\n",
      "augmentation methods unfolds in the application to new se-\n",
      "tups and problems.We evaluated many different settings\n",
      "and augmentation methods and we would like to pass on\n",
      "the gained knowledge.First, we have compiled a short summary of learnings for\n",
      "the application of augmentation methods in Appendix F.\n",
      "Second, in addition to our full codebase, we provide\n",
      "a simple one-file python library that implements the more\n",
      "practical augmentation methods: RA, UA and TA.\n",
      "翻譯結果: 我們觀察到的另一個現象是，在任何設置中，與所有31種增強方法相比，將其減少到三種或僅兩種增強強度並不重要。這似乎表明強弱增強方法的混合很重要。同時，對於這些設置來說，相比於31種，三種不同的強度似乎已經足夠了。\n",
      "\n",
      "5.自動增強方法在實踐中\n",
      "\n",
      "儘管有許多昂貴或難以再現的自動增強方法，但重要的是增強方法要實用：自動增強方法的影響在於應用於新的設置和問題。我們評估了許多不同的設置和增強方法，我們希望傳遞所獲得的知識。首先，我們在附錄F中總結了增強方法應用的經驗教訓。其次，除了完整的代碼庫之外，我們還提供了一個簡單的一個文件的Python庫，該庫實現了更實用的增強方法：RA、UA和TA。\n",
      "原文: It even\n",
      "allows choosing from all augmentation spaces considered\n",
      "in this work.For example, to get an image augmenter for\n",
      "TA and transform a PIL image img, one can call\n",
      "1aug = TrivialAugment(n,m)\n",
      "2augmented_img = aug(img)6.Best Practices Proposal for Research\n",
      "We found that it is difficult to reimplement many of the\n",
      "published methods, see Table 12 in the appendix.We also\n",
      "found that many methods performed similarly to the simple\n",
      "TA baseline, when we follow their setup.Here, we com-\n",
      "pile a short bullet point list of best practices we believe are\n",
      "important for sustainable research in this field.• Share code as much as possible for easy entry of be-\n",
      "ginners and to make sure that setups are similar across\n",
      "papers.Otherwise, differences between the actual im-\n",
      "plementation and its description in the paper can im-\n",
      "pair reproducibility.• Compare fairly to other methods and baselines with\n",
      "the same setup, train budget and augmentation space,\n",
      "or reproduce results of previous methods in your setup\n",
      "and mention differences.\n",
      "翻譯結果: 甚至可以選擇此工作中考慮的所有增強空間。例如，要獲取用於TA的圖像增強新，並轉換PIL圖像img，可以調用：\n",
      "1aug = TrivialAugment(n,m)\n",
      "2augmented_img = aug(img)\n",
      "\n",
      "6. 最佳實踐研究建議\n",
      "我們發現重新實現許多已發表的方法很困難，請參閱附錄中的表12。此外，當我們遵循它們的設置時，許多方法表現類似於簡單的TA基線。在這裡，我們編譯了一個短的彈珠清單，我們認為這些是可持續研究在此領域的重要最佳實踐。\n",
      "• 盡可能共享代碼，以便初學者輕鬆入門，並確保跨論文之間設置相似。否則，實際實現與論文描述之間的差異可能會影響可重現性。\n",
      "• 公平地比較其他方法和有相同設置，訓練預算和增強空間的基線，或在您的設置中重現以前方法的結果並註明差異。\n",
      "原文: • Report confidence intervals to discern “outperform-\n",
      "ing” from “performing comparably”.7.Limitations\n",
      "While we could not find settings where TA failed for im-\n",
      "age classification, we found that TA does not work out-of-\n",
      "the-box for object detection setups and also needs tuning to\n",
      "work for this task.So far, we can only wholeheartedly rec-\n",
      "ommend the use of TA for image classification; its applica-\n",
      "tion to other computer vision tasks requires further study.8.Conclusion\n",
      "Most of the approaches considered as automatic aug-\n",
      "mentation methods are complicated.In this work, we pre-\n",
      "sented TA, a very simple augmentation algorithm from\n",
      "which we can learn three main things.First, TA teaches us about a crucial baseline missing for\n",
      "automatic augmentation methods.Second, TA teaches us to never overlook the simplest so-\n",
      "lutions.There are a lot of complicated methods to automat-\n",
      "ically find augmentation policies, but the simplest method\n",
      "was so-far overlooked, even though it performs comparably\n",
      "or better.Third, randomness in the chosen strengths appears to be\n",
      "very important for good performance.\n",
      "翻譯結果: • 報告置信區間以區分「表現優異」和「表現相似」。\n",
      "• 限制\n",
      "雖然我們找不到 TA 在圖像分類中失敗的情況，但我們發現 TA 不適用於物體檢測設置，並且需要調整以適用於此任務。到目前為止，我們只能全心全意地推薦 TA 用於圖像分類；其對其他計算機視覺任務的應用需要進一步研究。\n",
      "• 總結\n",
      "大多數被認為是自動擴充方法的方法都很複雜。在這項工作中，我們介紹了 TA，一種非常簡單的擴充算法，我們可以從中學到三個主要的東西。首先，TA教我們缺失了一個關鍵的基線自動擴充方法。其次，TA教我們不要忽略最簡單的解決方案。有許多複雜的方法可以自動查找擴充策略，但是最簡單的方法迄今被忽視了，即使它的表現相當或更好。第三，所選擇的強度中的隨機性似乎對良好表現非常重要。\n",
      "原文: Acknowledgements\n",
      "We want to thank Ildoo Kim for his open-source code-\n",
      "base which ours forks from, and the reviewers for their in-\n",
      "sightful comments.We acknowledge funding by the Robert\n",
      "Bosch GmbH and the European Research Council (ERC)\n",
      "under the European Union Horizon 2020 research and in-\n",
      "novation programme through grant no.716721.\n",
      "翻譯結果: 致謝\n",
      "我們要感謝Ildoo Kim提供的開源程式碼庫，這是我們參考的出處，同時也由衷感謝審稿人們具有啟發性的評論。我們也感謝Robert Bosch GmbH和歐洲研究委員會（ERC）的資助，該項目通過716721號贈款在歐盟Horizon 2020研究和創新計劃下開展研究。\n",
      "原文: References\n",
      "[1] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-\n",
      "van, and Quoc V Le.Autoaugment: Learning augmentation\n",
      "strategies from data.In Proceedings of the IEEE/CVF Con-\n",
      "ference on Computer Vision and Pattern Recognition , pages\n",
      "113–123, 2019.[2] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\n",
      "Le.Randaugment: Practical automated data augmenta-\n",
      "tion with a reduced search space.In Proceedings of the\n",
      "IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition Workshops , pages 702–703, 2020.[3] Terrance DeVries and Graham W. Taylor.Improved regular-\n",
      "ization of convolutional neural networks with cutout, 2017.[4] Marzieh Fadaee, Arianna Bisazza, and Christof Monz.Data\n",
      "augmentation for low-resource neural machine translation.InProceedings of the 55th Annual Meeting of the Associ-\n",
      "ation for Computational Linguistics (Volume 2: Short Pa-\n",
      "pers) , pages 567–573, 2017.[5] Xavier Gastaldi.Shake-shake regularization, 2017.[6] Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr\n",
      "Doll´ar, and Kaiming He.\n",
      "翻譯結果: 參考文獻\n",
      "[1] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan和Quoc V Le。Autoaugment：從數據中學習增強策略。在IEEE/CVF計算機視覺和模式識別會議論文集中，頁113-123，2019年。\n",
      "[2] Ekin D Cubuk，Barret Zoph，Jonathon Shlens和Quoc V Le。Randaugment：具有減少搜索空間的實用自動數據增強。在IEEE/CVF計算機視覺和模式識別會議論文集工作坊中，頁702-703，2020年。\n",
      "[3] Terrance DeVries和Graham W. Taylor。使用刪除改進卷積神經網絡的正則化，2017年。\n",
      "[4] Marzieh Fadaee，Arianna Bisazza和Christof Monz。用於低資源神經機器翻譯的數據增強。在計算語言學協會第55屆年會（第2卷：短論文）中，頁567-573，2017年。\n",
      "[5] Xavier Gastaldi。Shake-shake正則化，2017年。\n",
      "[6] Ross Girshick，Ilija Radosavovic，Georgia Gkioxari，Piotr Dollar和Kaiming He。\n",
      "原文: Detectron.https://github.com/facebookresearch/detectron , 2018.[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition.In Proceed-\n",
      "ings of the IEEE conference on computer vision and pattern\n",
      "recognition , pages 770–778, 2016.[8] Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph,\n",
      "Justin Gilmer, and Balaji Lakshminarayanan.AugMix: A\n",
      "simple data processing method to improve robustness and\n",
      "uncertainty.Proceedings of the International Conference on\n",
      "Learning Representations (ICLR) , 2020.[9] Daniel Ho, Eric Liang, Xi Chen, Ion Stoica, and Pieter\n",
      "Abbeel.Population based augmentation: Efficient learning\n",
      "of augmentation policy schedules.In International Confer-\n",
      "ence on Machine Learning , pages 2731–2741.PMLR, 2019.[10] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten\n",
      "Hoefler, and Daniel Soudry.Augment your batch: Improving\n",
      "generalization through instance repetition.In Proceedings of\n",
      "the IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition (CVPR) , June 2020.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻譯結果: Detectron。https://github.com/facebookresearch/detectron，2018年。[7] Kaiming He、Xiangyu Zhang、Shaoqing Ren和Jian Sun。深度殘差學習用於圖像識別。在計算機視覺和模式識別IEEE會議錄中的文章，第770-778頁，2016年。[8] Dan Hendrycks、Norman Mu、Ekin D. Cubuk、Barret Zoph、Justin Gilmer和Balaji Lakshminarayanan。 AugMix：一種簡單的數據處理方法，以提高韌性和不確定性。國際學習表示（ICLR）會議論文集，2020年。[9] Daniel Ho、Eric Liang、Xi Chen、Ion Stoica和Pieter Abbeel。基於人口的增強：增強策略表現的有效學習。在機器學習的國際會議上，第2731-2741頁。PMLR，2019年。[10] Elad Hoffer、Tal Ben-Nun、Itay Hubara、Niv Giladi、Torsten Hoefler和Daniel Soudry。增加你的批次：通過實例重複提高泛化能力。在計算機視覺和模式識別（CVPR）IEEE / CVF會議上的會議論文集，2020年6月。\n",
      "原文: [11] Alex Krizhevsky et al.Learning multiple layers of features\n",
      "from tiny images.2009.[12] A. Krizhevsky, I. Sutskever, and G. Hinton.ImageNet clas-\n",
      "sification with deep convolutional neural networks.In P.\n",
      "Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Wein-\n",
      "berger, editors, Proceedings of the 26th International Con-\n",
      "ference on Advances in Neural Information Processing Sys-\n",
      "tems (NeurIPS’12) , pages 1097–1105, 2012.[13] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and\n",
      "Sungwoong Kim.Fast autoaugment.In H. Wallach, H.\n",
      "Larochelle, A. Beygelzimer, F. d'Alch ´e-Buc, E. Fox, and R.\n",
      "Garnett, editors, Advances in Neural Information Processing\n",
      "Systems , volume 32, pages 6665–6675.Curran Associates,\n",
      "Inc., 2019.[14] Chen Lin, Minghao Guo, Chuming Li, Xin Yuan, Wei\n",
      "Wu, Junjie Yan, Dahua Lin, and Wanli Ouyang.Online\n",
      "hyper-parameter learning for auto-augmentation strategy.InProceedings of the IEEE/CVF International Conference on\n",
      "Computer Vision (ICCV) , October 2019.[15] Tom Ching LingChen, Ava Khonsari, Amirreza Lashkari,\n",
      "Mina Rafi Nazari, Jaspreet Singh Sambee, and Mario A.\n",
      "Nascimento.\n",
      "翻譯結果: [11] Alex Krizhevsky等人。從微小圖像學習多層特徵。2009年。\n",
      "\n",
      "[12] A. Krizhevsky，I. Sutskever和G. Hinton。使用深度卷積神經網絡的ImageNet分類。在P. Bartlett，F. Pereira，C. Burges，L. Bottou和K. Weinberger（編者）的第26屆國際進展中的會議上，神經信息處理系統（NeurIPS'12）的論文集，頁1097-1105，2012。\n",
      "\n",
      "[13] Sungbin Lim，Ildoo Kim，Taesup Kim，Chiheon Kim和Sungwoong Kim。快速自動增強。在H. Wallach，H. Larochelle，A. Beygelzimer，F. d'Alch'e-Buc，E. Fox和R. Garnett（編者）的神經信息處理進展中，第32卷，頁6665-6675。Curran Associates，Inc.，2019年。\n",
      "\n",
      "[14] Chen Lin，Minghao Guo，Chuming Li，Xin Yuan，Wei Wu，Junjie Yan，Dahua Lin和Wanli Ouyang。用於自動增強策略的在線超參數學習。在IEEE / CVF國際計算機視覺會議（ICCV）的論文集中，於2019年10月。\n",
      "\n",
      "[15] Tom Ching LingChen，Ava Khonsari，Amirreza Lashkari，Mina Rafi Nazari，Jaspreet Singh Sambee和Mario A.Nascimento。\n",
      "原文: Uniformaugment: A search-free probabilistic\n",
      "data augmentation approach, 2020.[16] I. Loshchilov and F. Hutter.Sgdr: Stochastic gradient de-\n",
      "scent with warm restarts.In Proceedings of the International\n",
      "Conference on Learning Representations (ICLR’17) , 2017.[17] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis-\n",
      "sacco, Bo Wu, and Andrew Y . Ng.Reading digits in natural\n",
      "images with unsupervised feature learning.In NIPS Work-\n",
      "shop on Deep Learning and Unsupervised Feature Learning\n",
      "2011 , 2011.[18] Mingxing Tan and Quoc Le.Efficientnet: Rethinking model\n",
      "scaling for convolutional neural networks.In International\n",
      "Conference on Machine Learning , pages 6105–6114.PMLR,\n",
      "2019.[19] Keyu Tian, Chen Lin, Ming Sun, Luping Zhou, Junjie\n",
      "Yan, and Wanli Ouyang.Improving auto-augment via\n",
      "augmentation-wise weight sharing.Advances in Neural In-\n",
      "formation Processing Systems , 33, 2020.[20] Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and\n",
      "Quoc Le.Unsupervised data augmentation for consistency\n",
      "training.In H. Larochelle, M. Ranzato, R. Hadsell, M. F.\n",
      "Balcan, and H. Lin, editors, Advances in Neural Information\n",
      "Processing Systems , volume 33, pages 6256–6268.\n",
      "翻譯結果: Uniformaugment：一種無需搜索的機率性\n",
      "數據增強方法，2020年[16]。I. Loshchilov和F. Hutter。SGDR: 帶有暖重啟的隨機梯度下降。於國際學習代表性會議(ICLR'17) ，2017年發表[17]。Yuval Netzer、Tao Wang、Adam Coates、Alessandro Bis-\n",
      "sacco、Bo Wu和Andrew Y. Ng。使用無監督特徵學習讀取自然圖像中的數字。於NIPS Work-\n",
      "shop on Deep Learning和Unsupervised Feature Learning\n",
      "2011 ，2011年發表[18]。Mingxing Tan和Quoc Le。Efficientnet：重新思考模型\n",
      "尺度對於卷積神經網絡的影響。於國際機器學習會議上，2019年發表[18]。Keyu Tian、Chen Lin、Ming Sun、Luping Zhou、Junjie Yan和Wanli Ouyang。通過增強式權重共享改善自動增強。於神經信息處理系統中取得進展，33卷，2020年[19]。Qizhe Xie、Zihang Dai、Eduard Hovy、Thang Luong和Quoc Le。無監督的數據增強，用於一致性訓練。於Advances in Neural Information Processing Systems的H. Larochelle、M. Ranzato、R. Hadsell、M. F.\n",
      "Balcan和H. Lin編輯，卷33，頁6256-6268。\n",
      "原文: Curran\n",
      "Associates, Inc., 2020.[21] Sergey Zagoruyko and Nikos Komodakis.Wide residual net-\n",
      "works.In In Proceedings of BMCV’16 , 2016.[22] Xinyu Zhang, Qiang Wang, Jian Zhang, and Zhao Zhong.Adversarial autoaugment.In International Conference on\n",
      "Learning Representations , 2020.[23] Barret Zoph, Ekin D Cubuk, Golnaz Ghiasi, Tsung-Yi Lin,\n",
      "Jonathon Shlens, and Quoc V Le.Learning data augmenta-\n",
      "tion strategies for object detection.In European Conference\n",
      "on Computer Vision , pages 566–583.Springer, 2020.\n",
      "翻譯結果: Curran Associates, Inc.，2020年[21]。Sergey Zagoruyko和Nikos Komodakis。Wide residual net-\n",
      "works. In BMCV'16會議論文集中，2016年[22]。Xinyu Zhang，Qiang Wang，Jian Zhang和Zhao Zhong。Adversarial autoaugment。在學習代表國際會議上，2020年[23]。Barret Zoph，Ekin D Cubuk，Golnaz Ghiasi，Tsung-Yi Lin，\n",
      "Jonathon Shlens和Quoc V Le。學習數據擴充策略以進行物體檢測。在歐洲計算機視覺會議上，頁面566-583.Springer，2020年。\n",
      "原文: A.Training Settings\n",
      "For all setups we normalize the images by training set\n",
      "mean and standard deviation after the application of all aug-\n",
      "mentations, besides a final cutout, if applicable.A.1.CIFAR\n",
      "Following previous work we apply the vertical flip and\n",
      "the pad-and-crop augmentations and finally a 16 pixel\n",
      "cutout [3] after TA or generally any augmentation method.We trained Wide-ResNet models [21] in the Wide-ResNet-\n",
      "40-2 and the larger Wide-ResNet-28-10 settings.We trained\n",
      "these models for 200 epochs using SGD with Nesterov Mo-\n",
      "mentum and a learning rate of 0.1, a batch size of 128, a\n",
      "5e-4 weight decay, cosine learning rate decay [16].We trained ShakeShake-26-2x96d for 1600 epochs using\n",
      "SGD with Nesterov Momentum, a learning rate of 0.01, a\n",
      "batch size of 128, 1e-3 weight decay and a cosine learning\n",
      "rate decay.For the augmented batch setups we followed Zhang et al.[22].We used the settings above for the Wide-ResNet-28-\n",
      "10 evaluations.And like Zhang slightly different settings\n",
      "for ShakeShake.\n",
      "翻譯結果: A.訓練設置\n",
      "對於所有設置，我們會在應用所有增強技術之後，使用訓練集的平均值和標準差對圖像進行標準化，除非有其他最終的剪裁處理，例如最後的cutout处理。\n",
      "A.1 CIFAR\n",
      "遵循以前的工作，我們會應用垂直翻轉和填充裁剪增強技術，最後會在TA或任何增強方法之後使用16像素的cutout处理[3]。我們會在Wide-ResNet-40-2和較大的Wide-ResNet-28-10設置中進行Wide-ResNet模型[21]的訓練。我們使用了SGD和Nesterov Momentum進行訓練，學習率為0.1，批量大小為128，使用了5e-4的權重衰減和cosine學習率衰減[16]。對於ShakeShake-26-2x96d，我們使用了SGD和Nesterov Momentum進行了1600次訓練，學習率為0.01，批量大小為128，使用了1e-3的權重衰減和cosine學習率衰減。對於增強的批量設置，我們遵循了Zhang等人的做法[22]。我們將上述設置用於Wide-ResNet-28-10的評估。對於ShakeShake，我們使用了與Zhang稍有不同的設置。\n",
      "原文: We use 600 epochs, with a 0.2 learning\n",
      "rate and a 1e-4 weight decay.A.2.SVHN\n",
      "Unlike for CIFAR we do not apply extra augmentations\n",
      "for SVHN, besides a final 16 pixel cutout [3].For the full\n",
      "dataset we trained for 160 epochs using SGD with Nesterov\n",
      "Momentum of 0.9, a learning rate of 0.005, a batch size of\n",
      "128, a 1e-3 weight decay and cosine learning rate decay.For SVHN Core we train with the same settings, except that\n",
      "we trained for 200 epochs and used a larger weight decay\n",
      "of 5e-3.A.3.ImageNet\n",
      "Like for the other datasets we performed the standard\n",
      "augmentations of the dataset after the learned augmenta-\n",
      "tions.That is we performed a randomly resized crop and\n",
      "scales between 0.08 and 1.0 using bicubic interpolation.We\n",
      "augmented with horizontal flips, applied a color jitter with\n",
      "brightness, contrast and saturation strength set to 0.4 and we\n",
      "applied lighting noise with an alpha of 0.1.We trained on Imagenet with a ResNet-50 [7] and fol-\n",
      "lowed the setup of AA [1].We train for 270 epochs with a\n",
      "batch size of 2048 distributed among 32 workers.\n",
      "翻譯結果: 我們在 SVHN 使用 600 個 epochs，學習率為 0.2，權重衰減為 1e-4。和 CIFAR 不同的是，在 SVHN 中，我們沒有額外進行增強，除了最後的 16 像素切割 [3]。對於完整的數據集，我們使用 Nesterov Momentum 為 0.9 的 SGD 訓練了 160 個 epochs，訓練速率為 0.005，批次大小為 128，權重衰減為 1e-3，餘弦學習率衰減。對於 SVHN Core，我們使用相同的設置進行訓練，但我們訓練了 200 個 epochs，使用了更大的權重衰減為 5e-3。 \n",
      "\n",
      "對於 Imagenet，我們像其它數據集一樣，使用基礎盡量多的增強技術。使用雙立方插值，執行隨機改變大小和縮放比例，介於 0.08 和 1.0 之間。將圖像進行水平翻轉，使用亮度、對比度和飽和度強度設置為 0.4 的彩色抖動，並使用 alpha 為 0.1 的照明噪聲進行增強。我們使用 ResNet-50 [7]，並遵循 AA [1] 的設置，使用 2048 的批次大小在 32 個工作者之間訓練了 270 次。\n",
      "原文: We use\n",
      "image crops of height 224 considered both a 244 width of\n",
      "the images, like RA, and a 224 width, like AA.The initial\n",
      "learning rate of 0.1 is scaled proportional to the batch size\n",
      "divided by 256.As learning rate schedule we apply a step-\n",
      "wise 10-fold reduction after 90, 180 and 240 epochs with a\n",
      "linear warmup of factor 4 over the first 3 epochs.We use\n",
      "Nesterov Momenutm with a momentum parameter of 0.9\n",
      "and a weight decay of 1e-4.Unlike [1] we only use 32 instead of 64 workers out of\n",
      "cluster limitations and scale the learning rate accordingly.PIL operation range PIL operation range\n",
      "identity - auto contrast -\n",
      "equalize - rotate−30◦-+30◦\n",
      "(−135◦-+135◦)\n",
      "solarize0 - 256\n",
      "(0 - 256)color0.1 - 1.9.(0.01 - 2.)posterize4 - 8\n",
      "(2 - 8)contrast0.1 - 1.9.(0.01 - 2.)brightness0.1 - 1.9.(0.01 - 2.)sharpness0.1 - 1.9.(0.01 - 2.)shear x0.0 - 0.3\n",
      "0.0 - 0.99shear y0.0 - 0.3\n",
      "0.0 - 0.99\n",
      "translate x0 - 10\n",
      "(0 - 32)translate y0 - 10\n",
      "(0 - 32)\n",
      "cutout 0 - 0.2 .......invert -\n",
      "fliplr - flipud -\n",
      "sample pairing 0.0 - 0.4 blur -\n",
      "smooth -\n",
      "Table 8: An overview of the augmentation spaces.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻譯結果: 我們使用224高的圖像裁切，考慮到圖像的244和224寬度，例如RA和AA。初始學習率為0.1，與256除以批量大小成比例。作為學習率計劃，我們在90、180和240個epoch之后應用10倍的步驟式減少，最初的3個epoch線性疊加因子4。我們使用具有0.9的動量參數和1e-4的權重衰減的Nesterov Momenutm。與[1]不同的是，由於叢集限制，我們僅使用32個工人，並相應地調整學習率。PIL操作範圍PIL操作範圍身份-自動對比度-均衡化-旋轉30◦- + 30◦（- 135◦- + 135◦）-電晕0-256（0-256）-顏色0.1-1.9（0.01-2.）-海報化4-8（2-8）-對比度0.1-1.9（0.01-2.）-亮度0.1-1.9（0.01-2.）-銳度0.1-1.9（0.01-2.）-剪切x 0.0-0.3 0.0-0.99-剪切y 0.0-0.3 0.0-0.99-平移x 0-10（0-32）-平移y 0-10（0-32）-剪切0-0.2 ………反相-翻轉左右-翻轉上下-樣本配對0.0-0.4模糊-平滑-表8：增強空間的概述。\n",
      "原文: The un-\n",
      "marked operations are shared by all augmentation spaces\n",
      "and make up the RA augmentation space.The UA augmen-\n",
      "tation space additionally contains the dash underlined oper-\n",
      "ations and the ......OHL augmentation space additionally con-\n",
      "tains the dotted underlined operations.The ranges given\n",
      "here are the ones used for AA and RA with a discretiza-\n",
      "tion to thirty values.The UA augmentation space al-\n",
      "lows translation up to 14 pixels, but inherits all other set-\n",
      "tings from RA.The Wide augmentation space we use for\n",
      "batch augmentation has the same operations as RA, but\n",
      "uses the strength ranges in parantheses.The OHL aug-\n",
      "mentation space uses different ranges and a discretization\n",
      "to three values, see [14] for more details.All methods\n",
      "are defined as part of Pillow ( https://github.com/\n",
      "python-pillow/Pillow ), as part of ImageEnhance,\n",
      "ImageOps or as image attribute, besides cutout [3].We also\n",
      "provide operations with the exact same names in our code.B.Comparison of Different Methods on the\n",
      "Same Augmentation Space\n",
      "While in the above experiments we used the augmenta-\n",
      "tion space corresponding to each method in the evaluations,\n",
      "in this section, we probe the impact of these differences.\n",
      "翻譯結果: 未標記的操作適用於所有擴增空間，並組成RA擴增空間。UA擴增空間另外包含短劃線的操作和......OHL擴增空間另外包含點虛線的操作。此處給出的範圍是用於AA和RA的，重劃為30個值。UA擴增空間允許最多14個像素的平移，但所有其他設置都繼承自RA。我們用於批量擴增的Wide擴增空間具有與RA相同的操作，但使用括號內的強度範圍。OHL擴增空間使用不同的範圍和重劃為三個值，詳情請參見[14]。所有方法都定義為Pillow的一部分( https://github.com/python-pillow/Pillow )，作為ImageEnhance、ImageOps或圖像屬性，除cutout[3]外。我們還在我們的代碼中提供了具有完全相同的名稱的操作。 \n",
      "B.在相同的擴增空間上比較不同方法\n",
      "在上述實驗中，我們在評估中使用了與每種方法對應的擴增空間，在本節中，我們探討這些差異的影響。\n",
      "原文: We follow the setup of section 4.1.2 and compare our re-\n",
      "produced results of each method to TA on the exact same\n",
      "augmentation space as in the paper introducing the respec-\n",
      "tive method.Table 9 shows that TA's improvements gener-\n",
      "alize across augmentation spaces and methods.\n",
      "翻譯結果: 我們遵循第4.1.2節的設置，並將每種方法的再現結果與TA在完全相同的擴增空間上進行比較，以與介紹各自方法的論文相同。表9顯示TA的改進跨越了擴增空間和方法。\n",
      "原文: Dataset Setup AA FAA RA UA\n",
      "CIFAR-10 Method 97.31±.22 97.43 ±.09 97.12 ±.1497.46±.14\n",
      "TA 97.55±.0697.51±.0997.46±.0997.42±.07\n",
      "CIFAR-100 Method 82.91±.4183.27±.13 83.1 ±.32 83.08 ±.27\n",
      "TA 83.34±.1083.36±.1583.54±.1283.33±.14\n",
      "SVHN Method 97.99±.06 - 98.06±.0498.05±.04\n",
      "Core TA 98.04±.02 97.84 ±.0398.05±.0298.06±.04\n",
      "Table 9: Comparisons of various methods (in our reimplementation) to TA, using the exact same augmentation space.E.g.,\n",
      "for CIFAR-10 on the AA space, AA reached 97.31 ±.22 and TA reached 97.55 ±.06.No policy is published for FAA on\n",
      "SVHN Core, since this setup was not part of the FAA paper.Therefore, we do not reproduce FAA on SVHN Core.C. Evaluation on Special Datasets\n",
      "To further show that this method generalizes to more par-\n",
      "ticular image classification datasets without fine-tuning, we\n",
      "considered two more datasets, following the settings of Sec-\n",
      "tion 4.1.2.(i) Since we are not aware of an image recogni-\n",
      "tion dataset that contains occlusions, we created an occlu-\n",
      "sion variant of CIFAR-10 (Occ.\n",
      "翻譯結果: 資料集設定 AA FAA RA UA\n",
      "\n",
      "CIFAR-10 方法 97.31±0.22 97.43±0.09 97.12±0.14 97.46±0.14\n",
      "TA 97.55±0.06 97.51±0.09 97.46±0.09 97.42±0.07\n",
      "CIFAR-100 方法 82.91±0.41 83.27±0.13 83.10±0.32 83.08±0.27\n",
      "TA 83.34±0.10 83.36±0.15 83.54±0.12 83.33±0.14\n",
      "SVHN 方法 97.99±0.06 - 98.06±0.04 98.05±0.04\n",
      "Core TA 98.04±0.02 97.84±0.03 98.05±0.02 98.06±0.04\n",
      "\n",
      "表 9：各種方法（在我們的再現中）與 TA 的比較，使用完全相同的增強空間。例如，在 AA 空間上的 CIFAR-10 中，AA 達到了 97.31±0.22，而 TA 達到了 97.55±0.06。由於此設置不是 FAA 論文的一部分，因此沒有發布 FAA 在 SVHN Core 上的策略。因此，我們不在 SVHN Core 上再現 FAA。\n",
      "\n",
      "C.對特殊資料集的評估\n",
      "\n",
      "為了進一步展示該方法對於更特殊的圖像分類資料集的泛化能力，我們考慮了另外兩個資料集，遵循 4.1.2 節的設置。（i）由於我們不知道包含遮蔽的圖像識別資料集，我們創建了一個 CIFAR-10 遮蔽變體（Occ.）。\n",
      "原文: CIFAR-10), where a 14x14\n",
      "square is occluded by a black box, in each image including\n",
      "the test images; we evaluate a WRN-28-10 on Occ.CIFAR-\n",
      "10.We follow the settings for CIFAR-10 closely for this\n",
      "experiment.(ii) Additionally, we evaluate an RN-50 on the\n",
      "Stanford Cars dataset, which is a dataset in which visual de-\n",
      "tails are important to distinguish car models.We train for\n",
      "1000 epochs.Table 10 shows that TA continues to perform\n",
      "well in these settings, outperforming even brute-force tuned\n",
      "RA.Method Baseline Transfer-RA BF-RA TA (RA)\n",
      "Occ.CIFAR-10 94.99±.11 95.2 ±.08 95.52 ±.1895.72±.09\n",
      "Stanford Cars 90.21±.16 92.47 ±.17 - 92.77±.12\n",
      "Table 10: A comparison on non-standard datasets.The RA\n",
      "setting of BruteForce-RA is searched in the same large set\n",
      "as in Section 4.1.2.Transfer-RA is transferred from Wide-\n",
      "ResNet-28-10 on CIFAR-10 and ResNet-50 on ImageNet,\n",
      "respectively.BF-RA for S. Cars was not feasible in the\n",
      "given time.D. Evaluation on EfficientNet-B1\n",
      "While we tried to evaluatate on as relevant setups as\n",
      "possible, we also had to make sure that we can compare\n",
      "with previous work for the main evaluation in Section 4.1.1.\n",
      "翻譯結果: 本文進行了以下實驗：(i) 在Occ.CIFAR-10上對一個14x14的方形進行遮擋，在每個圖像上包括測試圖像；我們對Occ.CIFAR-10進行了WRN-28-10的評估。對於此實驗，我們緊密遵循CIFAR-10的設置。(ii) 此外，我們在Stanford Cars數據集上評估了RN-50，這是一個區分汽車型號的視覺細節很重要的數據集。我們訓練了1000個epoch。表10顯示，TA在這些設置中仍表現良好，甚至超過了粗暴調整的RA。方法 基線轉移RA 粗暴調整RA TA (RA) Occ.CIFAR-10 94.99±.11 95.2 ±.08 95.52 ±.1895.72±.09 Stanford Cars 90.21±.16 92.47 ±.17 - 92.77±.12 表10:在非標準數據集上進行比較。粗暴調整RA的RA設置在與4.1.2節中相同的大集合中搜索。轉移RA來自於CIFAR-10上的Wide-ResNet-28-10和ImageNet上的ResNet-50。S.Cars的BF-RA在給定的時間內不可行。D.對EfficientNet-B1的評估雖然我們試圖在盡可能相關的設置下進行評估，但我們還必須確保在4.1.1節的主要評估中可以與以前的工作進行比較。\n",
      "原文: Here, we add an Evaluation of an EfficientNet-B1 follow-\n",
      "ing the ImageNet setup described in the original paper [18]\n",
      "closely.None of the methods we compare to compares\n",
      "on this task, thus we re-implemented UA and RA as base-\n",
      "lines.For RA we performed a search over 3 settings for m,\n",
      "namely 8, 14 and 21, and fixed the number of augmentations\n",
      "nto 2 following the EfficientNet evaluations in [2].RA UA TA (Wide)\n",
      "EfficientNet-B1 78.75±.16 78.83 ±.23 78.99±.12\n",
      "Table 11: The average performance of an EfficientNet-B1\n",
      "across 5 re-runs on ImageNet with different augmentation\n",
      "methods.E. Approximation of the Compute Costs for\n",
      "Different Methods\n",
      "In this section, we discuss the data underlying our perfor-\n",
      "mance per compute comparison.To fairly compare meth-\n",
      "ods, we do not rely on published GPU times as much as\n",
      "possible, but instead calculate all costs for a RTX 2080 Ti\n",
      "for which we know many training times.Therefore, we can\n",
      "only compare methods for which we ran the models.That\n",
      "means for CIFAR-100 we consider only, the consider the\n",
      "Wide-ResNets as well as Shake-Shake-26-2x96d.\n",
      "翻譯結果: 在這裡，我們添加了對EfficientNet-B1的評估，並且在原論文[18]中描述的ImageNet設置中進行了密切的跟踪。在我們比較的方法中，沒有一種方法在這項任務上進行比較，因此我們重新實現了UA和RA作為基線。對於RA，我們對m進行了3種設置的搜索，分別為8、14和21，并且將增強數量固定為2，這遵循了[2]中對EfficientNet進行的評估。RA UA TA（Wide） EfficientNet-B1 78.75±.16 78.83±.23 78.99±.12\n",
      "\n",
      "表11：使用不同的增強方法對EfficientNet-B1進行5次重新運行的平均性能。E.不同方法的計算成本近似值在本節中，我們討論了我們性能和計算比較的基礎數據。為了公平地比較方法，我們盡可能不依賴於已發布的GPU時間，而是計算所有成本，以適合我們所知道的RTX 2080 Ti的訓練時間。因此，我們只能比較我們運行過模型的方法。這意味著對於CIFAR-100，我們僅考慮Wide-ResNets以及Shake-Shake-26-2 x 96d。\n",
      "原文: Our estimates for the cost of one epoch on the full\n",
      "CIFAR-10 dataset with 50,000 examples for each model:\n",
      "• Wide-ResNet-28-2: 16s\n",
      "• Wide-ResNet-40-2: 40s\n",
      "• Wide-ResNet-28-10: 101s\n",
      "• Shake-Shake-26-2x96d: 83s\n",
      "AA In the AA paper [1] the policy is trained over 15’000\n",
      "evaluations of a Wide-ResNet-40-2 on 120 epochs of 4000\n",
      "examples from CIFAR-10 for all models.We therefore\n",
      "estimate the search cost of AA as 15000 ·4000/50000 ·\n",
      "120·40s= 1600 h. Additionally we add the standard time\n",
      "for 200 epochs of standard training for each mode.Wide-\n",
      "ResNet-40-2: 1600h+ 40s·200/60/60, Wide-ResNet-28-\n",
      "10:1600h+ 101 s·200/60/60, Shake96: 1600h+ 83s·\n",
      "1800/60/60.Fast AA For Fast AA [13], we estimate, based on the\n",
      "GPU times in the paper that the search costs more than one\n",
      "翻譯結果: 個完整的CIFAR-10數據集中，每個模型的一個訓練循環的成本預估如下：\n",
      "• Wide-ResNet-28-2：16秒\n",
      "• Wide-ResNet-40-2：40秒\n",
      "• Wide-ResNet-28-10：101秒\n",
      "• Shake-Shake-26-2x96d：83秒\n",
      "在AA的論文[1]中，這個策略是在所有模型上進行了15,000次對CIFAR-10中的120個訓練循環，每個循環有4,000個示例。因此，我們估計AA的搜索成本為1600小時，即15000·4000/50000·120·40秒。此外，我們還為每個模型添加了200個訓練循環的標準時間。Wide-ResNet-40-2：1600h+ 40秒·200/60/60、Wide-ResNet-28-10：1600h+ 101秒·200/60/60、Shake96：1600h+ 83秒·1800/60/60。\n",
      "對於Fast AA [13]，我們根據論文中的GPU時間估計搜索成本多於一個訓練循環。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文: full training.We therefore estimate the compute cost as one\n",
      "training.UA and TA No search costs.Therefore the total cost\n",
      "simply is the cost of a single training.This is #epochs ·\n",
      "costperepoch.Adv.AA and TA x8 We assume for both setups no costs,\n",
      "even though this of course is only a lower bound on the\n",
      "compute requirements of Adv.AA.We simply multiply the\n",
      "number of epochs with the cost per epoch and 8, the number\n",
      "of workers.RA The authors of RA use a search space of 5 settings\n",
      "each is evaluated on 90% of the full dataset with the same\n",
      "number of epochs and model.So we have a factor of 5·9/10\n",
      "with which we multiply the standard costs to get the search\n",
      "costs.OHL OHL uses 300 epochs for the Wide-ResNets and\n",
      "trains with 8 parallel workers.We therefore have a factor\n",
      "of8·300/200compared to standard costs for search and\n",
      "training combined.A WS For AWS the data is not completely clear.First, we\n",
      "have an earlier version of the paper that says it evaluates\n",
      "800 policies, but in a later version this was corrected down\n",
      "to 500.\n",
      "翻譯結果: 完整訓練。我們因此估計計算成本為一次訓練。UA和TA無搜索成本。因此總成本僅為單次訓練的成本。這是＃epochs·costperepoch。Adv.AA和TA x8，我們假設兩個設置均沒有成本，盡管這當然只是Adv.AA計算要求的下限。我們只需將周期數與每個周期的成本和8（工人數）相乘即可。RA RA的作者使用5個設置的搜索空間，每個設置在完整數據集的90％上進行評估，具有相同的周期數和模型。因此，我們有一個因子5·9/10，我們將標準成本乘以該因子以獲得搜索成本。OHL OHL對Wide-ResNets使用300個周期，並使用8個並行工作人員進行培訓。因此，相對於標準成本，我們有一個因子8·300/200，用於搜索和訓練的結合。AWS對於AWS的數據並不完全清晰。首先，我們有論文的早期版本，該版本指出評估了800個策略，但在稍後的版本中，這個數字被修正為500。\n",
      "原文: We therefore assume only 500 policy evaluations\n",
      "to be conservative.They used a Wide-ResNet-28-10 for\n",
      "the augmentation search CIFAR-100 experiments.During\n",
      "augmentation search they train on 80% of the training set\n",
      "for 200 epochs first, and then for 10 epochs for each policy\n",
      "evaluation.This yields 0.8·(200 + 500 ·10)·101 = 117 h.\n",
      "For AWS's x8 setting (8-times augmented batches), we\n",
      "assume the same search costs as above and 8-times the train-\n",
      "ing costs.F. Recommendations for the Application of\n",
      "Automatic Augmentation Methods\n",
      "Based on our intense study of automatic augmentation\n",
      "methods for different image classification tasks using dif-\n",
      "ferent models we recommend the following steps when ap-\n",
      "plying automatic augmentation methods.In the application\n",
      "of an automatic augmentation method it is of course impor-\n",
      "tant to know, whether a method is easy to reimplement.We\n",
      "thus put together Table 12 for easy guidance.Standard Model and Dataset If the model and dataset\n",
      "combination you are using is part of automatic augmenta-\n",
      "tion literature, we recommend to simply use the best pub-\n",
      "lished method for your setup with published code and poli-\n",
      "cies.Novel model or Novel dataset If you are using a setup\n",
      "not evaluated in the automatic augmentation literature, it\n",
      "is a good approach to try both the best performing model\n",
      "on a similar task as well as a parameter-free baseline.\n",
      "翻譯結果: 為了保守起見，我們僅假設進行500次政策評估。在CIFAR-100實驗中，他們使用了Wide-ResNet-28-10進行增強搜索。在增強搜索期間，他們首先對訓練集的80％進行200個epoch的訓練，然後對每個政策評估進行10個epoch的訓練。這將產生0.8·(200 + 500·10)·101 = 117小時。對於AWS的x8設置（8倍增強批次），我們假設與上述相同的搜索成本，並且是8倍的訓練成本。\n",
      "\n",
      "F. 自動增強方法的應用建議\n",
      "通過對使用不同模型進行不同圖像分類任務的自動增強方法進行深入研究，我們建議在應用自動增強方法時應採取以下步驟。當應用自動增強方法時，當然重要的是要知道該方法是否易於重新實施。因此，我們提供第12表以進行簡單指導。\n",
      "標準模型和數據集：如果您使用的模型和數據集組合是自動增強文獻的一部分，我們建議您使用發布了代碼和政策的最佳方法進行您的設置。\n",
      "新模型或新數據集：如果您使用的設置未在自動增強文獻中進行評估，則嘗試使用更類似的任務上表現最佳的模型以及一個不依賴參數的基線的方法是很好的途徑。\n",
      "原文: The parameter-free baseline, like UniformAugment or Triv-\n",
      "ialAugment, especially can be expected to generalize to the\n",
      "new task, since they generalized to all standard automatic\n",
      "evluation benchmarks without any tuning.If you have tun-\n",
      "ing budget, you can of course tune something like PBA\n",
      "to your particular task.This likely is a good idea if your\n",
      "images are very dissimilar to the automatic augmentation\n",
      "benchmarks.\n",
      "翻譯結果: 沒有參數的基礎模型，如UniformAugment或TrivialAugment，尤其可以期望其泛化到新任務上，因為它們在沒有調整的情況下泛化到所有標準的自動評估基準上。如果您有調整預算，當然可以調整像PBA這樣的模型以適用於特定任務。如果您的圖像與自動擴增基準非常不同，這可能是個好主意。\n",
      "原文: Method Policies for Training Code for training Code for meta-training\n",
      "Cheap Search\n",
      "TA ✓ ✓ -\n",
      "UA ✓ ✗ -\n",
      "Fast AA ✓p✓ ✓\n",
      "Expensive Search ( >2×)\n",
      "RA ✓ ✗ ✗\n",
      "Adv.AA ✗ ✗ ✗\n",
      "OHL ✗ ✗ ✗p\n",
      "Very Expensive Search ( >10×)\n",
      "PBA ✓s✓i✓i\n",
      "AWS ✗ ✗ ✗p\n",
      "AA ✓ ✓i✗\n",
      "Table 12: In this table we compare the reproducibility of different methods in three categories.(i) Whether the augmentation\n",
      "policies used for model trainings are available, (ii) whether the authors provide code for training a model with the policies on\n",
      "which they report their performance and (iii) whether there is code available to run the search for training policies, code for a\n",
      "meta-training.We mark entries with - if it is not an applicable category for the given augmentation method and additionally\n",
      "use the following symbols.✓s: Only available for subset of experiments, ✓i: Not available for ImageNet trainings, which\n",
      "for PBA was also not considered in the paper, ✗p: there is publicly work in progress.\n",
      "翻譯結果: 方法方針，用於訓練代碼元訓練代碼的訓練\n",
      "廉價的搜索\n",
      "TA ✓ ✓ -\n",
      "UA ✓ ✗ -\n",
      "快速AA ✓p✓ ✓\n",
      "昂貴的搜索（>2×）\n",
      "RA ✓ ✗ ✗\n",
      "Adv.AA ✗ ✗ ✗\n",
      "OHL ✗ ✗ ✗p\n",
      "非常昂貴的搜索（>10×）\n",
      "PBA ✓s✓i✓i\n",
      "AWS ✗ ✗ ✗p\n",
      "AA ✓ ✓i✗\n",
      "\n",
      "表12：在這個表格中，我們比較了不同方法在三個類別中的再現性。 (i)是否有可用的模型訓練中使用的擴增政策， (ii) 作者是否提供可以使用政策來訓練模型的代碼，並且他們報告了其性能和 (iii) 是否有代碼可用於運行搜索訓練政策的代碼，用於元訓練。 如果對於給定的擴增方法不適用類別，我們將標記條目為 -，並且使用以下符號。 ✓s：僅對實驗子集可用，✓i：無法用於ImageNet訓練，對於PBA也未在論文中考慮，✗p：有正在進行的公共工作。\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for i in range(len(chunks)):\n",
    "    if len(chunks[i]) > 10000 or len(chunks[i]) == 0:\n",
    "        continue\n",
    "    print('原文:', chunks[i])\n",
    "    completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"請你成為文章翻譯的小幫手，請協助翻譯以下技術文件，以繁體中文輸出\"},\n",
    "        {\"role\": \"user\", \"content\": chunks[i]},\n",
    "    ]\n",
    "    )\n",
    "    f.write('原文:\\n'+chunks[i]+'\\n')\n",
    "    print('翻譯結果:',completion.choices[0].message.content)\n",
    "    f.write('翻譯結果:\\n'+completion.choices[0].message.content+'\\n')\n",
    "    time.sleep(15)\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93678e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
